---
title: '飞书一面'
tags:
date: '2022-11-18'
categories:
draft: true
---

大佬内推飞书，挂过一次，这次准备了一番，再次上阵。但是没有问太死的八股文，希望后续面试不会太难。

## 问题

1. 项目中用到 mongo, Watch 是怎么使用的？是 pull 还是 push 机制？都会获取到哪些信息？
答：使用的第三方库 [mongo-driver-Watch](https://pkg.go.dev/go.mongodb.org/mongo-driver/mongo#Collection.Watch)，是 pull 机制。其实面试官想问的应该是[Mongo.watch](https://www.mongodb.com/docs/manual/reference/method/Mongo.watch/)，具体的需要学习补充一下。
2. 谈谈 gorm 的使用？它都为开发者提供了哪些好处？

我的回答仅涉及到使用上，所以可能更好的回答：
- 全功能 ORM (无限接近)
- 关联 (Has One, Has Many, Belongs To, Many To Many, 多态)
- 钩子 (在创建/保存/更新/删除/查找之前或之后)
- 预加载
- 事务
- 复合主键
- SQL 生成器
- 数据库自动迁移
- 自定义日志
- 可扩展性, 可基于 GORM 回调编写插件
- 所有功能都被测试覆盖
- 开发者友好

3. slice 扩容

略

4. kafka 中的几个概念
- Broker：一台kafka服务器就是一个broker。一个集群由多个broker组成。一个broker可以容纳多个topic。
- Topic：主题，由用户定义并配置在Kafka服务器，用于建立Producer和Consumer之间的订阅关系
- Partition：消息分区，一个topic可以分为多个 partition，每个partition是一个有序的队列。partition中的每条消息都会被分配一个有序的id（offset）。
- Consumer Group - 消费者分组，用于归组同类消费者。每个consumer属于一个特定的consumer group，多个消费者可以共同消息一个Topic下的消息，每个消费者消费其中的部分消息，这些消费者就组成了一个分组，拥有同一个分组名称，通常也被称为消费者集群
- Offset - 消息在partition中的偏移量。每一条消息在partition都有唯一的偏移量，消息者可以指定偏移量来指定要消费的消息。

5. 关于 kafka 中 partition 数量的说法
为了提高kafka的吞吐量，增加parition的数量，提升生产消费并行数是个不错的选择。但partition数量也不是越多越好，以下是partition数量太多可能导致的问题，供大家选择partition数量时参考。

1. partition数量该如何选择：
假设一个partition的生产速度是p， 消费速度是c。而你预期的消息吞吐速度是t。则partition的数量至少为max(t/p, t/c) 。

生产速度取决于你的config参数，比如linger_ms, batch_size,  replication factor, type of ack。 而消费速度则取决于你自身的消费逻辑。

2. 太多的partition需要打开更多文件句柄
每个partition在broker的文件系统上都映射着对于的文件夹。 在文件夹里面，每个segment（partition下的存储单位）会有两个文件，一个是索引，另一个是消息数据。  每个broker都会为每个segment打开两个文件句柄。所以如果parition太多的话，你就需要在操作系统里配置下可以同时打开的文件句柄数以使得可以支撑如此之多的segment。曾经见过一个kafka集群，里面的broker平均同时打开了30000多个句柄（约15000个分区）。

 
3. 太多的partition导致可用性降低

一个partition对应着多个副本。在一个broker优雅退出时，集群的controller会提前地将该broker上的leader迁移到其他broker上。 迁移一个leader只需要几毫秒的时间，所以从客户端角度来说，这只会有一小段时间不可用（只在迁移时的时间不可用，毫秒级）

然而如果一台broker是被粗暴的退出了，那不可用时间就取决于partition的数量了。 假设现在有个broker有2000个partition， 每个partition有两个副本，那这个broker大概有1000个leader。 如果这个broker被粗暴退出了，那这1000个partition马上不可用。如果每个分区需要花5ms的时间去选择一个新的leader，则1000个分区需要花费5s。所以，对于其中的partition来说，他们之中最长的不可用时间是5s加上检测出broker挂的时间。

如果更加不幸，这个挂了的broker是集群的controller，那它还需要先在集群里选举一个新的controller后才能进行leader迁移。新的controller还需要从zookeeper（新版kafka已经不从zk里读取）读取metadata。如果一个分区需要2ms，假设一个集群里有10000个partition，那这里将有20s的不可用时间。

建议：一个broker下的partition数量限制在2000-4000，整个集群的partition数量不高于一万。

4. 太多的partition增加了消息的延迟
消息的延迟指的是一条消息从生产者发送到被消费者消费的时间差。 Kafka只会把已经被所有副本复制成功的消息下发给消费者。而kafka的broker只使用单线程去从其他broker里复制消息。 我们实验表明从一个broker复制1000个partition到另一个broker会增加约20ms的延时。 如果复制因子设置为2，那这个问题在一个大集群里不会很明显。 假设现在有一个broker里有1000个leader， 集群里还有另外10个broker。 其他的10个broker平均只需要从第一个broker里获取100个partition的信息，因此，这种情况下的延时就非常小，一般只有几ms。

建议：每个broker下的partition数量应该限制为100*b*r，b是集群里的broker数量，r是复制因子。

5. 太多的partition使得客户端需要占用更多的内存
生产者会给每个partition申请一份内存空间用以缓存将要发送但还没发送的数据。这种批量发送的方式能够减少网络交换次数，提高发送的效率。但如果你的partition太多，则生产者需要更多个内存来缓存将要发送的消息。

消费者也一样，消费者也可以从partition里批量拉取消息，分区越多，则拉取消息占用的内存越多。

6. 基于以上，如果有A broker（有200个partition）和 B broker（1个partition），且 B 的数据写入更多一些，哪个 broker 的压力更大？

明显 B 的压力更大，消息多，分区少，造成写入和消费延迟，且数据不能及时从内存刷到磁盘，内存占用也多。

7. 已读消息的设计思路，记录需要永久保存

使用 redis 存储对应的 `messageID ： 已读用户的 ID 集合`，并设置过期时间。因为记录需要永久保存，要保证并发处理性能，使用 kafka 进行异步处理，消费端将数据进行合并之后入库（一个记录只存一行）。当 redis 中数据过期之后，一般只有少量用户可能会翻找很久的记录是否已读，这时再查库并缓存到 redis，设置一个较短过期时间。


8. 算法题： 一个升序链表，删除其中重复了的的节点，如输入 `[1,2,2,3,3,4]`,输出`[1,2,3,4]`

