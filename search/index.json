[{"content":"简介 切片(slice)是 Golang 中一种比较特殊的数据结构，这种数据结构更便于使用和管理数据集合。切片是围绕动态数组的概念构建的，可以按需自动增长和缩小。切片的动态增长是通过内置函数 append() 来实现的，这个函数可以快速且高效地增长切片，也可以通过对切片再次切割，缩小一个切片的大小。因为切片的底层也是在连续的内存块中分配的，所以切片还能获得索引、迭代以及为垃圾回收优化的好处。\n实现 1 2 3 4 5 type slice struct { array unsafe.Pointer // 指向底层数组的指针 len int // 长度 cap int // 容量 } nil 和 空切片 nil 切片:\n1 var nums []int 空切片：\n1 2 3 4 //使用make s1 := make([]int, 0) //使用切片字面量 s2 := []int{} 空切片与nil切片的区别:\nnil切片=nil, 而空切片！=nil，在使用切片进行逻辑运算时尽量不要使用空切片 空切片指针指向一个特殊的 zerobase 地址，而 nil 为 0 在JSON序列化有区别：nil 切片为 {\u0026quot;values\u0026quot;:null}, 而空切片为 {\u0026quot;value\u0026quot; []} 扩容 go1.15.6 扩容的基本原则：\n如果 slice 的容量小于 1024，则新的扩容会是原来的 2 倍。 如果原来的 slice 的容量大于或者等于 1024，则新的扩容将扩大大于或者等于原来 1.25 倍。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 //go1.15.6 源码 src/runtime/slice.go func growslice(et *_type, old slice, cap int) slice { //省略部分判断代码 //计算扩容部分 //其中，cap : 所需容量，newcap : 最终申请容量 newcap := old.cap doublecap := newcap + newcap if cap \u0026gt; doublecap { newcap = cap } else { if old.len \u0026lt; 1024 { newcap = doublecap } else { // Check 0 \u0026lt; newcap to detect overflow // and prevent an infinite loop. for 0 \u0026lt; newcap \u0026amp;\u0026amp; newcap \u0026lt; cap { newcap += newcap / 4 } // Set newcap to the requested cap when // the newcap calculation overflowed. if newcap \u0026lt;= 0 { newcap = cap } } } //省略部分判断代码 } go1.19 扩容的基本原则：\n如果 slice 的容量小于 256，则新的扩容会是原来的 2 倍。 如果 slice 的容量大于 256，则使用计算公式 newcap += (newcap + 3 * 256) / 4。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 func growslice(et *_type, old slice, cap int) slice { if raceenabled { callerpc := getcallerpc() racereadrangepc(old.array, uintptr(old.len*int(et.size)), callerpc, abi.FuncPCABIInternal(growslice)) } if msanenabled { msanread(old.array, uintptr(old.len*int(et.size))) } if asanenabled { asanread(old.array, uintptr(old.len*int(et.size))) } if cap \u0026lt; old.cap { panic(errorString(\u0026#34;growslice: cap out of range\u0026#34;)) } if et.size == 0 { // append should not create a slice with nil pointer but non-zero len. // We assume that append doesn\u0026#39;t need to preserve old.array in this case. return slice{unsafe.Pointer(\u0026amp;zerobase), old.len, cap} } newcap := old.cap doublecap := newcap + newcap if cap \u0026gt; doublecap { newcap = cap } else { const threshold = 256 if old.cap \u0026lt; threshold { newcap = doublecap } else { // Check 0 \u0026lt; newcap to detect overflow // and prevent an infinite loop. for 0 \u0026lt; newcap \u0026amp;\u0026amp; newcap \u0026lt; cap { // Transition from growing 2x for small slices // to growing 1.25x for large slices. This formula // gives a smooth-ish transition between the two. newcap += (newcap + 3*threshold) / 4 } // Set newcap to the requested cap when // the newcap calculation overflowed. if newcap \u0026lt;= 0 { newcap = cap } } } ... } 对于 append 向 slice 添加元素的步骤：\n加入 slice 容量够用，则追加新元素进去，slice.len++，返回原来的slice。 当原容量不够，则 slice 先扩容，扩容之后得到新的 slice，将元素追加进新的 slice，slice.len++，返回新的 slice。 ","date":"2022-11-11T00:00:00Z","permalink":"https://ggxxll.github.io/p/golang-slice/","title":"Golang Slice"},{"content":"简介 跳表全称为跳跃列表，它允许快速查询，插入和删除一个有序连续元素的数据链表。跳跃列表的平均查找和插入时间复杂度都是 O(logn)。快速查询是通过维护一个多层次的链表，且每一层链表中的元素是前一层链表元素的子集（见示意图）。一开始时，算法在最稀疏的层次进行搜索，直至需要查找的元素在该层两个相邻的元素中间。这时，算法将跳转到下一个层次，重复刚才的搜索，直到找到需要查找的元素为止。\n一张跳跃列表的示意图。每个带有箭头的框表示一个指针, 而每行是一个稀疏子序列的链表；底部的编号框（黄色）表示有序的数据序列。查找从顶部最稀疏的子序列向下进行, 直至需要查找的元素在该层两个相邻的元素中间。\n","date":"2022-11-11T00:00:00Z","permalink":"https://ggxxll.github.io/p/%E8%B7%B3%E8%A1%A8/","title":"跳表"},{"content":"MVCC 全称 Multi-Version Concurrency Control, 即多版本并发控制。MVCC 是一种并发控制的方法，一般在数据库管理系统中，实现对数据库的并发访问，在编程语言中实现事务内存。\nMVCC 在 MySQL InnoDB 中的实现主要是为了提高数据库并发性能，用更好的方式去处理读-写冲突，做到即使有读写冲突时，也能做到不加锁，非阻塞并发读\n当前读和快照读 当前读 像 select lock in share mode (共享锁), select for update; update; insert; delete (排他锁)这些操作都是一种当前读，为什么叫当前读？就是它读取的是记录的最新版本，读取时还要保证其他并发事务不能修改当前记录，会对读取的记录进行加锁\n快照读 像不加锁的 select 操作就是快照读，即不加锁的非阻塞读；快照读的前提是隔离级别不是串行级别，串行级别下的快照读会退化成当前读；之所以出现快照读的情况，是基于提高并发性能的考虑，快照读的实现是基于多版本并发控制，即 MVCC ,可以认为 MVCC 是行锁的一个变种，但它在很多情况下，避免了加锁操作，降低了开销；既然是基于多版本，即快照读可能读到的并不一定是数据的最新版本，而有可能是之前的历史版本\nLog BinLog binlog：是mysql服务层产生的日志，常用来进行数据恢复、数据库复制，常见的mysql主从架构，就是采用slave同步master的binlog实现的, 另外通过解析binlog能够实现mysql到其他数据源（如 ElasticSearch )的数据复制。\nRedoLog redo log：记录了数据操作在物理层面的修改，mysql中使用了大量缓存，缓存存在于内存中，修改操作时会直接修改内存，而不是立刻修改磁盘，当内存和磁盘的数据不一致时，称内存中的数据为脏页(dirty page)。为了保证数据的安全性，事务进行中时会不断的产生 redo log，在事务提交时进行一次 flush 操作，保存到磁盘中, redo log 是按照顺序写入的，磁盘的顺序读写的速度远大于随机读写。当数据库或主机失效重启时，会根据 redo log 进行数据的恢复，如果 redo log 中有事务提交，则进行事务提交修改数据。这样实现了事务的原子性、一致性和持久性。\nUndoLog undo log：除了记录 redo log 外，当进行数据修改时还会记录 undo log，undo log用于数据的撤回操作，它记录了修改的反向操作，比如，插入对应删除，修改对应修改为原来的数据，通过 undo log 可以实现事务回滚，并且可以根据 undo log 回溯到某个特定的版本的数据，实现 MVCC。\nReadView MVCC 只在 read-committed（RC） 和 repeatable-read（RR） 两个隔离级别下工作，而 read-committed 和 repeatable-read 的区别就在于它们生成 ReadView 的时机不同。\n对于使用 read-committed 和 repeatable-read 隔离级别的事务来说，都必须保证读到已经提交了的事务修改过的记录，也就是说假如另一个事务已经修改了记录但是尚未提交，是不能直接读取最新版本的记录的。\n核心问题就是：需要判断一下，版本链中的哪个版本是当前事务可见的。\n为此，InnoDB 提出了一个 ReadView 的概念，这个 ReadView 中有个 id 列表 trx_ids 来存储系统中当前活跃着的读写事务，也就是 begin 了还未 commit 或 rollback 的事务。其中最主要的与可见性相关的属性如下：\nup_limit_id：当前已经提交的事务号 + 1，事务号 \u0026lt; up_limit_id ，对于当前 Read View 都是可见的。理解起来就是创建 Read View 视图的时候，之前已经提交的事务对于该事务肯定是可见的。 low_limit_id：当前最大的事务号 + 1，事务号 \u0026gt;= low_limit_id，对于当前 Read View 都是不可见的。理解起来就是在创建 Read View 视图之后创建的事务对于该事务肯定是不可见的。 trx_ids：为活跃事务 id 列表，即 Read View 初始化时当前未提交的事务列表。所以当进行 RR 读的时候，trx_ids 中的事务对于本事务是不可见的（除了自身事务，自身事务对于表的修改对于自己当然是可见的）。理解起来就是创建 Read View 时，将当前活跃事务 ID 记录下来，后续即使他们提交对于本事务也是不可见的。 creator_trx_id：当前事务自身的 id 可重复读隔离级别下，ReadView 只会在第一次查询时创建，同一个事务中后续所有的查询共用一个 ReadView，由此便解决了不可重复读的问题。\n读已提交隔离级别下，每次查询都会创建一个新的 ReadView。新建的 ReadView 会更新 creator_trx_id 以外的其余字段，因此不可重复读现象依然存在。但是由于ReadView可以判断出修改此数据的事务是否已经提交，因此可以避免脏读的出现。\n其次，从上述MVCC实现逻辑中可以发现，没有任何加锁、获取锁的操作，因此MVCC读操作不会因为等待锁而阻塞（也就是常说的非阻塞读）。\nMVCC 原理 MySQL的每行记录逻辑上其实是一个链表。\nMySQL行记录中除了记录业务数据外，还有隐藏的 trx_id 和 roll_pointer\ntrx_id：表示最近修改的事务的id ，每次一个事务对某条聚簇索引记录进行改动时，都会把该事务的事务 id 赋值给 trx_id 隐藏列。新增一个事务时，trx_id 会递增，因此 trx_id 能够表示事务开始的先后顺序。 roll_pointer：指向该行上一个版本的地址，每次对某条聚簇索引记录进行改动时，都会把旧的版本写入到 UndoLog 中，然后这个隐藏列就相当于一个指针，可以通过它来找到该记录修改前的信息。 MVCC 能解决什么问题，好处是？ 数据库并发场景有三种，分别为：\n读-读：不存在任何问题，也不需要并发控制 读-写：有线程安全问题，可能会造成事务隔离性问题，可能遇到脏读，幻读，不可重复读 写-写：有线程安全问题，可能会存在更新丢失问题，比如第一类更新丢失，第二类更新丢失 ","date":"2022-11-10T00:00:00Z","permalink":"https://ggxxll.github.io/p/mysql-mvcc/","title":"Mysql MVCC"},{"content":"介绍 索引是一个单独的、存储在磁盘上的数据库结构，它们包含着对数据表里所有记录的引用指针。使用索引用于快速找出在某个或多个列中有一特定值的行，所有MySQL列类型都可以被索引，对相关列使用索引是提高查询操作速度的最佳途径。\nMySQL索引的建立对于MySQL的高效运行是很重要的，索引可以大大提高MySQL的检索速度。比如我们在查字典的时候，前面都有检索的拼音和偏旁、笔画等，然后找到对应字典页码，这样然后就打开字典的页数就可以知道我们要搜索的某一个key的全部值的信息了。\n创建索引时，你需要确保该索引是应用在 SQL 查询语句的条件(一般作为 WHERE 子句的条件)，而不是在 SELECT 的字段中，实际上，索引也是一张“表”，该表保存了主键与索引字段，并指向实体表的记录，虽然索引大大提高了查询速度，同时却会降低更新表的速度，如对表进行 INSERT、UPDATE和DELETE。因为更新表时，MySQL 不仅要保存数据，还要保存一下索引文件，建立索引会占用磁盘空间的索引文件。说白了索引就是用来提高速度的，但是就需要维护索引造成资源的浪费，所以合理的创建索引是必要的。\n分类 先去官网文档看看支持的索引类型，索引的实现方式如下图所示：官网\n由于本文是基于 MySQL 的 InnoDB 存储引擎，索引我们主要看第一个表格，其他的表格可以自行的观看，都不难，从表格我们可以看出来，InnoDB存储引擎索引只支持BTREE类型的索引，索引的类别有Primary Key，Unique，Key，FULLTEXT和SPATIAL。当然也有其他的分法，按照索引列的数量分为单列索引和组合索引。\nPrimary Key（聚集索引）：InnoDB 存储引擎的表会存在主键（唯一非 null），如果建表的时候没有指定主键，则会使用第一非空的唯一索引作为聚集索引，否则 InnoDB 会自动帮你创建一个不可见的、长度为 6 字节的 row_id 用来作为聚集索引。 单列索引：单列索引即一个索引只包含单个列 组合索引：组合索引指在表的多个字段组合上创建的索引，只有在查询条件中使用了这些字段的左边字段时，索引才会被使用。使用组合索引时遵循最左前缀集合 Unique（唯一索引）：索引列的值必须唯一，但允许有空值。若是组合索引，则列值的组合必须唯一。主键索引是一种特殊的唯一索引，不允许有空值 Key（普通索引）：是 MySQL 中的基本索引类型，允许在定义索引的列中插入重复值和空值 FULLTEXT（全文索引）：全文索引类型为FULLTEXT，在定义索引的列上支持值的全文查找，允许在这些索引列中插入重复值和空值。全文索引可以在 CHAR、VARCHAR 或者 TEXT 类型的列上创建 SPATIAL（空间索引）：空间索引是对空间数据类型的字段建立的索引，MySQL 中的空间数据类型有4种，分别是 GEOMETRY、POINT、LINESTRING和POLYGON。MySQL 使用 SPATIAL 关键字进行扩展，使得能够用于创建正规索引类似的语法创建空间索引。创建空间索引的列必须声明为 NOT NULL 创建原则 索引并非越多越好，一个表中如果有大量的索引，不仅占用磁盘空间，而且会影响 INSERT、DELETE、UPDATE 等语句的性能，因为在表中的数据更改的同时，索引也会进行调整和更新 避免对经常更新的表创建过多的索引，并且索引中的列尽可能少。而对经常用于查询的字段应该创建索引，但要避免添加不必要的字段。 数据量小的表最好不要使用索引，由于数据较少，查询花费的时间可能比遍历索引的时间还要短，索引可能不会产生优化效果。 在条件表达式中经常用到的不同值较多的列上建立索引，在不同值很少的列上不要建立索引。比如在学生表的“性别”字段上只有“男”与“女”两个不同值，因此就无须建立索引。如果建立索引，不但不会提高查询效率，反而会严重降低数据更新速度。 当唯一性是某种数据本身的特征时，指定唯一索引。使用唯一索引需能确保定义的列的数据完整性，以提高查询速度。 在频繁进行排序或分组（即进行 group by 或 order by 操作）的列上建立索引，如果待排序的列有多个，可以在这些列上建立组合索引。 搜索的索引列，不一定是所要选择的列。换句话说，最适合索引的列是出现在 WHERE 子句中的列，或连接子句中指定的列，而不是出现在 SELECT 关键字后的选择列表中的列。 使用短索引。如果对字符串列进行索引，应该指定一个前缀长度，只要有可能就应该这样做。例如，有一个 CHAR(200) 列，如果在前10个或20个字符内，多数值是唯一的，那么就不要对整个列进行索引。对前10个或20个字符进行索引能够节省大量索引空间，也可能会使查询更快。较小的索引涉及的磁盘 IO 较少，较短的值比较起来更快。更为重要的是，对于较短的键值，索引高速缓存中的块能容纳更多的键值，因此，MySQL 也可以在内存中容纳更多的值。这样就增加了找到行而不用读取索引中较多块的可能性。 利用最左前缀。在创建一个 n 列的索引时，实际是创建了 MySQL 可利用的 n 个索引。多列索引可起几个索引的作用，因为可利用索引中最左边的列集来匹配行。这样的列集称为最左前缀。 对于 InnoDB 存储引擎的表，记录默认会按照一定的顺序保存，如果有明确定义的主键，则按照主键顺序保存。如果没有主键，但是有唯一索引，那么就是按照唯一索引的顺序保存。如果既没有主键又没有唯一索引，那么表中会自动生成一个内部列，按照这个列的顺序保存。按照主键或者内部列进行的访问是最快的，所以InnoDB表尽量自己指定主键，当表中同时有几个列都是唯一的，都可以作为主键的时候，要选择最常作为访问条件的列作为主键，提高查询的效率。另外，还需要注意，InnoDB 表的普通索引都会保存主键的键值，所以主键要尽可能选择较短的数据类型，可以有效地减少索引的磁盘占用，提高索引的使用效果 索引的管理和使用 EXPLAIN 基本语法：EXPLAIN SELECT ...\n结果：\nid：SELECT 识别符。这是 SELECT 的查询序列号,表示查询中执行 SELECT 子句或操作表的顺序, id 相同，执行顺序从上到下,id 不同，id 值越大执行优先级越高 select_type：表示 SELECT 语句的类型。它可以是以下几种取值： SIMPLE：表示简单查询，其中不包括连接查询和子查询； PRIMARY：表示主查询，或者是最外层的查询语句，最外层查询为 PRIMARY ，也就是最后加载的就是- PRIMARY； UNION：表示连接查询的第2个或后面的查询语句， 不依赖于外部查询的结果集 DEPENDENT UNION：连接查询中的第2个或后面的 SELECT 语句，依赖于外面的查询； UNION RESULT：连接查询的结果； SUBQUERY：子查询中的第1个 SELECT 语句；不依赖于外部查询的结果集 DEPENDENT SUBQUERY：子查询中的第1个 SELECT，依赖于外面的查询； DERIVED：导出表的 SELECT（FROM 子句的子查询）,MySQL 会递归执行这些子查询，把结果放在临时表里。 DEPENDENT DERIVED：派生表依赖于另一个表 MATERIALIZED：物化子查询 UNCACHEABLE SUBQUERY：子查询，其结果无法缓存，必须针对外部查询的每一行重新进行评估 UNCACHEABLE UNION：UNION中的第二个或随后的 SELECT 查询，属于不可缓存的子查询 table：表示查询的表 partitions：查询将从中匹配记录的分区。该值适用NULL于未分区的表 type：表示表的连接类型 system：该表是仅有一行的系统表。这是 const 连接类型的一个特例 const： 数据表最多只有一个匹配行，它将在查询开始时被读取，并在余下的查询优化中作为常量对待。const 表查询速度很快，因为只读取一次,const 用于使用常数值比较 PRIMARY KEY 或 UNIQUE 索引的所有部分的场合。 eq_ref：对于每个来自前面的表的行组合，从该表中读取一行,可以用于使用“＝”运算符进行比较的索引列 。比较值可以是常量，也可以是使用在此表之前读取的表中列的表达式 ref：对于来自前面的表的任意行组合，将从该表中读取所有匹配的行，ref可以用于使用“＝”或“＜＝＞”操作符的带索引的列。 fulltext：使用 FULLTEXT 索引执行联接 ref_or_null：这种连接类型类似于 ref，但是除了 MySQL 还会额外搜索包含 NULL 值的行。此联接类型优化最常用于解析子查询 index_merge：此联接类型指示使用索引合并优化。在这种情况下，key 输出行中的列包含使用的索引列表，并且 key_len 包含使用的索引的最长键部分的列表 unique_subquery：类型替换 以下形式的eq_ref某些 IN子查询, unique_subquery 只是一个索引查找函数，它完全替代了子查询以提高效率。 index_subquery：连接类型类似于 unique_subquery。它代替 IN 子查询,但只适合子查询中的非唯一索引 range：只检索给定范围的行，使用一个索引来选择行。key列显示使用了哪个索引。key_len 包含所使用索引的最长关键元素。当使用 ＝、＜＞、＞、＞＝、＜、＜＝、IS NULL、＜＝＞、BETWEEN 或者 IN 操作符用常量比较关键字列时，类型为range index：该 index 联接类型是一样的 ALL，只是索引树被扫描。这发生两种方式： 如果索引是查询的覆盖索引，并且可用于满足表中所需的所有数据，则仅扫描索引树。在这种情况下，Extra列显示为 - Using index; 使用对索引的读取执行全表扫描，以按索引顺序查找数据行。 Uses index 没有出现在 Extra列中; ALL：对于前面的表的任意行组合进行完整的表扫描 possible_keys：指出MySQL能使用哪个索引在该表中找到行。若该列是 NULL，则没有相关的索引。在这种情况下，可以通过检查 WHERE 子句看它是否引用某些列或适合索引的列来提高查询性能。如果是这样，可以创建适合的索引来提高查询的性能。 kye：表示查询实际使用的索引，如果没有选择索引，该列的值是 NULL。要想强制 MySQL 使用或忽视 possible_keys 列中的索引，在查询中使用 FORCE INDEX、USE INDEX 或者 IGNORE INDEX key_len：表示 MySQL 选择的索引字段按字节计算的长度，若键是 NULL，则长度为 NULL。注意，通过 key_len 值可以确定 MySQL 将实际使用一个多列索引中的几个字段 ref：表示使用哪个列或常数与索引一起来查询记录。 rows：显示 MySQL 在表中进行查询时必须检查的行数。 Extra：表示 MySQL 在处理查询时的详细信息 更详细说明见官网：https://dev.mysql.com/doc/refman/8.0/en/explain-output.html\n操作索引 创建索引的语法（如下都是默认的innodb存储引擎）：https://dev.mysql.com/doc/refman/8.0/en/create-index.html\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 CREATE [UNIQUE | FULLTEXT | SPATIAL] INDEX index_name [index_type] ON tbl_name (key_part,...) [index_option] [algorithm_option | lock_option] ... key_part: {col_name [(length)] | (expr)} [ASC | DESC] index_option: { KEY_BLOCK_SIZE [=] value | index_type | WITH PARSER parser_name | COMMENT \u0026#39;string\u0026#39; | {VISIBLE | INVISIBLE} | ENGINE_ATTRIBUTE [=] \u0026#39;string\u0026#39; | SECONDARY_ENGINE_ATTRIBUTE [=] \u0026#39;string\u0026#39; } index_type: USING {BTREE | HASH} algorithm_option: ALGORITHM [=] {DEFAULT | INPLACE | COPY} lock_option: LOCK [=] {DEFAULT | NONE | SHARED | EXCLUSIVE} 1 ALTER TABLE table_name ADD [UNIQUE|FULLTEXT|SPATIAL] [INDEX|KEY] [index_name] (col_name[length],...) [ASC|DESC] 1 ALTER TABLE table_name DROP INDEX index_name 1 DROP INDEX index_name ON table_name; 聚集索引和二级索引 聚集索引 InnoDB 存储引擎表是索引组织表，即表中数据按照主键顺序存放。而聚集索引（clustered index）就是按照每张表的主键构造一棵 B+ 树，同时叶子节点中存放的即为整张表的行记录数据，也将聚集索引的叶子节点称为数据页。聚集索引的这个特性决定了索引组织表中数据也是索引的一部分。同 B+ 树数据结构一样，每个数据页都通过一个双向链表来进行链接。\n由于实际的数据页只能按照一棵 B+ 树进行排序，因此每张表只能拥有一个聚集索引。由于定义了数据的逻辑顺序，聚集索引能够特别快地访问针对范围值的查询。查询优化器能够快速发现某一段范围的数据页需要扫描。\n聚集索引的存储并不是物理上连续的，而是逻辑上连续的。这其中有两点：一是前面说过的页通过双向链表链接，页按照主键的顺序排序；另一点是每个页中的记录也是通过双向链表进行维护的，物理存储上可以同样不按照主键存储。\n二级索引 对于辅助索引（Secondary Index），叶子节点并不包含行记录的全部数据。叶子节点除了包含键值以外，每个叶子节点中的索引行中还包含了一个书签（bookmark）。该书签用来告诉 InnoDB 存储引擎哪里可以找到与索引相对应的行数据。由于 InnoDB 存储引擎表是索引组织表，因此 InnoDB 存储引擎的辅助索引的书签就是相应行数据的聚集索引键。\n当通过辅助索引来寻找数据时，InnoDB 存储引擎会遍历辅助索引并通过叶级别的指针获得指向主键索引的主键，然后再通过主键索引来找到一个完整的行记录。\n覆盖索引 InnoDB 存储引擎支持覆盖索引（covering index，或称索引覆盖），即从辅助索引中就可以得到查询的记录，而不需要查询聚集索引中的记录。使用覆盖索引的一个好处是辅助索引不包含整行记录的所有信息，故其大小要远小于聚集索引，因此可以减少大量的 IO 操作。\nMulti-Range Read 优化 Multi-Range Read 优化的目的就是为了减少磁盘的随机访问，并且将随机访问转化为较为顺序的数据访问，这对于 IO-bound 类型的 SQL 查询语句可带来性能极大的提升。Multi-RangeRead 优化可适用于 range、ref、eq_ref 类型的查询。\nMulti-Range Read 的好处：\nMRR 使数据访问变得较为顺序。在查询辅助索引时，首先根据得到的查询结果，按照主键进行排序，并按照主键排序的顺序进行书签查找。 减少缓冲池中页被替换的次数。 批量处理对键值的查询操作 MRR 的工作方式如下：\n将查询得到的辅助索引键值存放于一个缓存中，这时缓存中的数据是根据辅助索引键值排序的。 将缓存中的键值根据 RowID 进行排序。 根据 RowID 的排序顺序来访问实际的数据文件。 MySQL5.6版本开始支持 Multi-Range Read（MRR）优化，通过参数 optimizer_switch 的标记来控制是否使用 MRR，当设置mrr=on 时，表示启用MRR优化。mrr_cost_based 表示是否通过 cost base 的方式来启用 MRR。如果选择 mrr=on,mrr_cost_based=off,则表示总是开启 MRR 优化。\nIndex Condition Pushdown（ICP）优化 MySQL 数据库会在取出索引的同时，判断是否可以进行 WHERE 条件的过滤，也就是将 WHERE 的部分过滤操作放在了存储引擎层。在某些查询下，可以大大减少上层 SQL 层对记录的索取（fetch），从而提高数据库的整体性能，优化支持 range、ref、eq_ref、ref_or_null 类型的查询，选择 Index Condition Pushdown 优化时，可在执行计划的列 Extra 看到 Using index condition 提示。\n当 SQL 需要全表访问时，ICP 的优化策略可用于 range, ref, eq_ref, ref_or_null 类型的访问数据方法。 支持 InnoDB 和 MyISAM 表。 ICP 只能用于二级索引，不能用于主索引。 并非全部 WHERE 条件都可以用ICP筛选，如果 WHERE 条件的字段不在索引列中，还是要读取整表的记录到 server 端做条件过滤。 索引实现的原理 InnoDB 存储的索引是基于 B+ 树实现的，不支持hash的实现方式。首先来了解下 B+ 树的特点；\nB+树的特征 有k个子树的中间节点包含有k个元素（B树中是k-1个元素），每个元素不保存数据，只用来索引，所有数据都保存在叶子节点。 所有的叶子结点中包含了全部元素的信息，及指向含这些元素记录的指针，且叶子结点本身依关键字的大小自小而大顺序链接。 所有的中间节点元素都同时存在于子节点，在子节点元素中是最大（或最小）元素。\nB+树的优势 单一节点存储更多的元素，使得查询的IO次数更少。 所有查询都要查找到叶子节点，查询性能稳定。 所有叶子节点形成有序链表，便于范围查询。 在B+树中，所有记录节点都是按键值的大小顺序存放在同一层的叶子节点上，由各叶子节点指针进行连接。先来看一个B+树，其高度为 2，每页可存放4条记录，扇出（fan out）为5，如下图所示：\n索引的设计思考 索引是一种存储方式，最相关的硬件就是磁盘，索引磁盘的性能会直接影响到数据库的查询效率 磁盘的性能和读写的顺序有关，普通磁盘顺序读写比随机读写快很多，所以尽量避免随机读写。 数据都是以行为单位一行一行的存储的，每一行都包括了所有的列，多行可以连续存储。 每一行数据中，一般都有一个键，其他的列可以称为值，可以理解为键值对。innodb必须有唯一非空的主键，就是默认的键。 在键值对中，键值可以排序，还可以组合键值。 索引的设计 磁盘空间会划分为许多个大小相等的块或者页，一个页中可以存储多行数据，这样就可以符合磁盘的顺序读写，这样一次IO就可以读取很多数据到内存，可以减少磁盘IO。 在一个页内，所有的数据可能会经常变动，并且大小也是相对固定的，所以内部通过链表或者数组管理。 每个键值可以排序，所以在一个块内的所有数据也可以是有序的，这样通过二分法查找可以很快的在一个页内找到指定键对应的数据 一个页设计好之后，可以把页作为B+树的节点，通过页来承载数据，通过B+数来组织不同页之间的关系 B+树的特点是在内节点存储键来提高搜索的性能，所以很自然的，内节点用来存储数据行的键，叶子节点存储所有数据行，可以很好的提升性能 接下来结合2.5节的聚集索引和二级索引来说：\n表中数据按照主键顺序存放。而聚集索引（clustered index）就是按照每张表的主键构造一棵B+树，同时叶子节点中存放的即为整张表的行记录数据，也将聚集索引的叶子节点称为数据页。聚集索引的这个特性决定了索引组织表中数据也是索引的一部分。同B+树数据结构一样，每个数据页都通过一个双向链表来进行链接。如下图所示：\n上图所示的是一个深度为2的B+树，也是我们所称的索引，这里假设页有随机唯一的编号，根页号为20。这里只有一个内节点（根节点），其他的都是叶子节点，也是数据节点，对于内节点来说，存有key和pageno的指针信息，对于叶子节点来说，只存有完整的数据。对于聚集索引，data部分存有除主键外的其他列的组合，如果是二级索引，则这里存放就是这行记录对应主键的组合，用于回表。\n最左边的MIN为了很好的组织树形结构的指针，和其他的内节点一样，主要用来标记它是最小记录Min，还有就是一个pageno指针指向下层最左边的Min记录，其他节点的Min记录用于判断搜索是否到了边界。每个页都有页头页尾用来管理和标记页面的状态，页面中的数据是如何存储，有没有空闲的空间，以什么样的顺序存储等。\n上图中所有的叶子节点从左到右都是从小到大的顺序以双向链表的方式存储的，所以当我们需要遍历全部的数据，只需要通过B+树找到最小的位置，然后通过遍历链表则可以查询到所有的数据，还有就是10,16,25这三条记录在内节点和叶子节点均存在，这既是B+数的特点，叶子节点会存有所有的key和值。而内节点只存储了key，不存储其他的数据，只有用来索引。叶子节点除了第一条记录会有上一层重复的存储，其他数据不会有这样的现象，所以浪费的空间也不大，由于每一个页的大小是固定的（16k），在内节点上只存储key，不存储其他数据，一个页就可以存储更多的key，这样检索也能减少磁盘的IO，由于页存储Key增多，这样就可以使得B+树的深度减少，这样也可以减少磁盘的IO，提高查询性能。\n例如一个三层的B+数，每一个页能存1000个key，所以第二层就有1000*（1+1000）个key，第三层就可以有100010011001=1002001000（十亿级别），一个简单的三层B+数据就可以存十亿级别的数据，很强大。\n上面说到的“回表”其实就是在使用二级索引进行搜索时，因为二级索引只保存了部分列的数据，如果需要获取键值不包括的列的数据时，需要通过二级索引的指针（书签：用于指向聚集索引的指针）来找到聚集索引的全部数据，然后返回需要查询的列的值。如果使用二级索引不能找到需要的值（需要回表），称为非覆盖索引，否则为2.6节介绍的覆盖索引。非覆盖索引需要回表，增加IO，所以性能会差一些。所以可以根据业务需求创建组合索引来避免回表。但是也要权衡索引带来的利是否大于弊。所以在统计行总数的时候可以通过二级索引来统计，这样速度会快一些。大概图形如下：\n这里附带的说一些不能走索引的情况，但是不多说，因为优化这个东西太多，后期准备写一两篇优化的文章，所以这里只是提一下，走索引的强大；虽然可能创建了很多索引，很多情况都不走索引，比如：like \u0026lsquo;%query_name%\u0026rsquo; ，where端使用or条件连接，where端使用函数等，在group by和order by使用的时候要注意组合索引的最左前缀原则。\n引用 copy from https://www.cnblogs.com/zsql/p/13808417.html#_label2\n","date":"2022-11-06T00:00:00Z","permalink":"https://ggxxll.github.io/p/mysql-%E7%B4%A2%E5%BC%95/","title":"Mysql 索引"},{"content":"根据加锁的范围，MySQL 里面的锁大致可以分成全局锁、表级锁和行锁三类\n全局锁 全局锁就是对整个数据库实例加锁。MySQL提供了一个加全局读锁的方法，命令是 Flush tables with read lock 。当需要让整个库处于只读状态的时候，可以使用这个命令，之后其他线程的以下语句会被阻塞：数据更新语句（数据的增删改）、数据定义语句（包括建表、修改表结构等）和更新类事务的提交语句\n全局锁的典型使用场景是，做全库逻辑备份。\n但是整个库都只读，可能出现以下问题：\n如果在主库上备份，那么在备份期间都不能执行更新，业务基本上就得停摆 如果在从库上备份，那么在备份期间从库不能执行主库同步过来的binlog，会导致主从延迟 在可重复读隔离级别下开启一个事务能够拿到一致性视图\n官方自带的逻辑备份工具是 mysqldump。当 mysqldump 使用参数 –single-transaction 的时候，导数据之前就会启动一个事务，来确保拿到一致性视图。而由于 MVCC 的支持，这个过程中数据是可以正常更新的。single-transaction 只适用于所有的表使用事务引擎的库\n1.既然要全库只读，为什么不使用set global readonly=true的方式？\n在有些系统中，readonly 的值会被用来做其他逻辑，比如用来判断一个库是主库还是备库。因此修改 global 变量的方式影响面更大 在异常处理机制上有差异。如果执行Flush tables with read lock命令之后由于客户端发生异常断开，那么MySQL会自动释放这个全局锁，整个库回到可以正常更新的状态。而将整个库设置为 readonly 之后，如果客户端发生异常，则数据库会一直保持 readonly 状态，这样会导致整个库长时间处于不可写状态，风险较高 表级锁 MySQL里面表级别的锁有两种：一种是表锁，一种是元数据锁（meta data lock，MDL）\n表锁 表锁的语法是 lock tables … read/write。可以用 unlock tables 主动释放锁，也可以在客户端断开的时候自动释放。lock tables 语法除了会限制别的线程的读写外，也限定了本线程接下来的操作对象\n如果在某个线程 A 中执行 lock tables t1 read,t2 wirte;这个语句，则其他线程写 t1、读写 t2 的语句都会被阻塞。同时，线程 A 在执行 unlock tables 之前，也只能执行读 t1、读写 t2 的操作，连写t1都不允许\n元数据锁 MDL 不需要显式使用，在访问一个表的时候会被自动加上。MDL 的作用是，保证读写的正确性。如果一个查询正在遍历一个表中的数据，而执行期间另一个线程对这个表结构做了变更，删了一列，那么查询线程拿到的结果跟表结构对不上，肯定不行\n在MySQL5.5版本引入了MDL，当对一个表做增删改查操作的时候，加MDL读锁；当要对表做结构变更操作的时候，加MDL写锁\n读锁之间不互斥，因此可以多个线程同时对一张表增删改查 读写锁之间、写锁之间是互斥的，用来保证变更表结构操作的安全性。 事务中的 MDL 锁，在语句执行开始时申请，但是语句结束后并不会马上释放，而会等到整个事务提交后再释放\n如果安全地给小表加字段？\n首先要解决长事务，事务不提交，就会一直占着 DML 锁。在 MySQL 的 information_schema 库的 innodb_trx 表中，可以查到当前执行的事务。如果要做DDL变更的表刚好有长事务在执行，要考虑先暂停 DDL，或者 kill 掉这个长事务\n如果要变更的表是一个热点表，虽然数据量不大，但是上面的请求很频繁，而又不得不加个字段，该怎么做？\n在 alter table 语句里面设定等待时间，如果在这个指定的等待时间里面能够拿到 MDL 写锁最好，拿不到也不要阻塞后面的业务语句，先放弃，之后再通过重试命令重复这个过程\n行锁 MySQL的行锁是在引擎层由各个引擎自己实现的。但不是所有的引擎都支持行锁，比如 MyISAM 引擎就不支持行锁\n行锁就是针对数据表中行记录的锁。比如事务 A 更新了一行，而这时候事务 B 也要更新同一行，则必须等事务 A 的操作完成后才能进行更。事务 A 持有的两个记录的行锁都是在 commit 的时候才释放的，事务 B 的 update 语句会被阻塞，直到事务 A 执行 commit 之后，事务 B 才能继续执行\n两阶段锁协议 在 InnoDB 事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。这个就是两阶段锁协议\n如果事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放\n死锁和死锁检测 在并发系统中不同线程出现循环资源依赖，涉及的线程都在等待别的线程释放资源时，就会导致这几个线程都进入无限等待的状态，称为死锁\n当出现死锁以后，有两种策略：\n一种策略是，直接进入等待，直到超时。这个超时时间可以通过参数 innodb_lock_wait_timeout 来设置 另一种策略是，发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数innodb_deadlock_detect 设置为 on，表示开启这个逻辑 在 InnoDB 中，innodb_lock_wait_timeout的默认值是 50s，意味着如果采用第一个策略，当出现死锁以后，第一个被锁住的线程要过 50s 才会超时退出，然后其他线程才有可能继续执行。对于在线服务来说，这个等待时间往往是无法接受的\n正常情况下还是要采用主动死锁检查策略，而且 innodb_deadlock_detect 的默认值本身就是on。主动死锁监测在发生死锁的时候，是能够快速发现并进行处理的，但是它有额外负担的。每当一个事务被锁的时候，就要看看它所依赖的线程有没有被别人锁住，如此循环，最后判断是否出现了循环等待，也就是死锁\n如果所有事务都要更新同一行的场景，每个新来的被堵住的线程都要判断会不会由于自己的加入导致死锁，这是一个时间复杂度是 O(n) 的操作\n怎么解决由这种热点行更新导致的性能问题？\n如果确保这个业务一定不会出现死锁，可以临时把死锁检测关掉 控制并发度 将一行改成逻辑上的多行来减少锁冲突。以影院账户为例，可以考虑放在多条记录上，比如10个记录，影院的账户总额等于这10个记录的值的总和。这样每次要给影院账户加金额的时候，随机选其中一条记录来加。这样每次冲突概率变成员原来的1/10，可以减少锁等待个数，也就减少了死锁检测的CPU消耗 引用 https://blog.csdn.net/qq_40378034/article/details/90904573\n","date":"2022-11-06T00:00:00Z","permalink":"https://ggxxll.github.io/p/mysql-%E9%94%81%E4%BA%8C/","title":"Mysql 锁（二）"},{"content":"概述 锁机制用于管理对共享资源的并发访问。\n锁可大致分为两类：\nlock：对象是事务，用来锁定的是数据库中的对象，如表、页、行。并且一般lock的对象仅在事务 commit 或者 rollback 后进行释放。有死锁检测机制。 latch：闩锁（shuang suo），其要求锁定的时间必须非常短。若持续的时间长，则应用的性能会非常差。在 InnoDB 存储引擎中，latch 又分为 mutex 互斥锁 和 rwLock 读写锁。 其目的是为了保证并发线程操作临界资源的正确性。通常没有死锁的检测机制。 锁的类型 共享锁，排他锁 InnoDB存储引擎实现了如下两种标准的行级锁：\n共享锁（S Lock）：允许事务读取一行数据。\n排他锁（X Lock）：允许事务删除或者更新一行数据。\n这里可以理解为读写锁，可并行读，但是不可以：并行写、并行读写。\n排它锁是很强的锁，不与其他类型的锁兼容。这也很好理解，修改和删除某一行的时候，必须获得强锁，禁止这一行上的其他并发，以保障数据的一致性。\n记录锁 Record Lock，仅锁定一行记录（如共享锁、排他锁）\n记录锁总是会去锁定索引记录，如果表在建立的时候，没有设置任何索引，InnoDB 会使用隐式的主键进行锁定。 查询条件的列是唯一索引的情况下，临键锁退化为记录锁。\n间隙锁 Gap Lock，锁定一个范围，但不包括记录本身。 关闭间隙锁的两种方式：\n将事务隔离级别设置为 读已提交（read committed） 将参数 innodb_locks_unsafe_for_binlog 设置为 1 在上述配置下，除了外键和唯一性检查依然需要间隙锁，其余情况均使用行锁进行锁定。\n临键锁 Next-Key Lock，等于记录锁+间隙锁，锁定一个范围，并锁定记录本身.\n主要是阻止多个事务将记录插入到同一个范围内，从而避免幻读。\n当查询的索引是唯一索引时，InnoDB 会将临键锁优化为记录锁，从而提高并发。这时候，将不再使用间隙锁来避免幻读。\n意向锁 事务可能要加共享/排他锁，先提前声明一个意向。\nInnoDB 支持多粒度锁定，这种锁定允许事务在行级上的锁和表级上的锁同时存在。为了支持不同粒度上进行加锁操作，InnoDB 存储引擎支持一种额外的锁方式， 称之为意向锁。意向锁是将锁定的对象分为多个层次，意向锁意味着事务希望在更粗的粒度上进行加锁。\n特点：\n意向锁是表级别锁 意向锁可分为： 意向共享锁（intention shared lock, IS）：表示事务有意向对表中的某些行加共享锁 意向排它锁（intention exclusive lock, IX）：表示事务有意向对表中的某些行加排它锁 意向锁协议： 事务要获得某些行的共享锁，必须先获得表的意向共享锁 IS 事务要获取某些行的排他锁，必须先获得表的意向排他锁 IX 由于意向锁仅表明意向，所以是比较弱的锁，意向锁之间不互斥，可以并行 意向锁之间可以并行，但与共享锁和排它锁互斥，表现为 S+IS 可并行读，其他均互斥。 插入意向锁 对已有数据行的修改与删除，必须使用排他锁，那对于数据的插入，是否还需要加这么强的锁，来实施互斥呢？插入意向锁，孕育而生。\n插入意向锁，是间隙锁（Gap Locks）的一种（所以，也是实施在索引上的），它是专门针对 INSERT 操作的。\n它的用处是：多个事务，在同一个索引上插入记录时，如果插入的位置不冲突，不会阻塞彼此。\n自增锁 自增锁是 MySQL一种特殊的锁，如果表中存在自增字段，MySQL便会自动维护一个自增锁。\n在 InnoDB 存储引擎的内存结构中，对每个含有自增长值的表都有一个自增长计数器。当对含有自增长计数器的表进行插入操作时，这个计数器会被初始化，执行如下操作来得到计数器的值：\nselect max(auto_inc_col) from t for update\n插入操作会依据这个自增长的计数器值加 1 赋予自增长列。这个实现方式称为 Auto-Inc Locking。这种锁其实是采用一种表锁的机制，为了提高插入的性能，自增长锁不是在一个事务完成以后才释放，而是在完成自增长值插入的SQL后立即释放。\n虽然 Auto-Inc Locking 从一定程度上提高了并发插入的效率，但还是存在一些性能上的问题。对于自增长列的并发插入性能较差，事务必须等待前一个插入的完成（虽然不用等待事务的完成）。\n从 MySQL5.12 版本开始，InnoDB 存储引擎提供了一种轻量级互斥量的自增长实现方式。这种方式大大提高了自增长值插入的性能。并且从该版本开始，InnoDB 存储引起提供了一个参数 innodb_innodb_autoinc_lock_mode 来控制自增长模式，该参数的默认值为 1\n自增长的插入分类:\ninsert like: 指所有的插入语句，如 INSERT、REPLACE、INSERT\u0026hellip;SELECT、RPELACE\u0026hellip;SELECT、LOAD DATA等 simple inserts：指能在插入前就确定插入行数的语句，如 INSERT、REPLACE 等。需要注意的是，不包含 INSERT\u0026hellip;ON DUPLICATE KEY UPDATE 这类语句。 bulk inserts：指在插入前不能确定插入行数的语句，如 INSERT\u0026hellip;SELECT、RPELACE\u0026hellip;SELECT、LOAD DATA等 mixed-mode inserts：指插入中有一部分的值是自增长的，有一部分是确定的。如 INSERT INTO T1 (id,name) VALUES (1,\u0026lsquo;A\u0026rsquo;),( NULL,\u0026lsquo;B\u0026rsquo;)；也可以指 INSERT\u0026hellip;ON DUPLICATE KEY UPDATE 这类语句。 innodb_innodb_autoinc_lock_mode 参数值的说明：\n0： 它提供了一个向后兼容的能力 在这一模式下，所有的 insert 语句(\u0026ldquo;insert like\u0026rdquo;) 都要在语句开始的时候得到一个表级的 Auto-Inc Locking 锁，在语句结束的时候才释放这把锁，注意呀，这里说的是语句级而不是事务级的，一个事务可能包涵有一个或多个语句。 它能保证值分配的可预见性，与连续性，可重复性，这个也就保证了 insert 语句在复制到 slave 的时候还能生成和master那边一样的值(它保证了基于语句复制的安全)。 由于在这种模式下 Auto-Inc Locking 锁一直要保持到语句的结束，所以这个就影响到了并发的插入。 1: 这一模式下去 simple insert 做了优化，由于simple insert一次性插入值的个数可以立马得到 确定，所以mysql可以一次生成几个连续的值，用于这个insert语句；总的来说这个对复制也是安全的(它保证了基于语句复制的安全) 这一模式也是mysql的默认模式，这个模式的好处是 Auto-Inc Locking 锁不要一直保持到语句的结束，只要语句得到了相应的值后就可以提前释放锁 2: 由于这个模式下已经没有了 Auto-Inc Locking 锁，所以这个模式下的性能是最好的；但是它也有一个问题，就是对于同一个语句来说它所得到的 auto_incremant 值可能不是连续的。 InnoDB 存储引擎中自增长的实现和 MyISAM 不同。MyISAM 存储引擎是表锁设计，自增长不用考虑并发插入的问题。在 InnoDB 存储引擎中，自增长值的列必须是索引，同时必须是索引的第一个列，如果不是第一个列，则 MySQL 会抛出异常。MyISAM 存储引擎没有这个问题。\n","date":"2022-11-05T00:00:00Z","permalink":"https://ggxxll.github.io/p/mysql-%E9%94%81%E4%B8%80/","title":"Mysql 锁（一）"},{"content":"概述 ACID，是指数据库管理系统（DBMS）在写入或更新资料的过程中，为保证事务（transaction）是正确可靠的，所必须具备的四个特性： 原子性（atomicity [ˌætəˈmɪsəti]， 或称不可分割性）、一致性（consistency [kənˈsɪstənsi]）、 隔离性（isolation [ˌaɪsəˈleɪʃn]，又称独立性）、持久性（durability [ˌdjʊərəˈbɪlɪti]）。\n在数据库系统中，一个事务是指：由一系列数据库操作组成的一个完整的逻辑过程。例如银行转帐，从原账户扣除金额，以及向目标账户添加金额，这两个数据库操作的总和，构成一个完整的逻辑过程，不可拆分。这个过程被称为一个事务，具有ACID特性。\n特性 Atomicity（原子性）：一个事务（transaction）中的所有操作，要么全部完成，要么全部不完成，不会结束在中间某个环节。事务在执行过程中发生错误，会被恢复（Rollback）到事务开始前的状态，就像这个事务从来没有执行过一样。 Consistency（一致性）：在事务开始之前和事务结束以后，数据库的完整性没有被破坏。这表示写入的资料必须完全符合所有的预设规则，这包含资料的精确度、串联性以及后续数据库可以自发性地完成预定的工作。 Isolation（隔离性）：数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。事务隔离分为不同级别，包括读未提交（Read uncommitted）、读提交（read committed）、可重复读（repeatable read）和串行化（Serializable）。 Durability（持久性）：事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。 事务并发时可能出现的问题 脏读（Dirty Read） A 事务读到了 B 未提交事务修改过的数据\n不可重复读（Non-Repeatable Read） A 事务只能读到 B 已经提交的事务修改过的数据，并且其他事务每对该数据进行一次修改并提交后，A 事务都能查询得到最新值。 （不可重复读在读未提交和读已提交隔离级别都可能会出现）\n幻读（Phantom） A 事务先根据某些条件查询出一些记录，之后 B 事务又向表中插入了符合这些条件的记录，A 事务再次按照该条件查询时，能把 B 事务插入的记录也读出来。 （幻读在读未提交、读已提交、可重复读隔离级别都可能会出现）\nMysql事务的隔离级别 MySQL的事务隔离级别一共有四个，分别是读未提交、读已提交、可重复读以及可串行化。\nMySQL的隔离级别的作用就是让事务之间互相隔离，互不影响，这样可以保证事务的一致性。\n隔离级别比较：可串行化 \u0026gt; 可重复读 \u0026gt; 读已提交 \u0026gt; 读未提交\n隔离级别对性能的影响比较：可串行化 \u0026gt; 可重复读 \u0026gt; 读已提交 \u0026gt; 读未提交\n由此看出，隔离级别越高，所需要消耗的MySQL性能越大（如事务并发严重性），为了平衡二者，一般建议设置的隔离级别为可重复读，MySQL Innodb默认的隔离级别也是可重复读。\n读未提交（READ UNCOMMITTED） 在读未提交隔离级别下，事务 A 可以读取到事务 B 修改过但未提交的数据。\n可能发生脏读、不可重复读和幻读问题，一般很少使用此隔离级别。\n读已提交（READ COMMITTED） 在读已提交隔离级别下，事务 A 只能在事务 B 修改过并且已提交后才能读取到事务 B 修改的数据。\n读已提交隔离级别解决了脏读的问题，但可能发生不可重复读和幻读问题，一般很少使用此隔离级别。\n普通读是快照读； 加锁的select, update, delete等语句，除了在外键约束检查(foreign-key constraint checking)以及重复键检查(duplicate-key checking)时会封锁区间，其他时刻都只使用记录锁； 可重复读（REPEATABLE READ） 在可重复读隔离级别下，事务 A 只能在事务 B 修改过数据并提交后，A 也提交事务后，才能读取到事务 B 修改的数据。\n可重复读隔离级别解决了脏读和不可重复读的问题，但可能发生幻读问题。\n提问：为什么上了写锁（写操作），别的事务还可以读操作？\n因为InnoDB有MVCC机制（多版本并发控制），可以使用快照读，而不会被阻塞。\n普通的select使用快照读(snapshot read)，这是一种不加锁的一致性读(Consistent Nonlocking Read)，底层使用MVCC来实现，具体的原理在《InnoDB并发如此高，原因竟然在这？》中有详细的描述； 加锁的select(select ... in share mode | select ... for update), update, delete等语句，它们的锁，依赖于它们是否在唯一索引(unique index)上使用了唯一的查询条件(unique search condition)，或者范围查询条件(range-type search condition)： 在唯一索引上使用唯一的查询条件，会使用记录锁(record lock)，而不会封锁记录之间的间隔，即不会使用间隙锁(gap lock)与临键锁(next-key lock) 范围查询条件，会使用间隙锁与临键锁，锁住索引记录之间的范围，避免范围间插入记录，以避免产生幻影行记录，以及避免不可重复的读 可串行化（SERIALIZABLE） 各种问题（脏读、不可重复读、幻读）都不会发生，通过加锁实现（读锁和写锁）。 事务并发执行时，阻塞情况与读写锁相同，可同时读，不可以读写交叉。\n所有select语句都会被隐式的转化为 select ... in share mode.\n隔离级别比较 级别 脏读 不可重复读 幻读 读未提交 可能 可能 可能 读已提交 不会 可能 可能 可重复读 不会 不会 可能 串行读 不会 不会 不会 隔离级别的实现原理 使用MySQL的默认隔离级别（可重复读）来进行说明。\n每条记录在更新的时候都会同时记录一条回滚操作（回滚操作日志undo log）。同一条记录在系统中可以存在多个版本，这就是数据库的多版本并发控制（MVCC）。 即通过回滚（rollback操作），可以回到前一个状态的值。\n提问：回滚操作日志（undo log）什么时候删除？\nMySQL会判断当没有事务需要用到这些回滚日志的时候，回滚日志会被删除。\n提问：什么时候不需要了？\n当系统里么有比这个回滚日志更早的read-view的时候。\n相关操作 查看隔离级别 SHOW VARIABLES LIKE 'transaction_isolation'; SELECT @@transaction_isolation; 设置隔离级别 SET 命令 SET [GLOBAL|SESSION] TRANSACTION ISOLATION LEVEL level;\n1 2 3 4 5 6 7 其中level有4种值： level: { REPEATABLE READ | READ COMMITTED | READ UNCOMMITTED | SERIALIZABLE } 1.　GLOBAL\n只对执行完该语句之后产生的会话起作用 当前已经存在的会话无效 2.　SESSION\n只对当前会话中下一个即将开启的事务有效 下一个事务执行完后，后续事务将恢复到之前的隔离级别 该语句不能在已经开启的事务中间执行，会报错的 服务启动项命令 可以修改启动参数transaction-isolation的值\n比方说我们在启动服务器时指定了\u0026ndash;transaction-isolation=READ UNCOMMITTED，那么事务的默认隔离级别就从原来的REPEATABLE READ变成了READ UNCOMMITTED。\n","date":"2022-10-23T00:00:00Z","permalink":"https://ggxxll.github.io/p/mysql-acid/","title":"Mysql ACID"},{"content":"问题 order by 和 limit 造成优化器选择索引错误\n测试表 1 2 CREATE DATABASE `app`; USE app; 1 2 3 4 5 6 7 8 9 CREATE TABLE `users` ( `id` int NOT NULL AUTO_INCREMENT, `name` varchar(100) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci DEFAULT NULL, `age` int DEFAULT NULL, `created_at` timestamp NOT NULL, `deleted_at` timestamp NULL DEFAULT NULL, PRIMARY KEY (`id`), KEY `users_age_IDX` (`age`) USING BTREE, ) ENGINE=InnoDB AUTO_INCREMENT=0 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci; 填充测试数据 1 2 3 4 5 6 7 8 9 10 11 12 13 DROP PROCEDURE IF EXISTS addData; DELIMITER $$ $$ CREATE PROCEDURE addData(IN n int) BEGIN DECLARE i int default 1; while (i \u0026lt; n) DO INSERT INTO users(name, age, created_at, deleted_at) VALUES(CONCAT(\u0026#39;name\u0026#39;,i), i, now(), null); set i=i | 1; End while; END$$ DELIMITER ; 1 CALL addData(400) 查询 sql 以下查询中使用的索引正确使用了 users_age_IDX。 1 2 explain SELECT * from users u where age \u0026gt; 1000 limit 1; explain SELECT * from users u where age \u0026gt; 1000 order by id desc limit 2; id select_type table partitions type possible_keys key key_len ref rows filtered Extra 1 SIMPLE u range users_age_IDX,users_deleted_at_IDX users_age_IDX 5 1988 100.0 Using index condition; Using where 这个查询中使用的索引是主键, 数据量 1 explain SELECT * from users u where age \u0026gt; 100 order by id desc limit 1; id select_type table partitions type possible_keys key key_len ref rows filtered Extra 1 SIMPLE u index users_age_IDX,users_deleted_at_IDX PRIMARY 4 65 15.33 Using where; Backward index scan 原因 MySQL 优化器认为在 limit 值较小的情况下，走主键索引能够更快的找到那一条数据，并且如果走联合索引需要扫描索引后进行排序，而主键索引天生有序，所以优化器综合考虑，走了主键索引。\n","date":"2022-09-19T00:00:00Z","permalink":"https://ggxxll.github.io/p/mysql-order-by-%E5%92%8C-limit-%E8%AF%AD%E5%8F%A5%E5%AF%BC%E8%87%B4%E6%85%A2-sql/","title":"Mysql order by 和 limit 语句导致慢 sql"},{"content":"基础知识 map的概念 map的直观意思是映射，是\u0026lt;key, value\u0026gt; 对组成的抽象结构，且 key 不会重复。map 的底层实现一般有两种：\n搜索树（search tree），天然有序，平均/最差的写入/查找复杂度是O(logN) 哈希表（hash table），无序存储，平均的写入/查找复杂度是O(1)，最差是O(N) 底层 源码 在 golang 中，map 的底层实现是哈希表：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 const ( bucketCntBits = 3 bucketCnt = 1 \u0026lt;\u0026lt; bucketCntBits ) type hmap struct { count int // map中存入元素的个数， golang 中调用 len(map) 的时候直接返回该字段 flags uint8 // 状态标记位，通过与定义的枚举值进行\u0026amp;操作可以判断当前是否处于这种状态 B uint8 // 2^B 表示 bucket 的数量， B 表示取hash后多少位来做 bucket 的分组 noverflow uint16 // overflow bucket 的数量的近似数 hash0 uint32 // hash seed （hash 种子） 一般是一个素数 buckets unsafe.Pointer // 共有2^B个 bucket，但是如果没有元素存入，这个字段可能为nil oldbuckets unsafe.Pointer // 在扩容期间，将旧的 bucket 数组放在这里， 新 bucket 会是这个的两倍大; 非扩容状态，值为 nil nevacuate uintptr // 表示已经完成扩容迁移的bucket的指针， 地址小于当前指针的 bucket 已经迁移完成 extra *mapextra // 为了优化GC扫描而设计的。当 key 和 value 均不包含指针，并且都可以inline时使用。extra是指向 mapextra 类型的指针 } // A bucket for a Go map. type bmap struct { tophash [bucketCnt]uint8 } // 编译期间会给它加料，动态地创建一个新的结构： type bmap struct { // 长度为8的数组，用来快速定位 key 是否在桶里 topbits [8]uint8 keys [8]keytype values [8]valuetype pad uintptr // 内存对齐使用，可能不需要 overflow uintptr // 当 bucket 的8个key 存满了之后 } hmap map 的底层结构是 hmap，hmap 包含若干个结构为 bmap 的 bucket 数组，每个 bucket 底层都采用链表结构。\nbmap bmap 就是我们常说的“桶”的底层数据结构， 一个桶中可以存放最多 8 个 key/value, map 使用 hash 函数 得到 hash 值决定分配到哪个桶， 然后又会根据 hash 值的高 8 位来寻找放在桶的哪个位置。具体的 map 的组成结构如下图所示：\nbmap 中 key 和 value 是各自放在一起的，并不是 key/value/key/value/\u0026hellip; 这样的形式。源码里说明这样的好处是在某些情况下可以省略掉 padding 字段，节省内存空间。\nmapextra 1 2 3 4 5 6 7 8 9 10 11 12 13 14 // mapextra holds fields that are not present on all maps. type mapextra struct { // 如果 key 和 value 都不包含指针，并且可以被 inline(\u0026lt;=128 字节) // 就使用 hmap 的 extra 字段 来存储 overflow buckets，这样可以避免 GC 扫描整个 map // 然而 bmap.overflow 也是个指针。这时候我们只能把这些 overflow 的指针 // 都放在 hmap.extra.overflow 和 hmap.extra.oldoverflow 中了 // overflow 包含的是 hmap.buckets 的 overflow 的 buckets // oldoverflow 包含扩容时的 hmap.oldbuckets 的 overflow 的 bucket overflow *[]*bmap oldoverflow *[]*bmap // 指向空闲的 overflow bucket 的指针 nextOverflow *bmap } 特性 引用类型 map是个指针，底层指向hmap，所以是个引用类型。 golang 有三个常用的高级类型slice、map、channel，它们都是引用类型，当引用类型作为函数参数时，可能会修改原内容数据。 golang 中没有引用传递，只有值和指针传递。所以 map 作为函数实参传递时本质上也是值传递，只不过因为 map 底层数据结构是通过指针指向实际的元素存储空间， 在被调函数中修改 map，对调用者同样可见，所以 map 作为函数实参传递时表现出了引用传递的效果。 因此，传递 map 时，如果想修改map的内容而不是map本身，函数形参无需使用指针。\n1 2 3 4 5 6 7 8 9 10 11 12 func TestSliceFn(t *testing.T) { m := map[string]int{} t.Log(m, len(m)) // map[a:1] mapAppend(m, \u0026#34;b\u0026#34;, 2) t.Log(m, len(m)) // map[a:1 b:2] 2 } func mapAppend(m map[string]int, key string, val int) { m[key] = val } 非线程安全 map默认是并发不安全的，原因如下：\nGo 官方在经过了长时间的讨论后，认为 Go map 更应适配典型使用场景（不需要从多个 goroutine 中进行安全访问），而不是为了小部分情况（并发访问），导致大部分程序付出加锁代价（性能），决定了不支持。\n更改 map 时会检查 hmap 的标志位 flags。如果 flags 的写标志位此时被置 1 了，说明有其他协程在执行“写”操作，进而导致程序 panic。这也说明了 map 对协程是不安全的。\n共享存储空间 map 底层数据结构是通过指针指向实际的元素存储空间 ，这种情况下，对其中一个map的更改，会影响到其他map\n1 2 3 4 5 6 7 8 9 func TestMapShareMemory(t *testing.T) { m1 := map[string]int{} m2 := m1 m1[\u0026#34;a\u0026#34;] = 1 t.Log(m1, len(m1)) // map[a:1] 1 t.Log(m2, len(m2)) // map[a:1] } 哈希冲突 golang中map是一个kv对集合。底层使用hash table，用链表来解决冲突 ，出现冲突时，不是每一个key都申请一个结构通过链表串起来， 而是以bmap为最小粒度挂载，一个bmap可以放8个kv。在哈希函数的选择上，会在程序启动时，检测 cpu 是否支持 aes，如果支持，则使用 aes hash，否则使用 memhash。\n遍历无序 map 在没有被修改的情况下，使用 range 多次遍历 map 时输出的 key 和 value 的顺序可能不同。这是 Go 语言的设计者们有意为之， 在每次 range 时的顺序被随机化，旨在提示开发者们，Go 底层实现并不保证 map 遍历顺序稳定，请大家不要依赖 range 遍历结果顺序。\n扩容 扩容条件 再来说触发 map 扩容的时机：在向 map 插入新 key 的时候，会进行条件检测，符合下面这 2 个条件，就会触发扩容：\n1 2 3 4 5 6 // If we hit the max load factor or we have too many overflow buckets, // and we\u0026#39;re not already in the middle of growing, start growing. if !h.growing() \u0026amp;\u0026amp; (overLoadFactor(h.count+1, h.B) || tooManyOverflowBuckets(h.noverflow, h.B)) { hashGrow(t, h) goto again // Growing the table invalidates everything, so try again } 装载因子超过阈值\n源码里定义的阈值是 6.5 (loadFactorNum/loadFactorDen)，是经过测试后取出的一个比较合理的因子。 我们知道，每个 bucket 有 8 个空位，在没有溢出，且所有的桶都装满了的情况下，装载因子算出来的结果是 8。因此当装载因子超过 6.5 时， 表明很多 bucket 都快要装满了，查找效率和插入效率都变低了。在这个时候进行扩容是有必要的。\n对于条件 1，元素太多，而 bucket 数量太少，很简单：将 B 加 1，bucket 最大数量(2^B)直接变成原来 bucket 数量的 2 倍。于是，就有新老 bucket 了。\n注意，这时候元素都在老 bucket 里，还没迁移到新的 bucket 来。新 bucket 只是最大数量变为原来最大数量的 2 倍(2^B * 2) 。\noverflow 的 bucket 数量过多\n在装载因子比较小的情况下，这时候 map 的查找和插入效率也很低，而第 1 点识别不出来这种情况。表面现象就是计算装载因子的分子比较小， 即 map 里元素总数少，但是 bucket 数量多（真实分配的 bucket 数量多，包括大量的 overflow bucket）。\n不难想像造成这种情况的原因：不停地插入、删除元素。先插入很多元素，导致创建了很多 bucket，但是装载因子达不到第 1 点的临界值，未触发扩容来缓解这种情况。 之后，删除元素降低元素总数量，再插入很多元素，导致创建很多的 overflow bucket，但就是不会触发第 1 点的规定，你能拿我怎么办？ overflow bucket 数量太多，导致 key 会很分散，查找插入效率低得吓人，因此出台第 2 点规定。\n这就像是一座空城，房子很多，但是住户很少，都分散了，找起人来很困难。\n对于条件 2，其实元素没那么多，但是 overflow bucket 数特别多，说明很多 bucket 都没装满。解决办法就是开辟一个新 bucket 空间， 将老 bucket 中的元素移动到新 bucket，使得同一个 bucket 中的 key 排列地更紧密。这样，原来，在 overflow bucket 中的 key 可以移动到 bucket 中来。 结果是节省空间，提高 bucket 利用率，map 的查找和插入效率自然就会提升。\n扩容函数 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 func hashGrow(t *maptype, h *hmap) { // If we\u0026#39;ve hit the load factor, get bigger. // Otherwise, there are too many overflow buckets, // so keep the same number of buckets and \u0026#34;grow\u0026#34; laterally. bigger := uint8(1) if !overLoadFactor(h.count+1, h.B) { bigger = 0 h.flags |= sameSizeGrow } oldbuckets := h.buckets newbuckets, nextOverflow := makeBucketArray(t, h.B+bigger, nil) flags := h.flags \u0026amp;^ (iterator | oldIterator) if h.flags\u0026amp;iterator != 0 { flags |= oldIterator } // commit the grow (atomic wrt gc) h.B += bigger h.flags = flags h.oldbuckets = oldbuckets h.buckets = newbuckets h.nevacuate = 0 h.noverflow = 0 if h.extra != nil \u0026amp;\u0026amp; h.extra.overflow != nil { // Promote current overflow buckets to the old generation. if h.extra.oldoverflow != nil { throw(\u0026#34;oldoverflow is not nil\u0026#34;) } h.extra.oldoverflow = h.extra.overflow h.extra.overflow = nil } if nextOverflow != nil { if h.extra == nil { h.extra = new(mapextra) } h.extra.nextOverflow = nextOverflow } // the actual copying of the hash table data is done incrementally // by growWork() and evacuate(). } 由于 map 扩容需要将原有的 key/value 重新搬迁到新的内存地址，如果有大量的 key/value 需要搬迁，会非常影响性能。因此 Go map 的扩容采取了一种称为“渐进式”的方式，原有的 key 并不会一次性搬迁完毕，每次最多只会搬迁 2 个 bucket。\n上面说的 hashGrow() 函数实际上并没有真正地“搬迁”，它只是分配好了新的 buckets，并将老的 buckets 挂到了 oldbuckets 字段上。真正搬迁 buckets 的动作在 growWork() 函数中，而调用 growWork() 函数的动作是在 mapassign 和 mapdelete 函数中。也就是插入或修改、删除 key 的时候，都会尝试进行搬迁 buckets 的工作。先检查 oldbuckets 是否搬迁完毕，具体来说就是检查 oldbuckets 是否为 nil。\n迁移 1 2 3 4 5 6 7 8 9 10 func growWork(t *maptype, h *hmap, bucket uintptr) { // make sure we evacuate the oldbucket corresponding // to the bucket we\u0026#39;re about to use evacuate(t, h, bucket\u0026amp;h.oldbucketmask()) // evacuate one more oldbucket to make progress on growing if h.growing() { evacuate(t, h, h.nevacuate) } } 迁移条件 如果未迁移完毕，赋值/删除的时候，扩容完毕后（预分配内存），不会马上就进行迁移。而是采取增量扩容的方式，当有访问到具体 bukcet 时，才会逐渐的进行迁移（将 oldbucket 迁移到 bucket）\n迁移函数 hmap.nevacuate 标识的是当前的进度，如果都搬迁完，应该和2^B的长度是一样的 在evacuate 方法实现是把这个位置对应的 bucket，以及其冲突链上的数据都转移到新的 bucket 上。\n先要判断当前 bucket 是不是已经转移。 (oldbucket 标识需要搬迁的 bucket 对应的位置)。转移的判断直接通过tophash 就可以，判断 tophash 中第一个 hash 值即可\n1 2 3 4 5 6 7 8 9 10 var ( emptyOne = 1 // this cell is empty minTopHash = 5 // minimum tophash for a normal filled cell. ) func evacuated(b *bmap) bool { h := b.tophash[0] return h \u0026gt; emptyOne \u0026amp;\u0026amp; h \u0026lt; minTopHash } 如果没有被转移，那就要迁移数据了。数据迁移时，可能是迁移到大小相同的 buckets 上，也可能迁移到2倍大的 buckets 上。这里 xy 都是标记目标迁移位置的标记：x 标识的是迁移到相同的位置，y 标识的是迁移到2倍大的位置上。我们先看下目标位置的确定：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 var xy [2]evacDst x := \u0026amp;xy[0] x.b = (*bmap)(add(h.buckets, oldbucket*uintptr(t.bucketsize))) x.k = add(unsafe.Pointer(x.b), dataOffset) x.e = add(x.k, bucketCnt*uintptr(t.keysize)) if !h.sameSizeGrow() { // Only calculate y pointers if we\u0026#39;re growing bigger. // Otherwise GC can see bad pointers. y := \u0026amp;xy[1] y.b = (*bmap)(add(h.buckets, (oldbucket+newbit)*uintptr(t.bucketsize))) y.k = add(unsafe.Pointer(y.b), dataOffset) y.e = add(y.k, bucketCnt*uintptr(t.keysize)) } 确定bucket位置后，需要按照kv 一条一条做迁移。\n如果当前搬迁的bucket 和 总体搬迁的bucket的位置是一样的，我们需要更新总体进度的标记 nevacuate\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 func advanceEvacuationMark(h *hmap, t *maptype, newbit uintptr) { h.nevacuate++ // Experiments suggest that 1024 is overkill by at least an order of magnitude. // Put it in there as a safeguard anyway, to ensure O(1) behavior. stop := h.nevacuate + 1024 if stop \u0026gt; newbit { stop = newbit } for h.nevacuate != stop \u0026amp;\u0026amp; bucketEvacuated(t, h, h.nevacuate) { h.nevacuate++ } if h.nevacuate == newbit { // newbit == # of oldbuckets // Growing is all done. Free old main bucket array. h.oldbuckets = nil // Can discard old overflow buckets as well. // If they are still referenced by an iterator, // then the iterator holds a pointers to the slice. if h.extra != nil { h.extra.oldoverflow = nil } h.flags \u0026amp;^= sameSizeGrow } } 总结 map是引用类型 map遍历是无序的 map是非线程安全的 map的哈希冲突解决方式是链表法 map的扩容不是一定会新增空间，也有可能是只是做了内存整理 map的迁移是逐步进行的，在每次赋值时，会做至少一次迁移工作 map中删除key，有可能导致出现很多空的kv，这会导致迁移操作，如果可以避免，尽量避免 ","date":"2022-08-25T00:00:00Z","permalink":"https://ggxxll.github.io/p/golang-map/","title":"Golang Map"},{"content":"概述 https://zhuanlan.zhihu.com/p/98135840\n链接 文档 在线体验 数据库引擎 Lazy\n会将最近一次查询的表保存在内存中, 过期时间为expiration_time_in_seconds. 仅适用于*Log表引擎. 如果表的数量较多, 数据量小, 且访问间隔长, 建议使用Lazy引擎数据库. Atomic\n不指定引擎时, 默认使用Atomic. 支持无阻塞删除和重命名表. 默认会生成UUID, 不建议指定UUID 不改变uuid和迁移表数据的话, 执行rename table可以立即生效 DROP/DETACH TABLE不会真正的删除表, 只是标记被删除了. 如果同步模式设置为SYNC, 在等待已有的查询/插入操作结束之后, 表会被立即删除. 使用表引擎ReplicatedMergeTree, 不建议指定path和replica name ReplicatedReplacingMergeTree('/clickhouse/tables/{shard}/{database}/{table}', '{replica}') Mysql\n远程映射Mysql的库表. 在clickhouse执行的查询等, 会被引擎转换, 发送到Mysql服务中. 不能执行 RENAME, CREATE TABLE, ALTER 简单 WHERE 条款如 =, !=, \u0026gt;, \u0026gt;=, \u0026lt;, \u0026lt;= 当前在MySQL服务器上执行。其余的条件和 LIMIT 只有在对MySQL的查询完成后，才会在ClickHouse中执行采样约束。 删除DROP和卸载DETACH 表，不会删除mysql中的表，且允许通过ATTACH再装载回来 不支持的Mysql的数据类型会转换成String, 所有类型都支持Nullable MaterializeMySQL\n使用MySQL中存在的所有表以及这些表中的所有数据创建ClickHouse数据库（即库级别的数据同步）。ClickHouse服务器用作MySQL副本。它读取binlog并执行DDL和DML查询。默认表引擎设置为ReplacingMergeTree\n使用表引擎 ReplacingMergeTree 时, 会添加虚拟列 _sign 和 _version\n_version — Transaction counter. Type [UInt64]\n_sign — Deletion mark. Type [Int8]. Possible values:\n1 — Row is not deleted, -1 — Row is deleted. MySQL DDL查询将转换为相应的ClickHouse DDL查询（ALTER，CREATE，DROP，RENAME）。如果ClickHouse无法解析某些DDL查询，则该查询将被忽略。\n不支持 INSERT, DELETE 和 UPDATE操作. 复制时都视为INSERT操作, 会在数据上使用_sign标记.\nINSERT, 1 DELETE, -1 UPDATE, 1或-1 执行SELECT查询时\n不指定_version, 默认使用FINAL, 即MAX(_version) 不指定_sign, 默认会限制查询条件WHERE _sign=1 注意事项:\n_sign=-1的数据是逻辑删除 不支持update/delete 复制过程很容易崩溃 禁止手动操作数据库和表 通过设置optimize_on_insert, 启用或禁用在插入之前进行数据转换，就像合并是在此块上完成的（根据表引擎）一样。 不支持的Mysql数据类型会异常Unhandled data type并停止复制， 都支持Nullable PostgreSQL 远程链接PostgreSQL. 支持查询, 插入等操作. 支持修改表结构 (注意使用缓存时的情况: DETACH 和ATTCH可刷新缓存). 数据类型中仅INTEGER支持Nullable 表引擎 文档 MergeTree Kakfa，Mysql 创建数据库 1 CREATE DATABASE [IF NOT EXISTS] db_name [ON CLUSTER cluster] [ENGINE = engine(...)] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster] ( name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1] [COMMENT expr1] [TTL expr1], name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2] [COMMENT expr1] [TTL expr2], ... INDEX index_name1 expr1 TYPE type1(...) GRANULARITY value1, INDEX index_name2 expr2 TYPE type2(...) GRANULARITY value2 ) ENGINE = MergeTree() -- 指定数据排序使用字段，默认等同主键 ORDER BY (expr1[, expr2,...]) -- 指定数据分区使用的字段 [PARTITION BY (expr1[, expr2,...])] -- 指定主键 [PRIMARY KEY (expr1[, expr2,...])] -- 指定采样数据使用的字段，启用数据采样时，不会对所有数据执行查询，而只对特定部分数据（样本）执行查询 [SAMPLE BY (expr1[, expr2,...])] [TTL expr [DELETE|TO DISK \u0026#39;xxx\u0026#39;|TO VOLUME \u0026#39;xxx\u0026#39; [, ...] ] [WHERE conditions] [GROUP BY key_expr [SET v1 = aggr_func(v1) [, v2 = aggr_func(v2) ...]] ] ] [SETTINGS name=value, ...] 字典 字典是一个映射 (键 -\u0026gt; 属性）, 是方便各种类型的参考清单。\nClickHouse支持一些特殊函数配合字典在查询中使用。 将字典与函数结合使用比将 JOIN 操作与引用表结合使用更简单、更有效。\n内置字典：处理地理数据库 外部字典：自定义字典 特殊查询 1 2 3 4 5 6 7 8 9 10 11 -- 计算pv值\u0026gt;20的记录条数 select sum(pv\u0026gt;20) from test_all; select count(*) from test_all where pv\u0026gt;20; -- 计算pv\u0026gt;20的记录条数占比：0.5 select avg(pv\u0026gt;20) from test_all; -- 计算pv在1,2，20这几个值中的记录条数的占比 select avg(pv in (1,2,2,20)) from test_all; -- Date系列类型， 插入时可以混合时间戳和字符串插入 INSERT INTO dt Values (1546300800, 1), (\u0026#39;2019-01-01\u0026#39;, 2); 数据类型 UUID FixedString Enum LowCardinality(T), 把其它数据类型转变为字典编码类型。如果该字段的值去重个数小于1W，性能优于普通类型。反之，性能下降。 通常， 一个字符串类型的Enum类型， 建议设置为LowCardinality， 更灵活和高效 Array(T\u0026hellip;) AggregateFunction， 适用于表引擎 AggregatingMergeTree Nested 嵌套字段， 不支持表引擎 MergeTree Tuple Nullable Domains： IPV4， IPV6 Geo： experimental feature Point ： 坐标，Tuple（x float64, y float64） Ring： Array(Point) Polygon： Array(Ring) MultiPolygon：Array(Polygon) Map(key,value) experimental feature SimpleAggregateFunction 性能优于 AggregateFunction， 适用于表引擎 AggregatingMergeTree 注意事项 使用 Nullable 几乎总是对性能产生负面影响，在设计数据库时请记住这一点, 最好设置default值。 删除表的时候， 会先在元数据标记被删除，exists、select、insert等会立即生效（表不存在）， 但是create table创建表等操作会抛异常（表存在）。 稍等几分钟，再新建表即可。 删除表数据，应当操作本地表，操作分布式Distributed表会报错。 ReplacingMergeTree表引擎， 完全相同的数据会直接去重， 数据是在后台不确定的时间去合并去重的。不建议使用OPTIMIZE发起合并， 使用 FINAL 关键字进行查询，可以得到主键相同的、最新被插入的数据。 那些有相同分区表达式值PARTITION BY的数据片段才会合并。这意味着 你不应该用太精细的分区方案（超过一千个分区）。否则，会因为文件系统中的文件数量过多和需要打开的文件描述符过多，导致 SELECT 查询效率不佳。可以根据group by语句来分析如何分区。绝对不能使用带有主键性质的字段做分区（比如唯一id）， 最好选择一个重复率高的字段（比如 日期， 渠道等）。 Alter语句DROP, MODIFY 时， 该字段不能是主键，会报错 使用 1 2 3 4 5 6 7 8 9 10 create table test on cluster cluster1 ( date Date, id UInt32 default 0 comment \u0026#39;123\u0026#39;, name String default \u0026#39;\u0026#39;, pv UInt32 default 0 ) ENGINE = ReplicatedReplacingMergeTree(\u0026#39;/clickhouse/tables/{shard}/{database}/{table}\u0026#39;, \u0026#39;{replica}\u0026#39;) PARTITION BY date ORDER BY (date,id); 1 2 3 4 5 6 7 8 create table test_all on cluster cluster1 ( date Date, id UInt32 default 0 comment \u0026#39;123\u0026#39;, name String default \u0026#39;\u0026#39;, pv UInt32 default 0 ) engine = Distributed(\u0026#39;cluster1\u0026#39;, \u0026#39;wangzhuan\u0026#39;, \u0026#39;test\u0026#39;, sipHash64(date)); 1 2 3 4 -- 结果是一条数据 insert into test_all (date, id, name, pv) VALUES (\u0026#39;2021-05-20\u0026#39;,1,\u0026#39;a\u0026#39;,10), (\u0026#39;2021-05-20\u0026#39;,1,\u0026#39;a\u0026#39;,10); 1 2 3 4 5 6 7 8 9 10 11 12 13 -- 结果是两条数据 insert into test_all (date, id, name, pv) VALUES (\u0026#39;2021-05-20\u0026#39;, 1, \u0026#39;a\u0026#39;, 10) , (\u0026#39;2021-05-20\u0026#39;, 1, \u0026#39;a\u0026#39;, 20); -- 结果是两条数据 30被20覆盖了， insert into test_all (date, id, name, pv) VALUES (\u0026#39;2021-05-20\u0026#39;, 1, \u0026#39;a\u0026#39;, 30) , (\u0026#39;2021-05-20\u0026#39;, 1, \u0026#39;a\u0026#39;, 20); -- 当然， 经过后台的数据合并之后， 应该是只剩1条数据 pv=30的 1 2 3 4 5 6 7 select * from test_all; select * from test_all final; select sum(pv) from test_all; select sum(pv) from test_all final; 1 2 3 alter table test on cluster cluster1 delete where 1; alter table test on cluster cluster1 delete where date=\u0026#39;2021-05-20\u0026#39;; 1 2 3 4 5 6 7 8 9 10 11 12 -- 仅当分区被使用到（PARTITION BY的字段的值超过一个），system.parts表里才会有数据 SELECT partition, name, active FROM system.parts WHERE table = \u0026#39;test\u0026#39;; SELECT table,count(*) FROM system.parts group by table order by count(*) desc; 1 2 drop table test on cluster cluster1; drop table test_all on cluster cluster1; ","date":"2022-08-24T00:00:00Z","permalink":"https://ggxxll.github.io/p/clickhouse%E5%88%9D%E4%BD%93%E9%AA%8C/","title":"Clickhouse初体验"},{"content":"环境 工具：docker，docker-compose 需求描述 在本地搭建业务中可能使用的服务，方便本地进行测试。\n配置文件 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 version: \u0026#34;3\u0026#34; services: zookeeper: container_name: zookeeper image: \u0026#39;bitnami/zookeeper:latest\u0026#39; ports: - \u0026#39;2181:2181\u0026#39; environment: - ALLOW_ANONYMOUS_LOGIN=yes kafka: container_name: kafka image: \u0026#39;bitnami/kafka:latest\u0026#39; ports: - \u0026#39;9092:9092\u0026#39; environment: - KAFKA_BROKER_ID=1 - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092 - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://127.0.0.1:9092 - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181 - ALLOW_PLAINTEXT_LISTENER=yes depends_on: - zookeeper volumes: - \u0026#39;kafka:/bitnami/kafka\u0026#39; redis: container_name: redis image: \u0026#39;redis:latest\u0026#39; ports: - \u0026#39;6379:6379\u0026#39; volumes: - \u0026#39;redis:/usr/local/etc/redis\u0026#39; mysql: container_name: mysql image: \u0026#39;mysql:latest\u0026#39; ports: - \u0026#39;3306:3306\u0026#39; environment: MYSQL_ALLOW_EMPTY_PASSWORD: \u0026#39;yes\u0026#39; MYSQL_DATABASE: app volumes: - \u0026#39;mysql:/var/lib/mysql\u0026#39; mongo: container_name: mongo image: \u0026#39;mongo:latest\u0026#39; ports: - \u0026#39;27017:27017\u0026#39; volumes: - \u0026#39;mongo:/data/db\u0026#39; etcd: container_name: etcd image: \u0026#39;quay.io/coreos/etcd:latest\u0026#39; ports: - \u0026#39;2379:2379\u0026#39; - \u0026#39;2380:2380\u0026#39; volumes: - \u0026#39;etcd:/etcd-data\u0026#39; command: - /usr/local/bin/etcd - --data-dir=/etcd-data - --name=node1 - --initial-advertise-peer-urls=http://192.168.82.116:2380 - --listen-peer-urls=http://0.0.0.0:2380 - --advertise-client-urls=http://192.168.82.116:2379 - --listen-client-urls=http://0.0.0.0:2379 - --initial-cluster - node1=http://192.168.82.116:2380 es: container_name: es image: docker.elastic.co/elasticsearch/elasticsearch:7.14.0 environment: - discovery.type=single-node ulimits: memlock: soft: -1 hard: -1 volumes: - es:/usr/share/elasticsearch/data ports: - \u0026#39;9200:9200\u0026#39; volumes: redis: {} mysql: {} kafka: {} es: {} mongo: {} etcd: {} ","date":"2022-08-24T00:00:00Z","permalink":"https://ggxxll.github.io/p/docker%E6%B5%8B%E8%AF%95%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/","title":"docker测试环境搭建"},{"content":"环境 系统: windows 语言: golang 框架: gin Bug描述 在如下代码中, ctx := c和ctx := context.Background()传递到cli.Set中是没有问题的.\n使用ctx := c.Request.Context()时, 会打印出错误: context canceled.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;github.com/gin-gonic/gin\u0026#34; \u0026#34;github.com/go-redis/redis/v8\u0026#34; ) func main() { cli := redis.NewClient(\u0026amp;redis.Options{ Addr: \u0026#34;127.0.0.1:6379\u0026#34;, }) r := gin.Default() r.GET(\u0026#34;/ping\u0026#34;, func(c *gin.Context) { //ctx := c //ctx := context.Background() ctx := c.Request.Context() go func() { if err := cli.Set(ctx, \u0026#34;hello\u0026#34;, 1, 0).Err(); err != nil { fmt.Println(err) } }() c.JSON(200, gin.H{ \u0026#34;message\u0026#34;: \u0026#34;pong\u0026#34;, }) }) r.Run() // listen and serve on 0.0.0.0:8080 } 分析 gin的Request对象来自*http.Request, handlerFunc被调用完成之后, context就被cancel了\n在ctx := c.Request.Context()打断点, 可以找到如下代码:\n1 2 3 4 5 6 func{ ... serverHandler{c.server}.ServeHTTP(w, w.req) w.cancelCtx() ... } 其他 使用gin+opentracing时, 不能使用 作为上下文传递, 要使用c.Request.Context(), 而不是gin.Context. 需要在goroutine中传递ctx, 应该拿出span新建一个ctx 1 2 3 4 func NewTracingContextWithParentContext(ctx context.Context) context.Context { span := opentracing.SpanFromContext(ctx) return opentracing.ContextWithSpan(context.Background(), span) } ginhttp中封装好了相应的中间件, 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 func Middleware(tr opentracing.Tracer, options ...MWOption) gin.HandlerFunc { opts := mwOptions{ opNameFunc: func(r *http.Request) string { return \u0026#34;HTTP \u0026#34; + r.Method }, spanObserver: func(span opentracing.Span, r *http.Request) {}, urlTagFunc: func(u *url.URL) string { return u.String() }, } for _, opt := range options { opt(\u0026amp;opts) } return func(c *gin.Context) { carrier := opentracing.HTTPHeadersCarrier(c.Request.Header) ctx, _ := tr.Extract(opentracing.HTTPHeaders, carrier) op := opts.opNameFunc(c.Request) sp := tr.StartSpan(op, ext.RPCServerOption(ctx)) ext.HTTPMethod.Set(sp, c.Request.Method) ext.HTTPUrl.Set(sp, opts.urlTagFunc(c.Request.URL)) opts.spanObserver(sp, c.Request) // set component name, use \u0026#34;net/http\u0026#34; if caller does not specify componentName := opts.componentName if componentName == \u0026#34;\u0026#34; { componentName = defaultComponentName } ext.Component.Set(sp, componentName) c.Request = c.Request.WithContext( opentracing.ContextWithSpan(c.Request.Context(), sp)) c.Next() ext.HTTPStatusCode.Set(sp, uint16(c.Writer.Status())) sp.Finish() } } ","date":"2022-08-24T00:00:00Z","permalink":"https://ggxxll.github.io/p/gin-opentracing%E7%9A%84%E4%BD%BF%E7%94%A8/","title":"gin+opentracing的使用"},{"content":"使用Kibana + Filebeat + Nginx 进行日志监控，Nginx的error日志的时区提前了8个小时，解决方案如下：\n原博： https://www.colabug.com/2018/0425/2780769/\n步骤一 将 /usr/share/filebeat/module/nginx/error/ingest/pipeline.json 里面的\n1 2 3 4 5 6 7 \u0026#34;date\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;nginx.error.time\u0026#34;, \u0026#34;target_field\u0026#34;: \u0026#34;@timestamp\u0026#34;, \u0026#34;formats\u0026#34;: [\u0026#34;yyyy/MM/dd H:m:s\u0026#34;], {\u0026lt; if .convert_timezone \u0026gt;}\u0026#34;timezone\u0026#34;: \u0026#34;{{ beat.timezone }}\u0026#34;,{\u0026lt; end \u0026gt;} \u0026#34;ignore_failure\u0026#34;: true } 改为：\n1 2 3 4 5 6 7 \u0026#34;date\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;nginx.error.time\u0026#34;, \u0026#34;target_field\u0026#34;: \u0026#34;@timestamp\u0026#34;, \u0026#34;formats\u0026#34;: [\u0026#34;yyyy/MM/dd H:m:s\u0026#34;], \u0026#34;timezone\u0026#34; : \u0026#34;Asia/Shanghai\u0026#34;, \u0026#34;ignore_failure\u0026#34;: true } 步骤二 1 2 3 4 5 6 7 8 9 删除旧的pipeline: curl -XDELETE \u0026#34;http://localhost:9200/_ingest/pipeline/filebeat-6.7.0-nginx-error-pipeline\u0026#34; # 重新设置 filebeat setup -e # 查看是否生效 curl -XGET \u0026#34;http://localhost:9200/_ingest/pipeline/filebeat-6.7.0-nginx-error-pipeline\u0026#34; 注意： 一般来说索引格式固定，修改版本号（6.7.0）为自己的安装版本\n","date":"2022-08-24T00:00:00Z","permalink":"https://ggxxll.github.io/p/kibana-filebeat-nginx%E7%9A%84error%E6%97%A5%E5%BF%97%E6%97%B6%E5%8C%BA%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/","title":"Kibana+Filebeat+Nginx的error日志时区问题解决方案"},{"content":"原贴: https://www.cnblogs.com/winkey4986/p/6433704.html\n\u0026ndash;数据库中单个表的大小（不包含索引）\n1 select pg_size_pretty(pg_relation_size(\u0026#39;表名\u0026#39;)); \u0026ndash;查出所有表（包含索引）并排序\n1 2 3 4 SELECT table_schema || \u0026#39;.\u0026#39; || table_name AS table_full_name, pg_size_pretty(pg_total_relation_size(\u0026#39;\u0026#34;\u0026#39; || table_schema || \u0026#39;\u0026#34;.\u0026#34;\u0026#39; || table_name || \u0026#39;\u0026#34;\u0026#39;)) AS size FROM information_schema.tables ORDER BY pg_total_relation_size(\u0026#39;\u0026#34;\u0026#39; || table_schema || \u0026#39;\u0026#34;.\u0026#34;\u0026#39; || table_name || \u0026#39;\u0026#34;\u0026#39;) DESC limit 20 \u0026ndash;查出表大小按大小排序并分离data与index\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 SELECT table_name, pg_size_pretty(table_size) AS table_size, pg_size_pretty(indexes_size) AS indexes_size, pg_size_pretty(total_size) AS total_size FROM ( SELECT table_name, pg_table_size(table_name) AS table_size, pg_indexes_size(table_name) AS indexes_size, pg_total_relation_size(table_name) AS total_size FROM ( SELECT (\u0026#39;\u0026#34;\u0026#39; || table_schema || \u0026#39;\u0026#34;.\u0026#34;\u0026#39; || table_name || \u0026#39;\u0026#34;\u0026#39;) AS table_name FROM information_schema.tables ) AS all_tables ORDER BY total_size DESC ) AS pretty_sizes ","date":"2022-08-24T00:00:00Z","permalink":"https://ggxxll.github.io/p/postgresql%E6%9F%A5%E8%AF%A2%E8%A1%A8%E7%9A%84%E5%A4%A7%E5%B0%8F/","title":"Postgresql查询表的大小"},{"content":"环境 系统: windows 语言: golang IDE: goland 安装 Protoc 下载protoc-xxx-win64.zip, 并解压到安装位置(如: d:/protoc)即可 添加环境变量d:/protoc/bin到PATH中 使用protoc --version验证是否安装成功 配置 Golang 环境 下载第三方protobuf库, 并解压到d:/protoc/include, 如: googleapis gogo/protobuf go install google.golang.org/protobuf/cmd/protoc-gen-go 安装goland插件 Protocol Buffer Editor, 按照README在配置中添加d:/protoc/include和自己实现的库的路径(目前好像是每个项目使用时都需要配置一下), 完成之后就有自动提示了（可能需要重启IDE） 使用 命令: protoc -I=. --go_out=. example.proto -I用于指定imports path, 第三方库统一放到d:/protoc/include的话是不需要指定的 官方文档 包括protobuf语法和其他语言的使用教程点击\n","date":"2022-08-24T00:00:00Z","permalink":"https://ggxxll.github.io/p/protoc%E4%BD%BF%E7%94%A8/","title":"Protoc使用"},{"content":"某日突然发现Sublime Text3（3.2.2）插件OmniMarkupPreviewer打开页面出现404错误\n1 2 3 4 5 6 7 Error: 404 Not Found Sorry, the requested URL \u0026#39;http://127.0.0.1:51004/view/26\u0026#39; caused an error: \u0026#39;buffer_id(26) is not valid (closed or unsupported file format)\u0026#39; **NOTE:** If you run multiple instances of Sublime Text, you may want to adjust the `server_port` option in order to get this plugin work again. 查看Console窗口出现如下错误：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 OmniMarkupPreviewer: [ERROR] Exception occured while rendering using MarkdownRenderer Traceback (most recent call last): File \u0026#34;%AppData%\\Roaming\\Sublime Text 3\\Packages\\OmniMarkupPreviewer\\OmniMarkupLib\\RendererManager.py\u0026#34;, line 266, in render_text rendered_text = renderer.render(text, filename=filename) File \u0026#34;%AppData%\\Roaming\\Sublime Text 3\\Packages\\OmniMarkupPreviewer\\OmniMarkupLib\\Renderers/MarkdownRenderer.py\u0026#34;, line 48, in render extensions=self.extensions) File \u0026#34;%AppData%\\Roaming\\SUBLIM~1\\Packages\\PYTHON~3\\st3\\markdown\\core.py\u0026#34;, line 390, in markdown md = Markdown(**kwargs) File \u0026#34;%AppData%\\Roaming\\SUBLIM~1\\Packages\\PYTHON~3\\st3\\markdown\\core.py\u0026#34;, line 100, in __init__ configs=kwargs.get(\u0026#39;extension_configs\u0026#39;, {})) File \u0026#34;%AppData%\\Roaming\\SUBLIM~1\\Packages\\PYTHON~3\\st3\\markdown\\core.py\u0026#34;, line 126, in registerExtensions ext = self.build_extension(ext, configs.get(ext, {})) File \u0026#34;%AppData%\\Roaming\\SUBLIM~1\\Packages\\PYTHON~3\\st3\\markdown\\core.py\u0026#34;, line 166, in build_extension module = importlib.import_module(ext_name) File \u0026#34;./python3.3/importlib/__init__.py\u0026#34;, line 90, in import_module File \u0026#34;\u0026lt;frozen importlib._bootstrap\u0026gt;\u0026#34;, line 1584, in _gcd_import File \u0026#34;\u0026lt;frozen importlib._bootstrap\u0026gt;\u0026#34;, line 1565, in _find_and_load File \u0026#34;\u0026lt;frozen importlib._bootstrap\u0026gt;\u0026#34;, line 1529, in _find_and_load_unlocked ImportError: No module named \u0026#39;xxx\u0026#39; 原因 OmniMarkupPreviewer插件使用python-markdown@3.2.2\nSublime Text3使用python-markdown@3.1.1\n当sys.path中的路径下有相同包名时，import会优先导入路径靠前的包\n所以Sublime Text3按照启动顺序加载sys.path后，会优先使用python-markdown@3.1.1，出现导包错误\n解决方案 本人是windows系统，使用%AppData%\\Sublime Text 3\\Packages\\OmniMarkupPreviewer\\OmniMarkupLib\\Renderers\\libs\\markdown文件夹覆盖%AppData%\\Sublime Text 3\\Packages\\python-markdown\\st3\\markdown\n然后重启Sublime Text3即可\n","date":"2022-08-24T00:00:00Z","permalink":"https://ggxxll.github.io/p/sublimetext3-omnimarkuppreviewer%E5%87%BA%E7%8E%B0404%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/","title":"SublimeText3+OmniMarkupPreviewer出现404问题解决方案"},{"content":"supervisor安装使用, 并配置服务崩溃邮件报警\n环境 系统: Centos7, python3\n安装依赖: 1 yum install -y supervisor sendmail mailx \u0026amp;\u0026amp; pip3 install superlance 配置邮件: vim /etc/mail.rc, 然后添加如下内容: 1 2 3 4 5 6 7 8 9 # 发件人邮箱 set from=xxx@xxx.com # smtp服务 set smtp=smtps://smtp.xxx.com:465 # 用户名 set smtp-auth-user=xxx@xxx.com # 密码 set smtp-auth-password=xxx set ssl-verify=ignore 测试邮件: echo 'this is test'| /usr/bin/mail -s 'xxxxx' xxx@xx.com 配置项目: 编辑项目的配置文件, vim /etc/supervisord.d/xxx.ini\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # 项目相关配置 [program:projectName] # 设置命令在指定的目录内执行 directory=projectPath # 这里为您要管理的项目的启动命令 command=projectRunCmd # 以哪个用户来运行该进程 user=root # supervisor 启动时自动该应用 autostart=true # 进程退出后自动重启进程 autorestart=true # 进程持续运行多久才认为是启动成功 startsecs=2 # 重试次数 startretries=3 # stderr 日志输出位置 stderr_logfile=path/stderr.log # stdout 日志输出位置 stdout_logfile=path/stdout.log 1 2 3 4 5 6 7 8 # 报警邮件相关配置 [eventlistener:crashmail] command=crashmail -p senddemo -s \u0026#34;echo \u0026#39;projectName crashed!!\u0026#39;| /usr/bin/mail -s \u0026#39;projectName\u0026#39; xxx@xx.com,xxx@xx.com\u0026#34; events=PROCESS_STATE_EXITED ## stderr 日志输出位置 stderr_logfile=path/crashmail/stderr.log ## stdout 日志输出位置 stdout_logfile=path/crashmail/stdout.log 相关命令: 1 2 3 4 5 6 7 8 9 supervisord -c /etc/supervisord.conf # 启动supervisor, 然后才可可以使用supervisorctl supervisorctl stop program_name # 停止某一个进程，program_name 为 [program:name] 里的 name supervisorctl start program_name # 启动某个进程 supervisorctl restart program_name # 重启某个进程 supervisorctl stop groupworker: # 结束所有属于名为 groupworker 这个分组的进程 (start，restart 同理) supervisorctl stop groupworker:name1 # 结束 groupworker:name1 这个进程 (start，restart 同理) supervisorctl stop all # 停止全部进程，注：start、restartUnlinking stale socket /tmp/supervisor.sock supervisorctl reload # 载入最新的配置文件，停止原有进程并按新的配置启动、管理所有进程 supervisorctl update # 根据最新的配置文件，启动新配置或有改动的进程，配置没有改动的进程不会受影响而重启 ","date":"2022-08-24T00:00:00Z","permalink":"https://ggxxll.github.io/p/supervisor%E4%BD%BF%E7%94%A8/","title":"Supervisor使用"},{"content":"简介 core 是由大佬Reasno创建的一个golang开源库.。这个库的定位是一个服务容器，负责管理和协调符合 Twelve-Factor的网络应用程序。\n目前这个库的结构：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 core ├── clihttp ├── config ├── container ├── contract ├── cronopts ├── di ├── doc ├── dtx ├── events ├── internal ├── key ├── leader ├── logging ├── observability ├── otes ├── otetcd ├── otgorm ├── otkafka ├── otmongo ├── otredis ├── ots3 ├── queue ├── srvhttp ├── text └── unierr ","date":"2022-08-24T00:00:00Z","permalink":"https://ggxxll.github.io/p/%E5%BC%80%E6%BA%90%E9%A1%B9%E7%9B%AEcore/","title":"开源项目core"},{"content":"题一 问题1： 会输出什么？为什么？ 问题2： 应该如何修改？ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 package main import \u0026#34;fmt\u0026#34; func main() { cache := Cache{} fmt.Println(cache.Get(\u0026#34;foo\u0026#34;)) cache.Set(\u0026#34;foo\u0026#34;, \u0026#34;bar\u0026#34;) fmt.Println(cache.Get(\u0026#34;foo\u0026#34;)) } type Cache struct { data map[string]interface{} } func (c Cache) Get(s string) (interface{},bool) { v, ok := c.data[s] return v, ok } func (c Cache) Set(s string, v interface{}) { c.data[s] = v } 题二 问题1：会输出什么？为什么？ 问题2：应该如何修改？ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 package main import \u0026#34;fmt\u0026#34; func main() { cache := Cache{} cache.Add(1) fmt.Println(cache.List()) } type Cache struct { data []interface{} } func (c Cache) Add(s interface{}) { c.data = append(c.data, s) } func (c Cache) List() []interface{} { return c.data } 题三 问题1：为什么题一需要初始化data，题二不需要? 问题2：实现这个Cache还需要什么补充的吗? 题一解 问题1： Get 返回 nil, false Set 会报空指针错误 问题2： Cache 需要初始化 data 题二解 问题1： 输出空数组 问题2： 修改 func (c Cache) Add 为 func (c *Cache) Add 题三解 问题1：因为 map 是引用传递，数组是值传递 问题2：需要添加锁 sync.Mutex ，并将 Cache 的方法改为指针调用 ","date":"2022-08-24T00:00:00Z","permalink":"https://ggxxll.github.io/p/%E4%BD%BF%E7%94%A8-map-%E8%AE%BE%E8%AE%A1%E7%BC%93%E5%AD%98/","title":"使用 map 设计缓存"},{"content":"环境 golang 问题 在k8s中部署多个节点服务时，CDN的Range回源经常失败。\n原因：每个节点生成渠道包的时间（Modify-Time）不同，导致CDN回源时认为文件变动了，自动停止回源。\n解决：使用http.ServeContent, 手动传递\n代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 // serveFile // 将newApkPath地址下的文件返回给请求方。 // 注意：曾经这里是使用ctx.File(调用了Go的http.ServeFile)方法返回文件， // 但是这样做在集群部署的情况下存在问题。集群每个节点文件生成的时间存在差异， // 而go标准库中的实现是根据文件的最后修改时间判断文件是否发生了变动。 // 大文件下载时CDN会采用分片回源（Range Header）， 每个分片获取到的最后修改时间不一致，会判定为下载失败！ // 这里改成http.ServeContent实现后，强制将修改时间置为0值，Go的标准库会忽略最后修改时间检测，在集群-多节点环境下也能正常分片下载了。 func serveFile(ctx *gin.Context, newApkPath string) error { _, apkName := filepath.Split(newApkPath) ctx.Header(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/octet-stream\u0026#34;) ctx.Header(\u0026#34;Content-Disposition\u0026#34;, \u0026#34;attachment; filename=\u0026#34;+apkName) ctx.Header(\u0026#34;Content-Transfer-Encoding\u0026#34;, \u0026#34;binary\u0026#34;) f, err := os.OpenFile(newApkPath, os.O_RDONLY, os.ModePerm) if err != nil { return err } defer f.Close() /* 最后修改时间，强制0值，跳过检查 */ http.ServeContent(ctx.Writer, ctx.Request, apkName, time.Time{}, f) return nil } ","date":"2022-08-24T00:00:00Z","permalink":"https://ggxxll.github.io/p/%E6%96%87%E4%BB%B6%E4%B8%8B%E8%BD%BDbug/","title":"文件下载Bug"}]