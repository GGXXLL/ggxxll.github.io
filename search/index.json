[{"content":"索引操作 1 2 3 4 5 6 7 8 9 10 11 // 创建索引 db.userinfos.createIndex({age:-1}) // 查看userinfos中的所有索引 db.userinfos.getIndexes() // 删除特定一个索引 db.userinfos.dropIndex({name:1,age:-1}) // 删除所有的索引(主键索引_id不会被删除) db.userinfos.dropIndexes() 分类 单键索引 单键索引(Single Field Indexes)顾名思义就是单个字段作为索引列，mongoDB 的所有 collection 默认都有一个单键索引 _id，我们也可以对一些经常作为过滤条件的字段设置索引，如给 age 字段添加一个索引，语法十分简单：\n1 2 3 4 5 // 为 age 添加升序索引 db.userinfos.createIndex({age:1}) // 嵌套字段添加索引 db.userinfos.createIndex({\u0026#34;ename.firstname\u0026#34;:1}) 复合索引 复合索引(Compound Indexes)指一个索引包含多个字段，用法和单键索引基本一致。使用复合索引时要注意字段的顺序，如下添加一个 name 和 age 的复合索引，name 正序，age 倒序，document 首先按照 name 正序排序，然后 name 相同的 document 按 age 进行倒序排序。mongoDB 中一个复合索引最多可以包含 32 个字段。\n1 2 3 4 5 6 7 8 9 10 //添加复合索引，name正序，age倒序 db.userinfos.createIndex({\u0026#34;name\u0026#34;:1,\u0026#34;age\u0026#34;:-1}) //过滤条件为name，或包含name的查询会使用索引(索引的第一个字段) db.userinfos.find({name:\u0026#39;张三\u0026#39;}).explain() db.userinfos.find({name:\u0026#34;张三\u0026#34;,level:10}).explain() db.userinfos.find({name:\u0026#34;张三\u0026#34;,age:23}).explain() //查询条件为age时，不会使用上边创建的索引,而是使用的全表扫描 db.userinfos.find({age:23}).explain() 多键索引 多键索引(mutiKey Indexes)是建在数组上的索引，在 mongoDB 的 document 中，有些字段的值为数组，多键索引就是为了提高查询这些数组的效率。看一个栗子：准备测试数据，classes 集合中添加两个班级，每个班级都有一个 students 数组，如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 db.classes.insertMany([ { \u0026#34;classname\u0026#34;:\u0026#34;class1\u0026#34;, \u0026#34;students\u0026#34;:[{name:\u0026#39;jack\u0026#39;,age:20}, {name:\u0026#39;tom\u0026#39;,age:22}, {name:\u0026#39;lilei\u0026#39;,age:25}] }, { \u0026#34;classname\u0026#34;:\u0026#34;class2\u0026#34;, \u0026#34;students\u0026#34;:[{name:\u0026#39;lucy\u0026#39;,age:20}, {name:\u0026#39;jim\u0026#39;,age:23}, {name:\u0026#39;jarry\u0026#39;,age:26}] }] ) db.classes.createIndex({\u0026#39;students.age\u0026#39;:1}) 哈希索引 哈希索引(hashed Indexes)就是将 field 的值进行 hash 计算后作为索引，其强大之处在于实现 O(1) 查找，当然用哈希索引最主要的功能也就是实现定值查找，对于经常需要排序或查询范围查询的集合不要使用哈希索引。\n1 db.userinfos.createIndex({\u0026#39;name\u0026#39;: \u0026#39;hashed\u0026#39;}) 属性 唯一索引 唯一索引(unique indexes)用于为collection添加唯一约束，即强制要求collection中的索引字段没有重复值。添加唯一索引的语法：\n1 2 //在userinfos的name字段添加唯一索引 db.userinfos.createIndex({name:1},{unique:true}) 局部索引 局部索引(Partial Indexes)顾名思义，只对collection的一部分添加索引。创建索引的时候，根据过滤条件判断是否对document添加索引，对于没有添加索引的文档查找时采用的全表扫描，对添加了索引的文档查找时使用索引。使用方法也比较简单：\n1 2 3 4 5 6 7 8 9 10 11 //userinfos集合中age\u0026gt;25的部分添加age字段索引 db.userinfos.createIndex( {age:1}, { partialFilterExpression: { age:{$gt: 25 }}} ) //查询age\u0026lt;25的document时，因为age\u0026lt;25的部分没有索引，会全表扫描查找(stage:COLLSCAN) db.userinfos.find({age:23}) //查询age\u0026gt;25的document时，因为age\u0026gt;25的部分创建了索引，会使用索引进行查找(stage:IXSCAN) db.userinfos.find({age:26}) 稀疏索引 稀疏索引(sparse indexes)在有索引字段的 document 上添加索引，如在 address 字段上添加稀疏索引时，只有 document 有 address 字段时才会添加索引。而普通索引则是为所有的 document 添加索引，使用普通索引时如果 document没有索引字段的话，设置索引字段的值为 null。\n稀疏索引的创建方式如下，当document包含address字段时才会创建索引：\n1 db.userinfos.createIndex({address:1},{sparse:true}) TTL索引 TTL索引(TTL indexes)是一种特殊的单键索引，用于设置 document 的过期时间，mongoDB 会在document 过期后将其删除，TTL 非常容易实现类似缓存过期策略的功能。我们看一个使用 TTL 索引的栗子：\n1 2 3 4 5 6 7 8 9 10 11 12 //添加测试数据 db.logs.insertMany([ {_id:1,createtime:new Date(),msg:\u0026#34;log1\u0026#34;}, {_id:2,createtime:new Date(),msg:\u0026#34;log2\u0026#34;}, {_id:3,createtime:new Date(),msg:\u0026#34;log3\u0026#34;}, {_id:4,createtime:new Date(),msg:\u0026#34;log4\u0026#34;} ]) //在createtime字段添加TTL索引，过期时间是120s db.logs.createIndex({createtime:1}, { expireAfterSeconds: 120 }) //logs中的document在创建后的120s后过期，会被mongoDB自动删除 注意：\nTTL索引只能设置在 date 类型字段(或者包含 date 类型的数组)上，过期时间为字段值 + exprireAfterSeconds； document过期时不一定就会被立即删除，因为 mongoDB 执行删除任务的时间间隔是60s； capped Collection 不能设置TTL索引，因为 mongoDB 不能主动删除 capped Collection 中的 document。 COPY mongoDB的索引详解 ","date":"2022-11-23T00:00:00Z","permalink":"https://ggxxll.github.io/p/mongodb-%E7%B4%A2%E5%BC%95/","title":"MongoDB 索引"},{"content":"内置recover函数必须在合适的位置调用才能发挥作用；否则，它的调用相当于空操作。 比如，在下面这个程序中，没有一个recover函数调用恢复了恐慌bye。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 package main func main() { defer func() { defer func() { recover() // 空操作 }() }() defer func() { func() { recover() // 空操作 }() }() func() { defer func() { recover() // 空操作 }() }() func() { defer recover() // 空操作 }() func() { recover() // 空操作 }() recover() // 空操作 defer recover() // 空操作 panic(\u0026#34;bye\u0026#34;) } 我们已经知道下面这个recover调用是有作用的。\n1 2 3 4 5 6 7 8 9 package main func main() { defer func() { recover() // 将恢复恐慌\u0026#34;byte\u0026#34; }() panic(\u0026#34;bye\u0026#34;) } 在下面的情况下，recover函数调用的返回值为nil：\n传递给相应panic函数调用的实参为nil； 当前协程并没有处于恐慌状态； recover函数并未直接在一个延迟函数调用中调用。 一个recover调用只有在它的直接外层调用（即recover调用的父调用）是一个延迟调用，并且此延迟调用（即父调用）的直接外层调用（即recover调用的爷调用）和当前协程中最新产生并且尚未恢复的恐慌相关联时才起作用。\n一个有效的recover调用将最新产生并且尚未恢复的恐慌和与此恐慌相关联的函数调用（即爷调用）剥离开来，并且返回当初传递给产生此恐慌的panic函数调用的参数。\n","date":"2022-11-20T00:00:00Z","permalink":"https://ggxxll.github.io/p/golang-recover/","title":"Golang Recover"},{"content":"结构 1 2 3 4 type _string struct { elements *byte // 引用着底层的字节 len int // 字符串中的字节数 } 从这个声明来看，我们可以将一个字符串的内部定义看作为一个字节序列。 事实上，我们确实可以把一个字符串看作是一个元素类型为 byte 的（且元素不可修改的）切片。\n注意，前面的文章已经提到过多次，byte 是内置类型 uint8 的一个别名。\n字符串编码和Unicode码点 Unicode标准为全球各种人类语言中的每个字符制定了一个独一无二的值。 但Unicode标准中的基本单位不是字符，而是码点（code point）。大多数的码点实际上就对应着一个字符。 但也有少数一些字符是由多个码点组成的。\n码点值在Go中用rune值来表示。 内置rune类型为内置int32类型的一个别名。\n字符串和字节切片之间的转换的编译器优化\n使用for-range循环遍历字符串中的码点 for-range循环控制中的range关键字后可以跟随一个字符串，用来遍历此字符串中的码点（而非字节元素）。 字符串中非法的UTF-8编码字节序列将被解读为Unicode替换码点值0xFFFD。\n那么如何遍历一个字符串中的字节呢？使用传统for i := 0; i \u0026lt; len(s); i++循环：\n字符串衔接方法 除了使用+运算符来衔接字符串，我们也可以用下面的方法来衔接字符串：\nfmt标准库包中的Sprintf/Sprint/Sprintln函数可以用来衔接各种类型的值的字符串表示，当然也包括字符串类型的值。 使用strings.Join。 bytes.Buffer类型可以用来构建一个字节切片，然后我们可以将此字节切片转换为一个字符串。 从Go 1.10开始，strings.Builder类型可以用来拼接字符串。 和bytes.Buffer类型类似，此类型内部也维护着一个字节切片，但是它在将此字节切片转换为字符串时避免了底层字节的深复制。 标准编译器对使用+运算符的字符串衔接做了特别的优化。 所以，一般说来，在被衔接的字符串的数量是已知的情况下，使用+运算符进行字符串衔接是比较高效的。\n字符串的比较 上面已经提到了比较两个字符串事实上逐个比较这两个字符串中的字节。 Go 编译器一般会做出如下的优化：\n对于==和!=比较，如果这两个字符串的长度不相等，则这两个字符串肯定不相等（无需进行字节比较）。 如果这两个字符串底层引用着字符串切片的指针相等，则比较结果等同于比较这两个字符串的长度。 所以两个相等的字符串的比较的时间复杂度取决于它们底层引用着字符串切片的指针是否相等。 如果相等，则对它们的比较的时间复杂度为O(1)，否则时间复杂度为O(n)。\n上面已经提到了，对于标准编译器，一个字符串赋值完成之后，目标字符串和源字符串将共享同一个底层字节序列。 所以比较这两个字符串的代价很小。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; ) func main() { bs := make([]byte, 1\u0026lt;\u0026lt;26) s0 := string(bs) s1 := string(bs) s2 := s1 // s0、s1和s2是三个相等的字符串。 // s0的底层字节序列是bs的一个深复制。 // s1的底层字节序列也是bs的一个深复制。 // s0和s1底层字节序列为两个不同的字节序列。 // s2和s1共享同一个底层字节序列。 startTime := time.Now() _ = s0 == s1 duration := time.Now().Sub(startTime) fmt.Println(\u0026#34;duration for (s0 == s1):\u0026#34;, duration) startTime = time.Now() _ = s1 == s2 duration = time.Now().Sub(startTime) fmt.Println(\u0026#34;duration for (s1 == s2):\u0026#34;, duration) } ","date":"2022-11-20T00:00:00Z","permalink":"https://ggxxll.github.io/p/golang-string/","title":"Golang String"},{"content":"判断是否平衡二叉树 1 2 3 4 5 6 func isBalanced(root *TreeNode) bool { if root == nil{ return true } return abs(depth(root.Left)-depth(root.Right)) \u0026lt;= 1 \u0026amp;\u0026amp; isBalanced(root.Left) \u0026amp;\u0026amp; isBalanced(root.Right) } 获取树的高度 1 2 3 4 5 6 func depth(root *TreeNode) int { if root == nil{ return 0 } return max(depth(root.Left),depth(root.Right)) + 1 } 二叉树的镜像 1 2 3 4 5 6 7 8 9 10 func mirrorTree(root *TreeNode) *TreeNode { if root == nil{ return nil } r := mirrorTree(root.Right) l := mirrorTree(root.Left) root.Right = l root.Left = r return root } 对称的二叉树 1 2 3 4 5 6 7 8 9 10 11 12 func isSymmetric(root *TreeNode) bool { return checkSymmetric(root,root)## } func checkSymmetric(q,p *TreeNode) bool{ if q == nil \u0026amp;\u0026amp; p == nil{ return true } if q == nil || p == nil{ return false } return q.Val == p.Val \u0026amp;\u0026amp; checkSymmetric(q.Left,p.Right) \u0026amp;\u0026amp; checkSymmetric(q.Right,p.Left) } 展平二叉搜索树 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 func increasingBST(root *TreeNode) *TreeNode { if root == nil{ return nil } dummy := \u0026amp;TreeNode{} res := dummy var dfs func(*TreeNode) dfs = func(r *TreeNode){ if r == nil{ return } dfs(r.Left) res.Right = r r.Left = nil res = r dfs(r.Right) } dfs(root) return dummy.Right } 二叉树的公共祖先 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 func lowestCommonAncestor(root, p, q *TreeNode) *TreeNode { if root == nil{ return nil } if root.Val == p.Val || root.Val == q.Val{ return root } l,r := lowestCommonAncestor(root.Left,p,q),lowestCommonAncestor(root.Right,p,q) if r != nil \u0026amp;\u0026amp; l != nil{ return root } if r == nil{ return l } return r } 是否二叉搜索树 1 2 3 4 5 6 7 8 9 10 11 12 func isValidBST(root *TreeNode) bool { return helper(root, math.MinInt64, math.MaxInt64)## } func helper(root *TreeNode, lower, upper int) bool { if root == nil { return true } if root.Val \u0026lt;= lower || root.Val \u0026gt;= upper { return false } return helper(root.Left, lower, root.Val) \u0026amp;\u0026amp; helper(root.Right, root.Val, upper) } 从上到下 左到右 分层 打印二叉树, 返回二维数组 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 func levelOrder(root *TreeNode) [][]int { ret := [][]int{} if root == nil { return ret } q := []*TreeNode{root} for i := 0; len(q) \u0026gt; 0; i++ { ret = append(ret, []int{}) p := []*TreeNode{} for j := 0; j \u0026lt; len(q); j++ { node := q[j] ret[i] = append(ret[i], node.Val) if node.Left != nil { p = append(p, node.Left) } if node.Right != nil { p = append(p, node.Right) } } q = p } return ret } 从上到下 打印二叉树 二 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 func levelOrder(root *TreeNode) []int { ret := []int{} if root == nil { return ret } q := []*TreeNode{root} for i := 0; len(q) \u0026gt; 0; i++ { p := []*TreeNode{} for j := 0; j \u0026lt; len(q); j++ { node := q[j] ret = append(ret, node.Val) if node.Left != nil { p = append(p, node.Left) } if node.Right != nil { p = append(p, node.Right) } } q = p } return ret } 从上到下 奇偶 打印二叉树 三 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 func zigzagLevelOrder(root *TreeNode) (ans [][]int) { if root == nil { return } queue := []*TreeNode{root} for level := 0; len(queue) \u0026gt; 0; level++ { vals := []int{} q := queue queue = nil for _, node := range q { vals = append(vals, node.Val) if node.Left != nil { queue = append(queue, node.Left) } if node.Right != nil { queue = append(queue, node.Right) } } // 本质上和层序遍历一样，我们只需要把奇数层的元素翻转即可 if level%2 == 1 { for i, n := 0, len(vals); i \u0026lt; n/2; i++ { vals[i], vals[n-1-i] = vals[n-1-i], vals[i] } } ans = append(ans, vals) } return } 二叉树的第 k 大节点 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 func kthLargest(root *TreeNode, k int) int { var dfs func(*TreeNode) var res = -1 dfs = func(node *TreeNode){ if node == nil{ return } dfs(node.Right) k-- if k == 0 { res = node.Val return } dfs(node.Left) } dfs(root) return res } 二叉搜索树中两个节点之和 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 func findTarget(root *TreeNode, k int) bool { set := map[int]struct{}{} var dfs func(*TreeNode) bool dfs = func(node *TreeNode) bool { if node == nil { return false } if _, ok := set[k-node.Val]; ok { return true } set[node.Val] = struct{}{} return dfs(node.Left) || dfs(node.Right) } return dfs(root) } 二叉树的直径 1 2 3 4 5 6 7 8 9 10 11 12 13 14 func diameterOfBinaryTree(root *TreeNode) int{ var ans = 1 var depth func(*TreeNode)int depth = func(node *TreeNode)int{ if node == nil{ return 0 } l, r := depth(node.Left), depth(node.Right) ans = max(ans, l + r + 1) return max(l, r) + 1 } depth(root) return ans - 1 } 二叉树最深最左 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 func findBottomLeftValue(root *TreeNode) (ans int) { q := []*TreeNode{root} for len(q) \u0026gt; 0 { node := q[0] q = q[1:] if node.Right != nil { q = append(q, node.Right) } if node.Left != nil { q = append(q, node.Left) } ans = node.Val } return } func findBottomLeftValue(root *TreeNode) (curVal int) { curHeight := 0 var dfs func(*TreeNode, int) dfs = func(node *TreeNode, height int) { if node == nil { return } height++ dfs(node.Left, height) dfs(node.Right, height) if height \u0026gt; curHeight { curHeight = height curVal = node.Val } } dfs(root, 0) return } 合并二叉树 1 2 3 4 5 6 7 8 9 10 11 12 func mergeTrees(t1, t2 *TreeNode) *TreeNode { if t1 == nil { return t2 } if t2 == nil { return t1 } t1.Val += t2.Val t1.Left = mergeTrees(t1.Left, t2.Left) t1.Right = mergeTrees(t1.Right, t2.Right) return t1 } 二叉树的坡度 1 2 3 4 5 6 7 8 9 10 11 12 13 14 func findTilt(root *TreeNode) (ans int) { var dfs func(*TreeNode) int dfs = func(node *TreeNode) int { if node == nil { return 0 } sumLeft := dfs(node.Left) sumRight := dfs(node.Right) ans += abs(sumLeft - sumRight) return sumLeft + sumRight + node.Val } dfs(root) return } 单值二叉树 1 2 3 4 func isUnivalTree(root *TreeNode) bool { return root == nil || (root.Left == nil || root.Val == root.Left.Val \u0026amp;\u0026amp; isUnivalTree(root.Left)) \u0026amp;\u0026amp; (root.Right == nil || root.Val == root.Right.Val \u0026amp;\u0026amp; isUnivalTree(root.Right)) } 单值二叉树 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 func isUnivalTree(root *TreeNode) bool { if root == nil{ return false } var dfs func(root *TreeNode) bool dfs = func(node *TreeNode) bool { if node == nil { return true } if node.Val != root.Val{ return false } return dfs(node.Left) \u0026amp;\u0026amp; dfs(node.Right) } return dfs(root) } 重建二叉树 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 func buildTree(preorder []int, inorder []int) *TreeNode { if len(preorder) == 0 { return nil } root := \u0026amp;TreeNode{preorder[0], nil, nil} i := 0 for ; i \u0026lt; len(inorder); i++ { if inorder[i] == preorder[0] { break } } root.Left = buildTree(preorder[1:len(inorder[:i])+1], inorder[:i]) root.Right = buildTree(preorder[len(inorder[:i])+1:], inorder[i+1:]) return root } 二叉搜索树，数组是否是后序 1 2 3 4 5 6 7 8 9 10 11 class Solution: def verifyPostorder(self, postorder: [int]) -\u0026gt; bool: def recur(i, j): if i \u0026gt;= j: return True p = i while postorder[p] \u0026lt; postorder[j]: p += 1 m = p while postorder[p] \u0026gt; postorder[j]: p += 1 return p == j and recur(i, m - 1) and recur(m, j - 1) return recur(0, len(postorder) - 1## 二叉树中 和 为某值 的路径 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 func pathSum(root *TreeNode, target int) (ans [][]int) { path := []int{} var dfs func(*TreeNode, int) dfs = func(node *TreeNode, left int) { if node == nil { return } left -= node.Val path = append(path, node.Val) defer func() { path = path[:len(path)-1] }() if node.Left == nil \u0026amp;\u0026amp; node.Right == nil \u0026amp;\u0026amp; left == 0 { ans = append(ans, append([]int(nil), path...)) return } dfs(node.Left, left) dfs(node.Right, left) } dfs(root, target) return } 没有重复项的字符串全排列 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 func permutation(s string) (ans []string) { t := []byte(s) sort.Slice(t, func(i, j int) bool { return t[i] \u0026lt; t[j] }) n := len(t) perm := make([]byte, 0, n) vis := make([]bool, n) var backtrack func(int) backtrack = func(i int) { if i == n { ans = append(ans, string(perm)) return } for j, b := range vis { if b || j \u0026gt; 0 \u0026amp;\u0026amp; !vis[j-1] \u0026amp;\u0026amp; t[j-1] == t[j] { continue } vis[j] = true perm = append(perm, t[j]) backtrack(i + 1) perm = perm[:len(perm)-1] vis[j] = false } } backtrack(0) return } 反转链表 1 2 3 4 5 6 7 8 9 10 11 func reverseList(head *ListNode) *ListNode { var prev *ListNode curr := head for curr != nil { next := curr.Next // b-\u0026gt;c c nil curr.Next = prev // a -\u0026gt; nil, b -\u0026gt; a, c -\u0026gt; b -\u0026gt; a prev = curr // a -\u0026gt; nil, b-\u0026gt;a curr = next // b-\u0026gt;c, c } return prev } 反转链表 1 2 3 4 5 6 7 8 9 func reverseList(head *ListNode) *ListNode { if head == nil || head.Next == nil { return head } newHead := reverseList(head.Next) head.Next.Next = head head.Next = nil return newHead } 倒序打印链表 递归 或者 使用计算链表长度，然后数组倒序放入 1 2 3 4 5 6 func reversePrint(head *ListNode) (out []int) { if head == nil { return []int{} } return append(reversePrint(head.Next),head.Val) } 合并两个有序链表 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 func mergeTwoLists(l1 *ListNode, l2 *ListNode) *ListNode { dummy := \u0026amp;ListNode{} node := dummy for l1 != nil \u0026amp;\u0026amp; l2 != nil{ if l1.Val \u0026lt;= l2.Val{ node.Next = l1 l1 = l1.Next }else{ node.Next = l2 l2 = l2.Next } node = node.Next } if l1 != nil{ node.Next = l1 }else{ node.Next = l2 } return dummy.Next } 链表的公共节点 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 func getIntersectionNode(headA, headB *ListNode) *ListNode { if headA == nil || headB == nil { return nil } pa, pb := headA, headB for pa != pb { if pa == nil { pa = headB } else { pa = pa.Next } if pb == nil { pb = headA } else { pb = pb.Next } } return pa } 环形链表 1 2 3 4 5 6 7 8 9 10 11 12 13 14 func hasCycle(head *ListNode) bool { if head == nil || head.Next == nil { return false } slow, fast := head, head.Next for fast != slow { if fast == nil || fast.Next == nil { return false } slow = slow.Next fast = fast.Next.Next } return true } 1的个数 1 2 3 4 5 6 func hammingWeight(num uint32) (ones int) { for ; num \u0026gt; 0; num \u0026amp;= num - 1 { ones++ } return } // 1,2,3,4,5\n旋转数组中的最小值 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 func minArray(numbers []int) int { low := 0 high := len(numbers) - 1 for low \u0026lt; high { pivot := low + (high - low) / 2 if numbers[pivot] \u0026lt; numbers[high] { high = pivot } else if numbers[pivot] \u0026gt; numbers[high] { low = pivot + 1 } else { high-- } } return numbers[low] } 连续子数组的最大和 1 2 3 4 5 6 7 8 9 10 11 12 func maxSubArray(nums []int) int { max := nums[0] for i := 1; i \u0026lt; len(nums); i++ { if nums[i-1] \u0026gt; 0 { nums[i] += nums[i-1] } if nums[i] \u0026gt; max { max = nums[i] } } return max } 股票最佳时机2 1 2 3 4 5 6 func maxProfit(prices []int) (ans int) { for i := 1; i \u0026lt; len(prices); i++ { ans += max(0, prices[i]-prices[i-1]) } return } 股票最佳时机1 1 2 3 4 5 6 7 8 9 10 11 12 13 func maxProfit(prices []int) (ans int) { min := math.MaxInt64 max := 0 for _,v := range prices{ if v \u0026lt; min{ min = v } if v - min \u0026gt; max{ max = v-min } } return max } 杨辉三角 1 2 3 4 5 6 7 8 9 10 11 12 func generate(numRows int) [][]int { ans := make([][]int, numRows) for i := range ans { ans[i] = make([]int, i+1) ans[i][0] = 1 ans[i][i] = 1 for j := 1; j \u0026lt; i; j++ { ans[i][j] = ans[i-1][j] + ans[i-1][j-1] } } return ans } 杨辉三角 第 n 行 1 2 3 4 5 6 7 8 9 10 func getRow(rowIndex int) []int { row := make([]int, rowIndex+1) row[0] = 1 for i := 1; i \u0026lt;= rowIndex; i++ { for j := i; j \u0026gt; 0; j-- { row[j] += row[j-1] } } return row } 杨辉三角 第 n 行 1 2 3 4 5 6 7 8 func getRow(rowIndex int) []int { row := make([]int, rowIndex+1) row[0] = 1 for i := 1; i \u0026lt;= rowIndex; i++ { row[i] = row[i-1] * (rowIndex - i + 1) / i } return row } 快速排序 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 func sortArray(nums []int) []int { quickSort(nums, 0, len(nums)-1) return nums } func quickSort_part(nums []int, l, r int) int { // 轴线 pivot := rand.Intn(r-l) + l // 将轴元素移至末尾 nums[pivot], nums[r] = nums[r], nums[pivot] i := l - 1 // 遍历 （l,r) 区间 for j := l; j \u0026lt; r; j++ { // 比轴元素小的值，放到 l 区间左边 if nums[j] \u0026lt; nums[r] { i++ nums[j], nums[i] = nums[i], nums[j] } } // 最后，将轴元素交换回停止位置 i++ nums[i], nums[r] = nums[r], nums[i] return i } func quickSort(nums []int, l, r int) { if r \u0026lt;= l { return } mid := quickSort_part(nums, l, r) quickSort(nums, l, mid-1) quickSort(nums, mid+1, r) } 青蛙跳台阶 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 // 可以跳1阶，或者2阶 // f(n) = f(n-1) + f(n-2) func numWays(n int) int { var a, b = 1, 1 for ; n \u0026gt; 0 ; n-- { a, b = b, (a + b) % (1e9 + 7) } return a } // 递归 func jumpFloor(N int) int { if N \u0026lt;= 2 { return N } ​ return jumpFloor(N-1) + jumpFloor(N-2) } // 可以跳1阶，或者3阶 // f(n) = f(n-1) + f(n-3) func jumpFloor(N int) int { if N == 0{ return 0 } if N \u0026lt;= 2 { return 1 } ​ return jumpFloor(N-1) + jumpFloor(N-3) } func jumpFloor(N int) int { if N == 0{ return 0 } if N \u0026lt;= 2 { return 1 } ​ dp := make([]int, N) dp = append(dp, 1, 1, 2, 3) for i:=2 ;i \u0026lt; N;i++{ dp[i] = dp[i-1] + dp[i-3] } return dp[N] } // 可以跳1阶，或者2阶...n阶 func jumpFloor(N int) int { if N \u0026lt;= 2 { return N } return jumpFloor(N-1) * 2 } func jumpFloor(N int) int { if N \u0026lt;= 2 { return N } ​ b := 2 for i := 3; i\u0026lt;= N;i++ { b = 2 * b } return b } func jumpFloor(N int) int { return 1 \u0026lt;\u0026lt; (N -1) } 最长公共前缀 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 func longestCommonPrefix(strs []string) string { if len(strs) == 0 { return \u0026#34;\u0026#34; } pre := strs[0] for i := 1; i \u0026lt; len(strs); i++ { pre = commonPrefix(pre, strs[i]) if pre == \u0026#34;\u0026#34; { return \u0026#34;\u0026#34; } } return pre } func commonPrefix(s string, t string) string { if s == t { return s } a := len(s) b := len(t) j := 0 for i := 0; i \u0026lt; a; i++ { if i \u0026gt;= b || s[i] != t[i] { return s[:i] } j++ } return s[:j] } 移动零 1 2 3 4 5 6 7 8 9 func moveZeroes(nums []int) { zeroIndex := 0 for i := 0; i \u0026lt; len(nums); i++ { if nums[i] != 0 { nums[i], nums[zeroIndex] = nums[zeroIndex], nums[i] zeroIndex++ } } } 删除排序数组中的重复项 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 func removeDuplicates(nums []int) int { j := 1 for i := 1; i \u0026lt; len(nums); i++ { if nums[i] != nums[i-1] { nums[j] = nums[i] j++ } } return j } func removeDuplicates2(nums []int) int { j := 0 n := 2 for i := 0; i \u0026lt; len(nums); i++ { if j \u0026lt; n || nums[i] != nums[j-n] { nums[j] = nums[i] j++ } } return j } 旋转数组 1 2 3 4 5 6 7 8 9 10 11 12 13 14 func rotate(nums []int, k int) { k %= len(nums) reverse(nums, 0, len(nums)-1) reverse(nums, 0, k-1) reverse(nums, k, len(nums)-1) } func reverse(nums []int, n, m int) { for n \u0026lt; m { nums[n], nums[m] = nums[m], nums[n] n++ m-- } } 合并两个有序数组 1 2 3 4 5 6 7 8 9 10 11 12 13 func merge(nums1 []int, m int, nums2 []int, n int) { i := m + n for n \u0026gt; 0 { if m \u0026gt; 0 \u0026amp;\u0026amp; nums1[m-1] \u0026gt; nums2[n-1] { nums1[i-1] = nums1[m-1] m-- } else { nums1[i-1] = nums2[n-1] n-- } i-- } } 数组相对排序 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 func relativeSortArray(arr1 []int, arr2 []int) []int { rank := map[int]int{} for i, v := range arr2 { rank[v] = i } sort.Slice(arr1, func(i, j int) bool { x, y := arr1[i], arr1[j] rankX, hasX := rank[x] rankY, hasY := rank[y] if hasX \u0026amp;\u0026amp; hasY { return rankX \u0026lt; rankY } if hasX || hasY { return hasX } return x \u0026lt; y }) return arr1 } func relativeSortArrayV2(arr1 []int, arr2 []int) []int { rank := map[int]int{} for i, v := range arr2 { rank[v] = i - len(arr2) } sort.Slice(arr1, func(i, j int) bool { x, y := arr1[i], arr1[j] if r, has := rank[x]; has { x = r } if r, has := rank[y]; has { y = r } return x \u0026lt; y }) return arr1 } 其他 go\n1 2 3 4 5 6 7 8 9 10 11 12 func max(a,b int) int{ if a\u0026gt;b{ return a } return b## }go func abs(a int) int{ if a\u0026lt;0{ return -a } return a } ","date":"2022-11-20T00:00:00Z","permalink":"https://ggxxll.github.io/p/leetcode_simple/","title":"Leetcode_simple"},{"content":"数据结构 select 在 Go 语言的源代码中不存在对应的结构体，但是我们使用 runtime.scase 结构体表示 select 控制结构中的 case：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 type scase struct { c *hchan // chan elem unsafe.Pointer // data element } type hchan struct { qcount uint // total data in the queue dataqsiz uint // size of the circular queue buf unsafe.Pointer // points to an array of dataqsiz elements elemsize uint16 closed uint32 elemtype *_type // element type sendx uint // send index recvx uint // receive index recvq waitq // list of recv waiters sendq waitq // list of send waiters lock mutex } 因为非默认的 case 中都与 Channel 的发送和接收有关，所以 runtime.scase 结构体中也包含一个 runtime.hchan 类型的字段存储 case 中使用的 Channel。\nselectgo 初始化 runtime.selectgo 函数首先会进行执行必要的初始化操作并决定处理 case 的两个顺序 — 轮询顺序 pollOrder 和加锁顺序 lockOrder：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 func selectgo(cas0 *scase, order0 *uint16, ncases int) (int, bool) { // scase 的数量最大为 65536 cas1 := (*[1 \u0026lt;\u0026lt; 16]scase)(unsafe.Pointer(cas0)) order1 := (*[1 \u0026lt;\u0026lt; 17]uint16)(unsafe.Pointer(order0)) ncases := nsends + nrecvs scases := cas1[:ncases:ncases] pollorder := order1[:ncases:ncases] lockorder := order1[ncases:][:ncases:ncases] norder := 0 for i := range scases { cas := \u0026amp;scases[i] } for i := 1; i \u0026lt; ncases; i++ { j := fastrandn(uint32(i + 1)) pollorder[norder] = pollorder[j] pollorder[j] = uint16(i) norder++ } pollorder = pollorder[:norder] lockorder = lockorder[:norder] // 根据 Channel 的地址排序确定加锁顺序 ... sellock(scases, lockorder) ... } 轮询顺序 pollOrder 和加锁顺序 lockOrder 分别是通过以下的方式确认的：\n轮询顺序：通过 runtime.fastrandn 函数引入随机性； 加锁顺序：按照 Channel 的地址排序后确定加锁顺序； 随机的轮询顺序可以避免 Channel 的饥饿问题，保证公平性；而根据 Channel 的地址顺序确定加锁顺序能够避免死锁的发生。这段代码最后调用的 runtime.sellock 会按照之前生成的加锁顺序锁定 select 语句中包含所有的 Channel。\n循环 当我们为 select 语句锁定了所有 Channel 之后就会进入 runtime.selectgo 函数的主循环，它会分三个阶段查找或者等待某个 Channel 准备就绪：\n查找是否已经存在准备就绪的 Channel，即可以执行收发操作； 将当前 Goroutine 加入 Channel 对应的收发队列上并等待其他 Goroutine 的唤醒； 当前 Goroutine 被唤醒之后找到满足条件的 Channel 并进行处理； runtime.selectgo 函数会根据不同情况通过 goto 语句跳转到函数内部的不同标签执行相应的逻辑，其中包括：\nbufrecv：可以从缓冲区读取数据； bufsend：可以向缓冲区写入数据； recv：可以从休眠的发送方获取数据； send：可以向休眠的接收方发送数据； rclose：可以从关闭的 Channel 读取 EOF； sclose：向关闭的 Channel 发送数据； retc：结束调用并返回； 我们先来分析循环执行的第一个阶段，查找已经准备就绪的 Channel。循环会遍历所有的 case 并找到需要被唤起的 runtime.sudog 结构，在这个阶段，我们会根据 case 的四种类型分别处理：\n当 case 不包含 Channel 时 这种 case 会被跳过； 当 case 会从 Channel 中接收数据时 如果当前 Channel 的 sendq 上有等待的 Goroutine，就会跳到 recv 标签并从缓冲区读取数据后将等待 Goroutine 中的数据放入到缓冲区中相同的位置； 如果当前 Channel 的缓冲区不为空，就会跳到 bufrecv 标签处从缓冲区获取数据； 如果当前 Channel 已经被关闭，就会跳到 rclose 做一些清除的收尾工作； 当 case 会向 Channel 发送数据时 如果当前 Channel 已经被关，闭就会直接跳到 sclose 标签，触发 panic 尝试中止程序； 如果当前 Channel 的 recvq 上有等待的 Goroutine，就会跳到 send 标签向 Channel 发送数据； 如果当前 Channel 的缓冲区存在空闲位置，就会将待发送的数据存入缓冲区； 当 select 语句中包含 default 时 表示前面的所有 case 都没有被执行，这里会解锁所有 Channel 并返回，意味着当前 select 结构中的收发都是非阻塞的； 第一阶段的主要职责是查找所有 case 中是否有可以立刻被处理的 Channel。无论是在等待的 Goroutine 上还是缓冲区中，只要存在数据满足条件就会立刻处理，如果不能立刻找到活跃的 Channel 就会进入循环的下一阶段，按照需要将当前 Goroutine 加入到 Channel 的 sendq 或者 recvq 队列中：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 func selectgo(cas0 *scase, order0 *uint16, ncases int) (int, bool) { ... gp = getg() nextp = \u0026amp;gp.waiting for _, casei := range lockorder { casi = int(casei) cas = \u0026amp;scases[casi] c = cas.c sg := acquireSudog() sg.g = gp sg.c = c if casi \u0026lt; nsends { c.sendq.enqueue(sg) } else { c.recvq.enqueue(sg) } } gopark(selparkcommit, nil, waitReasonSelect, traceEvGoBlockSelect, 1) ... } 除了将当前 Goroutine 对应的 runtime.sudog 结构体加入队列之外，这些结构体都会被串成链表附着在 Goroutine 上。在入队之后会调用 runtime.gopark 挂起当前 Goroutine 等待调度器的唤醒。\n当 select 中的一些 Channel 准备就绪之后，当前 Goroutine 就会被调度器唤醒。这时会继续执行 runtime.selectgo 函数的第三部分，从 runtime.sudog 中读取数据：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 func selectgo(cas0 *scase, order0 *uint16, ncases int) (int, bool) { ... sg = (*sudog)(gp.param) gp.param = nil casi = -1 cas = nil sglist = gp.waiting for _, casei := range lockorder { k = \u0026amp;scases[casei] if sg == sglist { casi = int(casei) cas = k } else { c = k.c if int(casei) \u0026lt; nsends { c.sendq.dequeueSudoG(sglist) } else { c.recvq.dequeueSudoG(sglist) } } sgnext = sglist.waitlink sglist.waitlink = nil releaseSudog(sglist) sglist = sgnext } c = cas.c goto retc ... } 第三次遍历全部 case 时，我们会先获取当前 Goroutine 接收到的参数 sudog 结构，我们会依次对比所有 case 对应的 sudog 结构找到被唤醒的 case，获取该 case 对应的索引并返回。\n由于当前的 select 结构找到了一个 case 执行，那么剩下 case 中没有被用到的 sudog 就会被忽略并且释放掉。为了不影响 Channel 的正常使用，我们还是需要将这些废弃的 sudog 从 Channel 中出队。\n当我们在循环中发现缓冲区中有元素或者缓冲区未满时就会通过 goto 关键字跳转到 bufrecv 和 bufsend 两个代码段，这两段代码的执行过程都很简单，它们只是向 Channel 中发送数据或者从缓冲区中获取新数据：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 bufrecv: recvOK = true qp = chanbuf(c, c.recvx) if cas.elem != nil { typedmemmove(c.elemtype, cas.elem, qp) } typedmemclr(c.elemtype, qp) c.recvx++ if c.recvx == c.dataqsiz { c.recvx = 0 } c.qcount-- selunlock(scases, lockorder) goto retc bufsend: typedmemmove(c.elemtype, chanbuf(c, c.sendx), cas.elem) c.sendx++ if c.sendx == c.dataqsiz { c.sendx = 0 } c.qcount++ selunlock(scases, lockorder) goto retc 这里在缓冲区进行的操作和直接调用 runtime.chansend 和 runtime.chanrecv 差不多，上述两个过程在执行结束之后都会直接跳到 retc 字段。\n两个直接收发 Channel 的情况会调用运行时函数 runtime.send 和 runtime.recv，这两个函数会与处于休眠状态的 Goroutine 打交道：\n1 2 3 4 5 6 7 8 recv: recv(c, sg, cas.elem, func() { selunlock(scases, lockorder) }, 2) recvOK = true goto retc send: send(c, sg, cas.elem, func() { selunlock(scases, lockorder) }, 2) goto retc 不过如果向关闭的 Channel 发送数据或者从关闭的 Channel 中接收数据，情况就稍微有一点复杂了：\n从一个关闭 Channel 中接收数据会直接清除 Channel 中的相关内容； 向一个关闭的 Channel 发送数据就会直接 panic 造成程序崩溃： 1 2 3 4 5 6 7 8 9 10 11 rclose: selunlock(scases, lockorder) recvOK = false if cas.elem != nil { typedmemclr(c.elemtype, cas.elem) } goto retc sclose: selunlock(scases, lockorder) panic(plainError(\u0026#34;send on closed channel\u0026#34;) 总结 select 结构的执行过程与实现原理，首先在编译期间，Go 语言会对 select 语句进行优化，它会根据 select 中 case 的不同选择不同的优化路径：\n空的 select 语句会被转换成调用 runtime.block 直接挂起当前 Goroutine； 1 2 3 func block() { gopark(nil, nil, waitReasonSelectNoCases, traceEvGoStop, 1) } 如果 select 语句中只包含一个 case，编译器会将其转换成 if ch == nil { block }; n; 表达式； 首先判断操作的 Channel 是不是空的； 然后执行 case 结构中的内容； 如果 select 语句中只包含两个 case 并且其中一个是 default，那么会使用 runtime.selectnbrecv 和 runtime.selectnbsend非阻塞地执行收发操作； 在默认情况下会通过 runtime.selectgo 获取执行 case 的索引，并通过多个 if 语句执行对应 case 中的代码； 在编译器已经对 select 语句进行优化之后，Go 语言会在运行时执行编译期间展开的 runtime.selectgo 函数，该函数会按照以下的流程执行：\n随机生成一个遍历的轮询顺序 pollOrder 并根据 Channel 地址生成锁定顺序 lockOrder； 根据 pollOrder 遍历所有的 case 查看是否有可以立刻处理的 Channel； 如果存在，直接获取 case 对应的索引并返回； 如果不存在，创建 runtime.sudog 结构体，将当前 Goroutine 加入到所有相关 Channel 的收发队列，并调用 runtime.gopark 挂起当前 Goroutine 等待调度器的唤醒； 当调度器唤醒当前 Goroutine 时，会再次按照 lockOrder 遍历所有的 case，从中查找需要被处理的 runtime.sudog 对应的索引； select 关键字是 Go 语言特有的控制结构，它的实现原理比较复杂，需要编译器和运行时函数的通力合作。\n","date":"2022-11-12T00:00:00Z","permalink":"https://ggxxll.github.io/p/golang-select/","title":"Golang Select"},{"content":"Mutex Go 语言的 sync.Mutex 由两个字段 state 和 sema 组成。其中 state 表示当前互斥锁的状态，而 sema 是用于控制锁状态的信号量。\n1 2 3 4 type Mutex struct { state int32 sema uint32 } 上述两个加起来只占 8 字节空间的结构体表示了 Go 语言中的互斥锁。\n状态 互斥锁的状态比较复杂，如下图所示，最低三位分别表示 mutexLocked、mutexWoken 和 mutexStarving，剩下的位置用来表示当前有多少个 Goroutine 在等待互斥锁的释放：\n在默认情况下，互斥锁的所有状态位都是 0，int32 中的不同位分别表示了不同的状态：\nmutexLocked：表示互斥锁的锁定状态； mutexWoken：表示从正常模式被从唤醒； mutexStarving：当前的互斥锁进入饥饿状态； waitersCount：当前互斥锁上等待的 Goroutine 个数； RWMutex 1 2 3 4 5 6 7 type RWMutex struct { w Mutex writerSem uint32 readerSem uint32 readerCount int32 readerWait int32 } w：复用互斥锁提供的能力 writerSem：写等待读 readerSem：读等待写 readerCount：存储了当前正在执行的读操作数量 readerWait：表示当写操作被阻塞时等待的读操作个数 WaitGroup 1 2 3 4 type WaitGroup struct { noCopy noCopy state1 [3]uint32 } noCopy：保证自身不会被开发者通过再赋值的方式拷贝； state1：存储着状态和信号量； Once 1 2 3 4 type Once struct { done uint32 m Mutex } Cond 1 2 3 4 5 6 type Cond struct { noCopy noCopy L Locker notify notifyList checker copyChecker } noCopy：用于保证结构体不会在编译期间拷贝 copyChecker：用于禁止运行期间发生的拷贝 L：用于保护内部的 notify 字段，Locker 接口类型的变量 notify：一个 Goroutine 的链表，它是实现同步机制的核心结构 1 2 3 4 5 6 7 8 type notifyList struct { wait uint32 notify uint32 lock mutex head *sudog tail *sudog } 在 sync.notifyList 结构体中，head 和 tail 分别指向的链表的头和尾，wait 和 notify 分别表示当前正在等待的和已经通知到的 Goroutine 的索引。\n","date":"2022-11-12T00:00:00Z","permalink":"https://ggxxll.github.io/p/golang-sync-lock/","title":"Golang sync Lock"},{"content":"数据结构 Go 语言的 Channel 在运行时使用 runtime.hchan 结构体表示。我们在 Go 语言中创建新的 Channel 时，实际上创建的都是如下所示的结构：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 type hchan struct { qcount uint // total data in the queue dataqsiz uint // size of the circular queue buf unsafe.Pointer // points to an array of dataqsiz elements elemsize uint16 closed uint32 elemtype *_type // element type sendx uint // send index recvx uint // receive index recvq waitq // list of recv waiters sendq waitq // list of send waiters lock mutex } type waitq struct { first *sudog last *sudog } qcount：元素个数 dataqsize：buffer 的大小，也就是make(chan T, N)中的N。 elemsize：单个元素的大小 elemtype：元素的类型 buf：缓冲区数据指针 closed：channel是否关闭，0-\u0026gt;打开，1-\u0026gt;关闭 sendx：发送操作处理到的位置 recvx：接收操作处理到的位置 recvq：读取数据而阻塞的 Goroutine 双向链表 sendq：写入数据而阻塞的 Goroutine 双向链表 lock：锁，保证并发安全 初始化 当我们在代码里面通过 make(chan int, 1), 实际调用了 runtime.makechan:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 func makechan(t *chantype, size int) *hchan { elem := t.elem // 判断 元素类型的大小 if elem.size \u0026gt;= 1\u0026lt;\u0026lt;16 { throw(\u0026#34;makechan: invalid channel element type\u0026#34;) } // 判断对齐限制 if hchanSize%maxAlign != 0 || elem.align \u0026gt; maxAlign { throw(\u0026#34;makechan: bad alignment\u0026#34;) } // 判断 size 非负 和 是否大于 maxAlloc 限制 mem, overflow := math.MulUintptr(elem.size, uintptr(size)) if overflow || mem \u0026gt; maxAlloc-hchanSize || size \u0026lt; 0 { panic(plainError(\u0026#34;makechan: size out of range\u0026#34;)) } var c *hchan switch { case mem == 0: // 无缓冲区，即 make 没设置大小 c = (*hchan)(mallocgc(hchanSize, nil, true)) c.buf = c.raceaddr() case elem.ptrdata == 0: // 数据类型不包含指针 c = (*hchan)(mallocgc(hchanSize+mem, nil, true)) c.buf = add(unsafe.Pointer(c), hchanSize) default: // 如果包含指针 // Elements contain pointers. c = new(hchan) c.buf = mallocgc(mem, elem, true) } c.elemsize = uint16(elem.size) c.elemtype = elem c.dataqsiz = uint(size) if debugChan { print(\u0026#34;makechan: chan=\u0026#34;, c, \u0026#34;; elemsize=\u0026#34;, elem.size, \u0026#34;; dataqsiz=\u0026#34;, size, \u0026#34;\\n\u0026#34;) } return c } 创建channel分为三种情况:\n无缓冲，只会为 runtime.hchan 分配一段内存空间\n有缓冲，且channel的类型不包含指针，此时 buf 为 hchanSize+元素大小*元素个数 的连续内存\n有缓冲，且channel的类型包含指针，则不能简单的根据元素的大小去申请内存，需要根据元素去分配内存\n发送数据 我们在这里可以简单梳理和总结一下使用 ch \u0026lt;- i 表达式向 Channel 发送数据时遇到的几种情况：\n如果当前 Channel 的 recvq 上存在已经被阻塞的 Goroutine，那么会直接将数据发送给当前 Goroutine 并将其设置成下一个运行的 Goroutine； 如果 Channel 存在缓冲区并且其中还有空闲的容量，我们会直接将数据存储到缓冲区 sendx 所在的位置上； 如果不满足上面的两种情况，会创建一个 runtime.sudog 结构并将其加入 Channel 的 sendq 队列中，当前 Goroutine 也会陷入阻塞等待其他的协程从 Channel 接收数据； 发送数据的过程中包含几个会触发 Goroutine 调度的时机：\n发送数据时发现 Channel 上存在等待接收数据的 Goroutine，立刻设置处理器的 runnext 属性，但是并不会立刻触发调度； 发送数据时并没有找到接收方并且缓冲区已经满了，这时会将自己加入 Channel 的 sendq 队列并调用 runtime.goparkunlock 触发 Goroutine 的调度让出处理器的使用权； 接收数据 我们梳理一下从 Channel 中接收数据时可能会发生的五种情况：\n如果 Channel 为空，那么会直接调用 runtime.gopark 挂起当前 Goroutine； 如果 Channel 已经关闭并且缓冲区没有任何数据，runtime.chanrecv 会直接返回； 如果 Channel 的 sendq 队列中存在挂起的 Goroutine，会将 recvx 索引所在的数据拷贝到接收变量所在的内存空间上并将 sendq 队列中 Goroutine 的数据拷贝到缓冲区； 如果 Channel 的缓冲区中包含数据，那么直接读取 recvx 索引对应的数据； 在默认情况下会挂起当前的 Goroutine，将 runtime.sudog 结构加入 recvq 队列并陷入休眠等待调度器的唤醒； 我们总结一下从 Channel 接收数据时，会触发 Goroutine 调度的两个时机：\n当 Channel 为空时； 当缓冲区中不存在数据并且也不存在数据的发送者时； 关闭通道 当 Channel 是一个空指针或者已经被关闭时，Go 语言运行时都会直接崩溃并抛出异常。 然后将 recvq 和 sendq 两个队列中的数据加入到 Goroutine 列表 gList 中，与此同时该函数会清除所有 runtime.sudog 上未被处理的元素。 最后会为所有被阻塞的 Goroutine 调用 runtime.goready 触发调度。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 func closechan(c *hchan) { if c == nil { panic(plainError(\u0026#34;close of nil channel\u0026#34;)) } lock(\u0026amp;c.lock) if c.closed != 0 { unlock(\u0026amp;c.lock) panic(plainError(\u0026#34;close of closed channel\u0026#34;)) } if raceenabled { callerpc := getcallerpc() racewritepc(c.raceaddr(), callerpc, funcPC(closechan)) racerelease(c.raceaddr()) } c.closed = 1 var glist gList // release all readers for { sg := c.recvq.dequeue() if sg == nil { break } if sg.elem != nil { typedmemclr(c.elemtype, sg.elem) sg.elem = nil } if sg.releasetime != 0 { sg.releasetime = cputicks() } gp := sg.g gp.param = unsafe.Pointer(sg) sg.success = false if raceenabled { raceacquireg(gp, c.raceaddr()) } glist.push(gp) } // release all writers (they will panic) for { sg := c.sendq.dequeue() if sg == nil { break } sg.elem = nil if sg.releasetime != 0 { sg.releasetime = cputicks() } gp := sg.g gp.param = unsafe.Pointer(sg) sg.success = false if raceenabled { raceacquireg(gp, c.raceaddr()) } glist.push(gp) } unlock(\u0026amp;c.lock) // Ready all Gs now that we\u0026#39;ve dropped the channel lock. for !glist.empty() { gp := glist.pop() gp.schedlink = 0 goready(gp, 3) } } 引用 Golang 深度剖析 \u0026ndash; channel的底层实现 Go 语言设计与实现\n","date":"2022-11-11T00:00:00Z","permalink":"https://ggxxll.github.io/p/golang-channel/","title":"Golang Channel"},{"content":"常见垃圾回收机制 引用计数 对每个对象维护一个引用计数，当引用对象的对象被销毁时，引用计数-1，如果引用计数为0，则进行垃圾回收\n优点：对象可以很快的被回收，不会出现内存耗尽或达到某个阀值时才回收。 缺点：不能很好的处理循环引用，而且实时维护引用计数，有也一定的代价。 代表语言：Python、PHP、Swift 标记-清除 标记清除（Mark-Sweep）算法是最常见的垃圾收集算法，标记清除收集器是跟踪式垃圾收集器，其执行过程可以分成标记（Mark）和清除（Sweep）两个阶段：\n标记阶段 — 从根对象出发查找并标记堆中所有存活的对象； 清除阶段 — 遍历堆中的全部对象，回收未被标记的垃圾对象并将回收的内存加入空闲链表；\n如下图所示，内存空间中包含多个对象，我们从根对象出发依次遍历对象的子对象并将从根节点可达的对象都标记成存活状态，即 A、C 和 D 三个对象，剩余的 B、E 和 F 三个对象因为从根节点不可达，所以会被当做垃圾：\n优点：解决了引用计数的缺点。 缺点：需要STW，即要暂时停掉程序运行。 代表语言：Golang(其采用三色标记法) 分代收集 按照对象生命周期长短划分不同的代空间，生命周期长的放入老年代，而短的放入新生代，不同代有不能的回收算法和回收频率。\n优点：回收性能好 缺点：算法复杂 代表语言： JAVA 屏障技术 内存屏障技术是一种屏障指令，它可以让 CPU 或者编译器在执行内存相关操作时遵循特定的约束，目前多数的现代处理器都会乱序执行指令以最大化性能，但是该技术能够保证内存操作的顺序性，在内存屏障前执行的操作一定会先于内存屏障后执行的操作。\n想要在并发或者增量的标记算法中保证正确性，我们需要达成以下两种三色不变性（Tri-color invariant）中的一种：\n强三色不变性 — 黑色对象不会指向白色对象，只会指向灰色对象或者黑色对象； 弱三色不变性 — 黑色对象指向的白色对象必须包含一条从灰色对象经由多个白色对象的可达路径 垃圾收集中的屏障技术更像是一个钩子方法，它是在用户程序读取对象、创建新对象以及更新对象指针时执行的一段代码，根据操作类型的不同，我们可以将它们分成读屏障（Read barrier）和写屏障（Write barrier）两种，因为读屏障需要在读操作中加入代码片段，对用户程序的性能影响很大，所以编程语言往往都会采用写屏障保证三色不变性。\n增量和并发 增量垃圾收集 — 增量地标记和清除垃圾，降低应用程序暂停的最长时间； 并发垃圾收集 — 利用多核的计算资源，在用户程序执行时并发标记和清除垃圾； 因为增量和并发两种方式都可以与用户程序交替运行，所以我们需要使用屏障技术保证垃圾收集的正确性；与此同时，应用程序也不能等到内存溢出时触发垃圾收集，因为当内存不足时，应用程序已经无法分配内存，这与直接暂停程序没有什么区别，增量和并发的垃圾收集需要提前触发并在内存不足前完成整个循环，避免程序的长时间暂停。\n增量收集器 增量式（Incremental）的垃圾收集是减少程序最长暂停时间的一种方案，它可以将原本时间较长的暂停时间切分成多个更小的 GC 时间片，虽然从垃圾收集开始到结束的时间更长了，但是这也减少了应用程序暂停的最大时间\n需要注意的是，增量式的垃圾收集需要与三色标记法一起使用，为了保证垃圾收集的正确性，我们需要在垃圾收集开始前打开写屏障，这样用户程序修改内存都会先经过写屏障的处理，保证了堆内存中对象关系的强三色不变性或者弱三色不变性。虽然增量式的垃圾收集能够减少最大的程序暂停时间，但是增量式收集也会增加一次 GC 循环的总时间，在垃圾收集期间，因为写屏障的影响用户程序也需要承担额外的计算开销，所以增量式的垃圾收集也不是只带来好处的，但是总体来说还是利大于弊。\n并发收集器 并发（Concurrent）的垃圾收集不仅能够减少程序的最长暂停时间，还能减少整个垃圾收集阶段的时间，通过开启读写屏障、利用多核优势与用户程序并行执行，并发垃圾收集器确实能够减少垃圾收集对应用程序的影响\n虽然并发收集器能够与用户程序一起运行，但是并不是所有阶段都可以与用户程序一起运行，部分阶段还是需要暂停用户程序的，不过与传统的算法相比，并发的垃圾收集可以将能够并发执行的工作尽量并发执行；当然，因为读写屏障的引入，并发的垃圾收集器也一定会带来额外开销，不仅会增加垃圾收集的总时间，还会影响用户程序，这是我们在设计垃圾收集策略时必须要注意的。\n三色标记法 灰色：对象已被标记，但这个对象包含的子对象未标记 黑色：对象已被标记，且这个对象包含的子对象也已标记，gcmarkBits 对应的位为1（该对象不会在本次GC中被清理） 白色：对象未被标记，gcmarkBits 对应的位为0（该对象将会在本次GC中被清理） 标记过程 起初所有的对象都是白色的； 从根对象出发扫描所有可达对象，标记为灰色，放入待处理队列； 从待处理队列中取出灰色对象，将其引用的对象标记为灰色并放入待处理队列中，自身标记为黑色； 重复步骤3，直到待处理队列为空，此时白色对象即为不可达的“垃圾”，回收白色对象； 根对象在垃圾回收的术语中又叫做根集合，它是垃圾回收器在标记过程时最先检查的对象。\n根对象包括：\n全局变量：程序在编译期就能确定的那些存在于程序整个生命周期的变量。 执行栈：每个 goroutine 都包含自己的执行栈，这些执行栈上包含栈上的变量及指向分配的堆内存区块的指针。 寄存器：寄存器的值可能表示一个指针，参与计算的这些指针可能指向某些赋值器分配的堆内存区块。 触发机制 阈值：一个环境变量 GOGC 来控制，默认是 100 ，即增长 100% 的堆内存才会触发 GC。 定期：默认2min触发一次gc 手动：runtime.GC() STW Stop The World 是 gc 的最大性能问题，对于 go 而言，需要停止所有的内存变化，即停止所有的 goroutine，等待 gc 结束之后才恢复。\n标记-清除 (mark and sweep) 算法的 STW(stop the world)操作，就是 runtime 把所有的线程全部冻结掉，所有的线程全部冻结意味着用户逻辑是暂停的。这样所有的对象都不会被修改了，这时候去扫描是绝对安全的。\nGC 流程 Sweep Termination（清理终止）: 会触发 STW ，所有的 P（处理器） 都会进入 safe-point（安全点）； 如果当前垃圾收集循环是强制触发的，我们还需要处理还未被清理的内存管理单元 span Mark（标记阶段）: 将 _GCoff GC 状态 改成 _GCmark，开启 Write Barrier （写入屏障）、mutator assists（协助线程），将根对象入队； 恢复程序执行，mark workers（标记进程）和 mutator assists（协助线程）会开始并发标记内存中的对象。对于任何指针写入和新的指针值，都会被写屏障覆盖，而所有新创建的对象都会被直接标记成黑色； GC 执行根节点的标记，这包括扫描所有的栈、全局对象以及不在堆中的运行时数据结构。扫描 goroutine 栈会停止 P，并对栈上找到的所有指针加置灰，然后继续执行 goroutine。 依次处理灰色队列中的对象，将对象标记成黑色并将它们指向的对象标记成灰色； GC 会使用分布式终止算法（distributed termination algorithm）来检测何时不再有根标记作业或灰色对象，如果没有了 GC 会转为 mark termination（标记终止）； Mark Termination（标记终止）: STW，然后将 GC 阶段转为 _GCmarktermination，关闭 GC 工作线程以及 mutator assists（协助线程）； 清理处理器上的线程缓存； Sweep: 将 GC 状态转变至 _GCoff，初始化清理状态并关闭 Write Barrier（写入屏障）； 恢复程序执行，从此开始新创建的对象都是白色的； 后台并发清理所有的内存管理单元 目前整个 GC 流程会进行两次 STW(Stop The World), 第一次是 Mark 阶段的开始, 第二次是 Mark Termination 阶段.\n第一次STW会准备根对象的扫描, 启动写屏障(Write Barrier)和辅助 GC(mutator assist). 第二次STW会重新扫描部分根对象, 禁用写屏障(Write Barrier)和辅助 GC(mutator assist). 需要注意的是, 不是所有根对象的扫描都需要 STW, 例如扫描栈上的对象只需要停止拥有该栈的 G.\n从 go1.9 开始, 写屏障的实现使用了 Hybrid Write Barrier, 大幅减少了第二次 STW 的时间.\n写屏障 写屏障：该屏障之前的写操作和之后的写操作相比，先被系统其它组件感知。\n通俗的讲：就是在 gc 跑的过程中，可以监控对象的内存修改，并对对象进行重新标记。(实际上也是超短暂的 stw，然后对对象进行标记)\n参考 《Go 语言设计与实现》 https://zhuanlan.zhihu.com/p/359582221 ","date":"2022-11-11T00:00:00Z","permalink":"https://ggxxll.github.io/p/golang-gc/","title":"Golang GC"},{"content":"简介 CPU 线程切换的开销是影响系统处理性能的一大障碍，Go 提供了一种机制，可以在用户空间实现任务的切换，上下文切换成本更小，可以达到使用较少的线程数量实现较大并发的能力，即 GMP 模型\nG（Goroutine）: 即Golang协程，协程是一种用户态线程，比内核态线程更加轻量。使用 go 关键字可以创建一个 Golang 协程 M（Machine）: 即工作线程。实质上实现业务逻辑的载体 P（Processor）: 处理器。是 Golang 内部的定义，非 CPU。包含运行 Golang 代码的必要资源，也有调度 Goroutine 的能力 全局队列（Global Queue）：存放等待运行的 G。 P 的本地队列：同全局队列类似，存放的也是等待运行的 G，存的数量有限，不超过 256 个。新建 G时，G优先加入到 P 的本地队列，如果队列满了，则会把本地队列中一半的 G 移动到全局队列。 P 列表：所有的 P 都在程序启动时创建，并保存在数组中，最多有 GOMAXPROCS(可配置) 个。 M：线程想运行任务就得获取 P，从 P 的本地队列获取 G，P 队列为空时，M 也会尝试从全局队列拿一批 G 放到 P 的本地队列，或从其他 P 的本地队列偷一半放到自己 P 的本地队列。M 运行 G，G 执行之后，M 会从 P 获取下一个 G，不断重复下去。 关于P和M，G的个数问题 G 的数量 无限制，理论上受内存的影响，创建一个 G 的初始栈大小为2-4K，配置一般的机器也能简简单单开启数十万个 Goroutine ，而且Go语言在 G 退出的时候还会把 G 清理之后放到 P 本地或者全局的闲置列表 gFree 中以便复用。 P 的数量 由启动时环境变量 GOMAXPROCS 或者是由 runtime.GOMAXPROCS() 决定。 M 的数量 go 语言本身的限制：go 程序启动时，会设置 M 的最大数量，默认 10000。但是内核很难支持这么多的线程数，所以这个限制可以忽略。 runtime/debug 中的 SetMaxThreads 函数，设置 M 的最大数量 一个 M 阻塞了，会创建新的 M。M 与 P 的数量没有绝对关系，一个 M 阻塞，P 就会去创建或者切换另一个 M，所以，即使 P 的默认数量是 1，也有可能会创建很多个 M 出来。 P和M何时被创建 P 何时创建：在确定了 P 的最大数量 n 后，运行时系统会根据这个数量创建 n 个 P。 M 何时创建：没有足够的 M 来关联 P 并运行其中的可运行的 G。比如所有的 M 此时都阻塞住了，而 P 中还有很多就绪任务，就会去寻找空闲的 M，而没有空闲的，就会去创建新的 M。 Goroutine 调度流程 从上图我们可以分析出几个结论：\n我们通过 go func () 来创建一个 goroutine； 有两个存储 G 的队列，一个是局部调度器 P 的本地队列、一个是全局 G 队列。新创建的 G 会先保存在 P 的本地队列中，如果 P 的本地队列已经满了就会保存在全局的队列中； G 只能运行在 M 中，一个 M 必须持有一个 P，M 与 P 是 1：1 的关系。M 会从 P 的本地队列弹出一个可执行状态的 G 来执行，如果 P 的本地队列为空，就会想其他的 MP 组合偷取一个可执行的 G 来执行； 一个 M 调度 G 执行的过程是一个循环机制； 当 M 执行某一个 G 时候如果发生了 syscall 或则其余阻塞操作，M 会阻塞，如果当前有一些 G 在执行，runtime 会把这个线程 M 从 P 中摘除 (detach)，然后再创建一个新的操作系统的线程 (如果有空闲的线程可用就复用空闲线程) 来服务于这个 P； 当 M 系统调用结束时候，这个 G 会尝试获取一个空闲的 P 执行，并放入到这个 P 的本地队列。如果获取不到 P，那么这个线程 M 变成休眠状态， 加入到空闲线程中，然后这个 G 会被放入全局队列中。 调度器的生命周期流程： 特殊的 M0 和 G0 M0 是启动程序后的编号为 0 的主线程，这个 M 对应的实例会在全局变量 runtime.m0 中，不需要在 heap 上分配，M0 负责执行初始化操作和启动第一个 G， 在之后 M0 就和其他的 M 一样了。\nG0 是每次启动一个 M 都会第一个创建的 gourtine，G0 仅用于负责调度的 G，G0 不指向任何可执行的函数，每个 M 都会有一个自己的 G0。在调度或系统调用时会使用 G0 的栈空间，全局变量的 G0 是 M0 的 G0。\n调度器的设计策略 复用线程：避免频繁的创建、销毁线程，而是对线程的复用。 work stealing机制\n当本线程无可运行的 G 时，尝试从其他线程绑定的 P 偷取 G，而不是销毁线程。\nhand off机制\n当本线程因为 G 进行系统调用阻塞时，线程释放绑定的 P，把 P 转移给其他空闲的线程执行。\n利用并行：GOMAXPROCS设置 P 的数量，最多有GOMAXPROCS个线程分布在多个 CPU 上同时运行。GOMAXPROCS也限制了并发的程度，比如GOMAXPROCS = 核数/2，则最多利用了一半的CPU核进行并行。 抢占：在 coroutine 中要等待一个协程主动让出 CPU 才执行下一个协程，在 Go 中，一个 goroutine 最多占用 CPU 10ms，防止其他 goroutine 被饿死，这就是 goroutine 不同于 coroutine 的一个地方。 全局 G 队列：在新的调度器中依然有全局 G 队列，但功能已经被弱化了，当 M 执行 work stealing 从其他 P 偷不到 G 时，它可以从全局 G 队列获取 G。 注意事项 Goroutine 本质上没有数量限制，最大数量取决于内存。 对每一个使用标准编译器编译的Go程序，在运行时刻，每一个协程将维护一个栈（stack）。 一个栈是一个预申请的内存段，它做为一个内存池供某些内存块从中开辟。 在 Go 1.12 之前没有最大限制；在 Go 1.14 ~ 1.19 版本之前，一个栈的初始尺寸总是 2KiB。 从 1.19 版本开始，栈的初始尺寸是自适应的。 每个栈的尺寸在协程运行的时候将按照需要增长和收缩。 栈的最小尺寸为 2KiB。 （注意：Go 运行时维护着一个协程栈的最大尺寸限制，此限制为全局的。 如果一个协程在增长它的栈的时候超过了此限制，整个程序将崩溃。 对于目前的官方标准 Go 工具链 1.19 版本，此最大限制的默认值在 64 位系统上为 1GB，在32位系统上为 250MB。 我们可以在运行时刻调用 runtime/debug 标准库包中的 SetMaxStack 来修改此值。 另外请注意，当前的官方标准编译器实现中，实际上允许的协程栈的最大尺寸为不超过最大尺寸限制的2的幂。 所以对于默认设置，实际上允许的协程栈的最大尺寸在64 位系统上为 512MiB，在 32 位系统上为 128MiB。）\n","date":"2022-11-11T00:00:00Z","permalink":"https://ggxxll.github.io/p/golang-gmp/","title":"Golang GMP"},{"content":"简介 切片（slice）是 Golang 中一种比较特殊的数据结构，这种数据结构更便于使用和管理数据集合。切片是围绕动态数组的概念构建的，可以按需自动增长和缩小。\n切片的动态增长是通过内置函数 append() 来实现的，这个函数可以快速且高效地增长切片，也可以通过对切片再次切割，缩小一个切片的大小。\n因为切片的底层也是在连续的内存块中分配的，所以切片还能获得索引、迭代以及为垃圾回收优化的好处。\n实现 1 2 3 4 5 type slice struct { array unsafe.Pointer // 指向底层数组的指针 len int // 长度 cap int // 容量 } nil 和 空切片 nil 切片:\n1 var nums []int 空切片：\n1 2 3 4 //使用make s1 := make([]int, 0) //使用切片字面量 s2 := []int{} 空切片与 nil 切片的区别:\nnil 切片 == nil , 而空切片 != nil ，在使用切片进行逻辑运算时尽量不要使用空切片 空切片指针指向一个特殊的 zerobase 地址，而 nil 为 0 在 JSON 序列化有区别： nil 切片为 {\u0026quot;values\u0026quot;:null}, 而空切片为 {\u0026quot;value\u0026quot; []} 追加 对于 append 向 slice 添加元素的步骤：\n如果 slice 容量够用，则追加新元素进去，slice.len++，返回原来的slice。 当原容量不够，则 slice 先扩容，扩容之后得到新的 slice，将元素追加进新的 slice，slice.len++，返回新的 slice。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 func main(){ a := []int{1, 2, 3, 4, 5} b := a[:3] // b 作为 a 的子切片，两者指向的是用一个底层数组，所以 cap 相同，只是限制了 b 的 len t.Log(a, len(a), cap(a)) // [1 2 3 4 5] 5 5 t.Log(b, len(b), cap(b)) // [1 2 3] 3 5 // 这种情况下，a b 会互相影响 a[0] = 0 b[1] = 1 t.Log(a, len(a), cap(a)) // [0 1 3 4 5] 5 5 t.Log(b, len(b), cap(b)) // [0 1 3] 3 5 // 在未产生扩容的情况下，b 的 append 操作会覆盖 a 中的值 // 如下：a[3] 被覆盖为 6 了 b = append(b, 6) t.Log(a, len(a), cap(a)) // [0 1 3 6 5] 5 5 t.Log(b, len(b), cap(b)) // [0 1 3 6] 4 5 // 产生扩容，b 指向的底层数组是新的数组了，a b 不再互相影响 b = append(b, 7, 8, 9, 10) t.Log(a, len(a), cap(a)) // [0 1 3 6 5] 5 5 t.Log(b, len(b), cap(b)) // [0 1 3 6 7 8 9 10] 8 10 a[0] = -1 b[1] = -2 t.Log(a, len(a), cap(a)) // [-1 1 3 6 5] 5 5 t.Log(b, len(b), cap(b)) // [0 -2 3 6 7 8 9 10] 8 10 } 扩容 go1.15.15 go1.15，go1.16 和 go1.17 版本的扩容的边界条件都是 1024， 但是 go1.15 基于 slice.len 判断的，go1.16 和 go1.17版本是基于 slice.cap 容量判断的；\n扩容的基本原则：\n如果要扩容的新容量已经超过旧容量的 2 倍，那么就直接使用新容量。 否则： 如果当前切片的长度小于 1024 就会将容量翻倍； 否则就会每次增加 25% 的容量，直到新容量大于期望容量。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 // go1.15.6 源码 src/runtime/slice.go func growslice(et *_type, old slice, cap int) slice { ... // 计算扩容部分 // 其中，cap : 所需容量，newcap : 最终申请容量 newcap := old.cap doublecap := newcap + newcap if cap \u0026gt; doublecap { newcap = cap } else { if old.len \u0026lt; 1024 { // 1.15 之后使用 old.cap newcap = doublecap } else { for 0 \u0026lt; newcap \u0026amp;\u0026amp; newcap \u0026lt; cap { newcap += newcap / 4 } if newcap \u0026lt;= 0 { newcap = cap } } } ... } go1.18 在 go1.18 之后， 扩容的基本原则：\n如果新容量已经超过旧容量的 2 倍，那么就直接使用新容量。 否则： 如果旧容量小于 256，则新的扩容会是原来的 2 倍。 否则使用计算公式 newcap += (newcap + 3 * 256) / 4，直到新容量大于期望容量：从小切片的 2 倍增长过渡到大切片的 1.25 倍增长，这个公式在两者之间给出了平滑的过渡。 扩容函数 growslice\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 func growslice(et *_type, old slice, cap int) slice { if cap \u0026lt; old.cap { panic(errorString(\u0026#34;growslice: cap out of range\u0026#34;)) } if et.size == 0 { return slice{unsafe.Pointer(\u0026amp;zerobase), old.len, cap} } newcap := old.cap doublecap := newcap + newcap if cap \u0026gt; doublecap { newcap = cap } else { const threshold = 256 if old.cap \u0026lt; threshold { newcap = doublecap } else { for 0 \u0026lt; newcap \u0026amp;\u0026amp; newcap \u0026lt; cap { newcap += (newcap + 3*threshold) / 4 } if newcap \u0026lt;= 0 { newcap = cap } } } ... } go1.20 在 go1.19 之后，扩容参数变更为 newLen， 扩容的基本原则：\n如果 newLen 已经超过 oldCap 的 2 倍，那么就直接使用 newLen 作为新容量。 否则： 如果 oldCap 小于 256，则新的扩容会 oldCap * 2。 否则使用计算公式 newcap += (newcap + 3 * 256) / 4，直到新容量大于期望容量：从小切片的 2 倍增长过渡到大切片的 1.25 倍增长，这个公式在两者之间给出了平滑的过渡。 扩容函数 growslice\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 // arguments: // //\toldPtr = slice.array 指针 //\tnewLen = oldLen + num //\toldCap = 原始 slice 的容量 //\tnum = 要添加的元素个数 //\tet = element type func growslice(oldPtr unsafe.Pointer, newLen, oldCap, num int, et *_type) slice { oldLen := newLen - num if newLen \u0026lt; 0 { panic(errorString(\u0026#34;growslice: len out of range\u0026#34;)) } if et.size == 0 { // append should not create a slice with nil pointer but non-zero len. // We assume that append doesn\u0026#39;t need to preserve oldPtr in this case. return slice{unsafe.Pointer(\u0026amp;zerobase), newLen, newLen} } newcap := oldCap doublecap := newcap + newcap if newLen \u0026gt; doublecap { newcap = newLen } else { const threshold = 256 if oldCap \u0026lt; threshold { newcap = doublecap } else { for 0 \u0026lt; newcap \u0026amp;\u0026amp; newcap \u0026lt; newLen { newcap += (newcap + 3*threshold) / 4 } if newcap \u0026lt;= 0 { newcap = newLen } } } ... } ","date":"2022-11-11T00:00:00Z","permalink":"https://ggxxll.github.io/p/golang-slice/","title":"Golang Slice"},{"content":"简介 一般情况下解决并发读写 map 的思路是加一把大锁，或者把一个 map 分成若干个小 map，对 key 进行哈希，只操作相应的小 map。前者锁的粒度比较大，影响效率；后者实现起来比较复杂，容易出错。\n而使用 sync.Map 之后，对 map 的读写，不需要加锁。并且它通过空间换时间的方式，使用 read 和 dirty 两个 map 来进行读写分离，降低锁时间来提高效率。\nsync.Map 适用于读多写少的场景。对于写多的场景，会导致 read 缓存失效，需要加锁，导致冲突变多；而且由于未命中 read 次数过多，导致 dirty 提升为 read ，这是一个 O(N) 的操作，会进一步降低性能。\n源码解析 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 type Map struct { mu Mutex read atomic.Value // readOnly dirty map[any]*entry misses int } // readOnly is an immutable struct stored atomically in the Map.read field. type readOnly struct { m map[any]*entry amended bool // true if the dirty map contains some key not in m. } // expunged is an arbitrary pointer that marks entries which have been deleted // from the dirty map. var expunged = unsafe.Pointer(new(any)) // An entry is a slot in the map corresponding to a particular key. type entry struct { p unsafe.Pointer // *any } 互斥量 mu 保护 read 和 dirty。\nread 是 atomic.Value 类型，可以并发地读。但如果需要更新 read，则需要加锁保护。对于 read 中存储的 entry 字段，可能会被并发地 CAS 更新。但是如果要更新一个之前已被删除的 entry，则需要先将其状态从 expunged 改为 nil，再拷贝到 dirty 中，然后再更新。\ndirty 是一个非线程安全的原始 map。包含新写入的 key，并且包含 read 中的所有未被删除的 key。这样，可以快速地将 dirty 提升为 read 对外提供服务。如果 dirty 为 nil，那么下一次写入时，会新建一个新的 dirty，这个初始的 dirty 是 read 的一个拷贝，但除掉了其中已被删除的 key。\n每当从 read 中读取失败，都会将 misses 的计数值加 1，当加到一定阈值以后，需要将 dirty 提升为 read，以期减少 miss 的情形。\nread 和 dirty 的存储方式是不一致的。前者使用 atomic.Value，后者只是单纯的使用 map。\n原因是 read 使用 lock free 操作，必须保证 load/store 的原子性；而 dirty 的 load+store 操作是由 lock（就是 mu）来保护的。\nentry 对象是一个指针，指向 value。看来，read 和 dirty 各自维护一套 key，key 指向的都是同一个 value。也就是说，只要修改了这个 entry，对 read 和 dirty 都是可见的。这个指针的状态有三种：\nnil: 说明这个键值对已被删除，并且 m.dirty == nil，或 m.dirty[k] 指向该 entry。 expunged: 说明这条键值对已被删除，并且 m.dirty != nil，且 m.dirty 中没有这个 key。 正常：指向一个正常的值，表示实际 any 的地址，并且被记录在 m.read.m[key] 中。如果这时 m.dirty != nil，那么它也被记录在 m.dirty[key] 中。两者实际上指向的是同一个值。 当删除 key 时，并不实际删除。一个 entry 可以通过原子地（CAS 操作）设置 p 为 nil 被删除。如果之后创建 m.dirty，nil 又会被原子地设置为 expunged，且不会拷贝到 dirty 中。\n如果 p 不为 expunged，和 entry 相关联的这个 value 可以被原子地更新；如果 p == expunged，那么仅当它初次被设置到 m.dirty 之后，才可以被更新。\nLoad 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 func (m *Map) Load(key any) (value any, ok bool) { // 因read只读，线程安全，优先读取 read, _ := m.read.Load().(readOnly) e, ok := read.m[key] // 如果read没有，并且dirty有新数据，那么去dirty中查找 if !ok \u0026amp;\u0026amp; read.amended { m.mu.Lock() // 双重检查（原因是前文的if判断和加锁非原子的，害怕这中间发生故事） read, _ = m.read.Load().(readOnly) e, ok = read.m[key] // 如果read中还是不存在，并且dirty中有新数据 if !ok \u0026amp;\u0026amp; read.amended { e, ok = m.dirty[key] // m计数+1 m.missLocked() } m.mu.Unlock() } if !ok { return nil, false } return e.load() } func (m *Map) missLocked() { m.misses++ if m.misses \u0026lt; len(m.dirty) { return } // 将dirty置给read，因为穿透概率太大了(原子操作，耗时很小) m.read.Store(readOnly{m: m.dirty}) m.dirty = nil m.misses = 0 } func (e *entry) load() (value any, ok bool) { p := atomic.LoadPointer(\u0026amp;e.p) if p == nil || p == expunged { return nil, false } return *(*any)(p), true } Delete 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 func (m *Map) Delete(key any) { // 读出read，断言为readOnly类型 read, _ := m.read.Load().(readOnly) e, ok := read.m[key] // 如果read中没有，并且dirty中有新元素，那么就去dirty中去找。这里用到了amended，当read与dirty不同时为true，说明dirty中有read没有的数据。 if !ok \u0026amp;\u0026amp; read.amended { m.mu.Lock() // 再检查一次，因为前文的判断和锁不是原子操作，防止期间发生了变化。 read, _ = m.read.Load().(readOnly) e, ok = read.m[key] if !ok \u0026amp;\u0026amp; read.amended { // 直接删除 delete(m.dirty, key) } m.mu.Unlock() } if ok { // 如果read中存在该key，则将该value 赋值nil（采用标记的方式删除！） e.delete() } } func (e *entry) delete() (hadValue bool) { for { // 数据的指针 p := atomic.LoadPointer(\u0026amp;e.p) if p == nil || p == expunged { return false } // 原子操作 if atomic.CompareAndSwapPointer(\u0026amp;e.p, p, nil) { return true } } } Store 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 func (m *Map) Store(key, value any) { // 如果m.read存在这个key，并且没有被标记删除，则尝试更新。 read, _ := m.read.Load().(readOnly) if e, ok := read.m[key]; ok \u0026amp;\u0026amp; e.tryStore(\u0026amp;value) { return } // 如果read不存在或者已经被标记删除 m.mu.Lock() read, _ = m.read.Load().(readOnly) if e, ok := read.m[key]; ok { // read 存在该key // 如果entry被标记expunge，则表明dirty没有key，可添加入dirty，并更新entry。 if e.unexpungeLocked() { // 加入dirty中，这儿是指针 m.dirty[key] = e } // 更新value值 e.storeLocked(\u0026amp;value) } else if e, ok := m.dirty[key]; ok { // dirty 存在该key，更新 e.storeLocked(\u0026amp;value) } else { // read 和 dirty都没有 // 如果read与dirty相同，则触发一次dirty刷新（因为当read重置的时候，dirty已置为nil了） if !read.amended { // 将read中未删除的数据加入到dirty中 m.dirtyLocked() // amended标记为read与dirty不相同，因为后面即将加入新数据。 m.read.Store(readOnly{m: read.m, amended: true}) } m.dirty[key] = newEntry(value) } m.mu.Unlock() } // 将read中未删除的数据加入到dirty中 func (m *Map) dirtyLocked() { if m.dirty != nil { return } read, _ := m.read.Load().(readOnly) m.dirty = make(map[any]*entry, len(read.m)) // 遍历read。 for k, e := range read.m { // 通过此次操作，dirty中的元素都是未被删除的，可见标记为expunged的元素不在dirty中！！！ if !e.tryExpungeLocked() { m.dirty[k] = e } } } // 判断entry是否被标记删除，并且将标记为nil的entry更新标记为expunge func (e *entry) tryExpungeLocked() (isExpunged bool) { p := atomic.LoadPointer(\u0026amp;e.p) for p == nil { // 将已经删除标记为nil的数据标记为expunged if atomic.CompareAndSwapPointer(\u0026amp;e.p, nil, expunged) { return true } p = atomic.LoadPointer(\u0026amp;e.p) } return p == expunged } // 对entry尝试更新 （原子cas操作） func (e *entry) tryStore(i *any) bool { p := atomic.LoadPointer(\u0026amp;e.p) if p == expunged { return false } for { if atomic.CompareAndSwapPointer(\u0026amp;e.p, p, unsafe.Pointer(i)) { return true } p = atomic.LoadPointer(\u0026amp;e.p) if p == expunged { return false } } } // read里 将标记为expunge的更新为nil func (e *entry) unexpungeLocked() (wasExpunged bool) { return atomic.CompareAndSwapPointer(\u0026amp;e.p, expunged, nil) } // 更新entry func (e *entry) storeLocked(i *any) { atomic.StorePointer(\u0026amp;e.p, unsafe.Pointer(i)) } ","date":"2022-11-11T00:00:00Z","permalink":"https://ggxxll.github.io/p/golang-sync.map/","title":"Golang sync.Map"},{"content":"编码类型 ziplist 介绍 ziplist 编码的 Zset 使用紧挨在一起的压缩列表节点来保存，第一个节点保存 member，第二个保存 score。ziplist 内的集合元素按 score 从小到大排序，其实质是一个双向链表。虽然元素是按 score 有序排序的， 但对 ziplist 的节点指针只能线性地移动，所以在 REDIS_ENCODING_ZIPLIST 编码的 Zset 中， 查找某个给定元素的复杂度为 O(N)。\n结构 各个部分在内存上是前后相邻的并连续的，每一部分作用如下：\nzlbytes： 存储一个无符号整数，固定四个字节长度（32bit），用于存储压缩列表所占用的字节（也包括 zlbytes 本身占用的4个字节），当重新分配内存的时候使用，不需要遍历整个列表来计算内存大小。 zltail： 存储一个无符号整数，固定四个字节长度（32bit），表示 ziplist 表中最后一项（entry）在 ziplist 中的偏移字节数。 zltail 的存在，使得我们可以很方便地找到最后一项（不用遍历整个ziplist），从而可以在ziplist尾端快速地执行 push 或 pop 操作。 zllen： 压缩列表包含的节点个数，固定两个字节长度（16bit）， 表示ziplist中数据项（entry）的个数。由于zllen字段只有16bit，所以可以表达的最大值为2^16-1。 注意点：如果ziplist中数据项个数超过了16bit能表达的最大值，ziplist仍然可以表示。ziplist是如何做到的？\n如果 zllen 小于等于2^16-2（也就是不等于2^16-1），那么 zllen 就表示ziplist中数据项的个数；否则，也就是 zllen 等于 16bit 全为 1 的情况，那么 zllen 就不表示数据项个数了，这时候要想知道 ziplist 中数据项总数，那么必须对 ziplist 从头到尾遍历各个数据项，才能计数出来。\nentry，表示真正存放数据的数据项，长度不定。一个数据项（entry）也有它自己的内部结构。\nzlend， ziplist最后1个字节，值固定等于255，其是一个结束标记。\nskiplist 介绍 skiplist 编码的 Zset 底层为一个被称为 zset 的结构体，这个结构体中包含一个字典和一个跳跃表。跳跃表按 score 从小到大保存所有集合元素，查找时间复杂度为平均 O(logN)，最坏 O(N) 。字典则保存着从 member 到 score 的映射，这样就可以用 O(1) 的复杂度来查找 member 对应的 score 值。虽然同时使用两种结构，但它们会通过指针来共享相同元素的 member 和 score，因此不会浪费额外的内存。\n详解 跳表(skip List)是一种随机化的数据结构，基于并联的链表，实现简单，插入、删除、查找的复杂度均为O(logN)。简单说来跳表也是链表的一种，只不过它在链表的基础上增加了跳跃功能，正是这个跳跃的功能，使得在查找元素时，跳表能够提供O(logN)的时间复杂度。\nQ\u0026amp;A Redis为什么用skiplist而不用平衡树？ 这里从内存占用、对范围查找的支持和实现难易程度这三方面总结的原因。\n也不是非常耗费内存，实际上取决于生成层数函数里的概率 p，取决得当的话其实和平衡树差不多。 因为有序集合经常会进行 ZRANGE 或 ZREVRANGE 这样的范围查找操作，跳表里面的双向链表可以十分方便地进行这类操作。 实现简单，ZRANK 操作还能达到 o(logn) 的时间复杂度。 类型编码 String 字符串的内部编码有三种：\nint： 8 字节的长整形 embstr：小于等于 39 字节的字符串 raw：大于 39 字节的字符串 Hash 哈希的内部编码有两种：\nziplist：当哈希类型元素个数小于 hash-max-ziplist-entries 配置（默认 512 个）、同时所有值都小于 hash-max-ziplist-value 配置（默认 64 字节）时，Redis 选择使用 ziplist 作为哈希的内部实现 hashtable：当哈希类型不满足 ziplist 的条件时，即选择 hashtable 作为内部实现，因为此时 ziplist 的读写效率会下降，而 hashtable 的读写时间复杂度为 O(1) List 列表的内部编码有两种：\nziplist：当列表的元素个数小于 list-max-ziplist-entries 配置（默认 512 个）、同时所有值都小于 list-max-ziplist-value 配置（默认 64 字节）时，Redis 选择使用 ziplist 作为哈希的内部实现 linkedlist：当列表类型不满足 ziplist 条件时，即选择链表作为列表的内部实现 quicklist：它是一个以 ziplist 为节点的 linkedlist，结合了 ziplist 和 linkedlist 两者的优势 Set 集合的内部编码有两种：\nintset：当列表的元素个数小于 set-max-intset-entries 配置（默认 512 个）时使用 hashtable：不满足 intset 条件时使用 ZSet 有序集合对象的编码有两种。\nziplist：当列表的元素个数小于 zset-max-ziplist-entries 配置（默认 128 个）、同时所有值都小于 zset-max-ziplist-value 配置（默认 64 字节）时，Redis 选择使用 ziplist 作为哈希的内部实现\nskiplist：不满足 ziplist 条件时使用\n","date":"2022-11-11T00:00:00Z","permalink":"https://ggxxll.github.io/p/redis-%E7%BC%96%E7%A0%81/","title":"Redis 编码"},{"content":"简介 Redis 运行时是将数据保存在内存中的，如果服务器宕机或者重启，内存中的数据必然会丢失。所以，必须要把数据持久化到磁盘，以便服务器故障时进行数据恢复。\nRedis持久化提供了两种方式，RDB（RedisDB）和 AOF（Appendonly File）。\nRDB RDB 就是在特定的条件下，将 Redis 的内存数据快照以二进制的方式保存到磁盘，文件名称 dump.rdb 。\n调用 fork 方式创建子进程进行数据的持久化，所以会保留了持久化开始时刻的数据状况。\n关于 RDB，Redis提供了2种方式：\n命令：可以通过 save （阻塞）和 bgsave 命令，触发持久化写文件操作。 配置文件： 1 2 3 4 5 6 7 # save \u0026lt;seconds\u0026gt; \u0026lt;changes\u0026gt; # save \u0026#34;\u0026#34; save 900 1 # 900 秒有 1 个 key 在被修改时持久化 save 300 10 save 60 10000 rdbcompression yes # 默认开启数据压缩 优点：\n适合大规模的数据恢复。 如果业务对数据完整性和一致性要求不高，RDB是很好的选择。 缺点：\n数据的完整性和一致性不高，因为RDB可能在最后一次备份时宕机了。 备份时占用内存，因为 Redis 在备份时会独立创建一个子进程，将数据写入到一个临时文件（此时内存中的数据是原来的两倍哦），最后再将临时文件替换之前的备份文件。 AOF AOF Redis 默认不开启。它的出现是为了弥补 RDB 的不足（数据的不一致性），所以它采用日志的形式来记录每个写操作，并追加到文件中。Redis 重启的会根据日志文件的内容将写指令从前到后执行一次以完成数据的恢复工作。。\n为了降低IO消耗，AOF写文件时，会先将数据写到缓冲区，然后再把缓冲区的内容 flush 到磁盘，这个过程叫做 fsync。\nAOF 的 flush 频率控制：\n1 2 3 appendfsync always //每次写操作都flush，影响性能 appendfsync everysec //每秒flush appendfsync no //消极等待OS刷新(一般30s),可能丢失数据 AOF 重写 前面也说到了，AOF的工作原理是将写操作追加到文件中，文件的冗余内容会越来越多。所以聪明的 Redis 新增了重写机制。当 AOF 文件的大小超过所设定的阈值时，Redis 就会对 AOF 文件的内容压缩。\n重写的原理：Redis 会fork出一条新进程，读取内存中的数据，并重新写到一个临时文件中。并没有读取旧文件（你都那么大了，我还去读你？？？ o(ﾟДﾟ)っ傻啊！）。最后替换旧的aof文件。\n触发机制：当 AOF 文件大小是上次 rewrite 后大小的一倍且文件大于 64M 时触发。这里的“一倍”和“64M”可以通过配置文件修改。\n触发机制的配置参数：\n1 2 auto-aof-rewrite-percentage 100 auto-aof-rewrite-min-size 64mb 优点：数据的完整性和一致性更高\n缺点：因为AOF记录的内容多，文件会越来越大，数据恢复也会越来越慢。\n","date":"2022-11-11T00:00:00Z","permalink":"https://ggxxll.github.io/p/redis-%E6%8C%81%E4%B9%85%E5%8C%96/","title":"Redis 持久化"},{"content":"简介 跳表全称为跳跃列表，它允许快速查询，插入和删除一个有序连续元素的数据链表。跳跃列表的平均查找和插入时间复杂度都是 O(logn)。快速查询是通过维护一个多层次的链表，且每一层链表中的元素是前一层链表元素的子集（见示意图）。一开始时，算法在最稀疏的层次进行搜索，直至需要查找的元素在该层两个相邻的元素中间。这时，算法将跳转到下一个层次，重复刚才的搜索，直到找到需要查找的元素为止。\n一张跳跃列表的示意图。每个带有箭头的框表示一个指针, 而每行是一个稀疏子序列的链表；底部的编号框（黄色）表示有序的数据序列。查找从顶部最稀疏的子序列向下进行, 直至需要查找的元素在该层两个相邻的元素中间。\n","date":"2022-11-11T00:00:00Z","permalink":"https://ggxxll.github.io/p/%E8%B7%B3%E8%A1%A8/","title":"跳表"},{"content":"MVCC 全称 Multi-Version Concurrency Control, 即多版本并发控制。MVCC 是一种并发控制的方法，一般在数据库管理系统中，实现对数据库的并发访问，在编程语言中实现事务内存。\nMVCC 在 MySQL InnoDB 中的实现主要是为了提高数据库并发性能，用更好的方式去处理读-写冲突，做到即使有读写冲突时，也能做到不加锁，非阻塞并发读\n当前读和快照读 当前读 像 select lock in share mode (共享锁), select for update; update; insert; delete (排他锁)这些操作都是一种当前读，为什么叫当前读？就是它读取的是记录的最新版本，读取时还要保证其他并发事务不能修改当前记录，会对读取的记录进行加锁\n快照读 像不加锁的 select 操作就是快照读，即不加锁的非阻塞读；快照读的前提是隔离级别不是串行级别，串行级别下的快照读会退化成当前读；之所以出现快照读的情况，是基于提高并发性能的考虑，快照读的实现是基于多版本并发控制，即 MVCC ,可以认为 MVCC 是行锁的一个变种，但它在很多情况下，避免了加锁操作，降低了开销；既然是基于多版本，即快照读可能读到的并不一定是数据的最新版本，而有可能是之前的历史版本\nLog BinLog binlog：是mysql服务层产生的日志，常用来进行数据恢复、数据库复制，常见的mysql主从架构，就是采用slave同步master的binlog实现的, 另外通过解析binlog能够实现mysql到其他数据源（如 ElasticSearch )的数据复制。\nRedoLog redo log：记录了数据操作在物理层面的修改，mysql中使用了大量缓存，缓存存在于内存中，修改操作时会直接修改内存，而不是立刻修改磁盘，当内存和磁盘的数据不一致时，称内存中的数据为脏页(dirty page)。为了保证数据的安全性，事务进行中时会不断的产生 redo log，在事务提交时进行一次 flush 操作，保存到磁盘中, redo log 是按照顺序写入的，磁盘的顺序读写的速度远大于随机读写。当数据库或主机失效重启时，会根据 redo log 进行数据的恢复，如果 redo log 中有事务提交，则进行事务提交修改数据。这样实现了事务的原子性、一致性和持久性。\nUndoLog undo log：除了记录 redo log 外，当进行数据修改时还会记录 undo log，undo log用于数据的撤回操作，它记录了修改的反向操作，比如，插入对应删除，修改对应修改为原来的数据，通过 undo log 可以实现事务回滚，并且可以根据 undo log 回溯到某个特定的版本的数据，实现 MVCC。\nReadView MVCC 只在 read-committed（RC） 和 repeatable-read（RR） 两个隔离级别下工作，而 read-committed 和 repeatable-read 的区别就在于它们生成 ReadView 的时机不同。\n对于使用 read-committed 和 repeatable-read 隔离级别的事务来说，都必须保证读到已经提交了的事务修改过的记录，也就是说假如另一个事务已经修改了记录但是尚未提交，是不能直接读取最新版本的记录的。\n核心问题就是：需要判断一下，版本链中的哪个版本是当前事务可见的。\n为此，InnoDB 提出了一个 ReadView 的概念，这个 ReadView 中有个 id 列表 trx_ids 来存储系统中当前活跃着的读写事务，也就是 begin 了还未 commit 或 rollback 的事务。其中最主要的与可见性相关的属性如下：\nup_limit_id：当前已经提交的事务号 + 1，事务号 \u0026lt; up_limit_id ，对于当前 Read View 都是可见的。理解起来就是创建 Read View 视图的时候，之前已经提交的事务对于该事务肯定是可见的。 low_limit_id：当前最大的事务号 + 1，事务号 \u0026gt;= low_limit_id，对于当前 Read View 都是不可见的。理解起来就是在创建 Read View 视图之后创建的事务对于该事务肯定是不可见的。 trx_ids：为活跃事务 id 列表，即 Read View 初始化时当前未提交的事务列表。所以当进行 RR 读的时候，trx_ids 中的事务对于本事务是不可见的（除了自身事务，自身事务对于表的修改对于自己当然是可见的）。理解起来就是创建 Read View 时，将当前活跃事务 ID 记录下来，后续即使他们提交对于本事务也是不可见的。 creator_trx_id：当前事务自身的 id 可重复读隔离级别下，ReadView 只会在第一次查询时创建，同一个事务中后续所有的查询共用一个 ReadView，由此便解决了不可重复读的问题。\n读已提交隔离级别下，每次查询都会创建一个新的 ReadView。新建的 ReadView 会更新 creator_trx_id 以外的其余字段，因此不可重复读现象依然存在。但是由于ReadView可以判断出修改此数据的事务是否已经提交，因此可以避免脏读的出现。\n其次，从上述MVCC实现逻辑中可以发现，没有任何加锁、获取锁的操作，因此MVCC读操作不会因为等待锁而阻塞（也就是常说的非阻塞读）。\nMVCC 原理 MySQL的每行记录逻辑上其实是一个链表。\nMySQL行记录中除了记录业务数据外，还有隐藏的 trx_id 和 roll_pointer\ntrx_id：表示最近修改的事务的id ，每次一个事务对某条聚簇索引记录进行改动时，都会把该事务的事务 id 赋值给 trx_id 隐藏列。新增一个事务时，trx_id 会递增，因此 trx_id 能够表示事务开始的先后顺序。 roll_pointer：指向该行上一个版本的地址，每次对某条聚簇索引记录进行改动时，都会把旧的版本写入到 UndoLog 中，然后这个隐藏列就相当于一个指针，可以通过它来找到该记录修改前的信息。 MVCC 能解决什么问题，好处是？ 数据库并发场景有三种，分别为：\n读-读：不存在任何问题，也不需要并发控制 读-写：有线程安全问题，可能会造成事务隔离性问题，可能遇到脏读，幻读，不可重复读 写-写：有线程安全问题，可能会存在更新丢失问题，比如第一类更新丢失，第二类更新丢失 ","date":"2022-11-10T00:00:00Z","permalink":"https://ggxxll.github.io/p/mysql-mvcc/","title":"Mysql MVCC"},{"content":"介绍 索引是一个单独的、存储在磁盘上的数据库结构，它们包含着对数据表里所有记录的引用指针。使用索引用于快速找出在某个或多个列中有一特定值的行，所有MySQL列类型都可以被索引，对相关列使用索引是提高查询操作速度的最佳途径。\nMySQL索引的建立对于MySQL的高效运行是很重要的，索引可以大大提高MySQL的检索速度。比如我们在查字典的时候，前面都有检索的拼音和偏旁、笔画等，然后找到对应字典页码，这样然后就打开字典的页数就可以知道我们要搜索的某一个key的全部值的信息了。\n创建索引时，你需要确保该索引是应用在 SQL 查询语句的条件(一般作为 WHERE 子句的条件)，而不是在 SELECT 的字段中，实际上，索引也是一张“表”，该表保存了主键与索引字段，并指向实体表的记录，虽然索引大大提高了查询速度，同时却会降低更新表的速度，如对表进行 INSERT、UPDATE和DELETE。因为更新表时，MySQL 不仅要保存数据，还要保存一下索引文件，建立索引会占用磁盘空间的索引文件。说白了索引就是用来提高速度的，但是就需要维护索引造成资源的浪费，所以合理的创建索引是必要的。\n分类 先去官网文档看看支持的索引类型，索引的实现方式如下图所示：官网\n由于本文是基于 MySQL 的 InnoDB 存储引擎，索引我们主要看第一个表格，其他的表格可以自行的观看，都不难，从表格我们可以看出来，InnoDB存储引擎索引只支持BTREE类型的索引，索引的类别有Primary Key，Unique，Key，FULLTEXT和SPATIAL。当然也有其他的分法，按照索引列的数量分为单列索引和组合索引。\nPrimary Key（聚集索引）：InnoDB 存储引擎的表会存在主键（唯一非 null），如果建表的时候没有指定主键，则会使用第一非空的唯一索引作为聚集索引，否则 InnoDB 会自动帮你创建一个不可见的、长度为 6 字节的 row_id 用来作为聚集索引。 单列索引：单列索引即一个索引只包含单个列 组合索引：组合索引指在表的多个字段组合上创建的索引，只有在查询条件中使用了这些字段的左边字段时，索引才会被使用。使用组合索引时遵循最左前缀集合 Unique（唯一索引）：索引列的值必须唯一，但允许有空值。若是组合索引，则列值的组合必须唯一。主键索引是一种特殊的唯一索引，不允许有空值 Key（普通索引）：是 MySQL 中的基本索引类型，允许在定义索引的列中插入重复值和空值 FULLTEXT（全文索引）：全文索引类型为FULLTEXT，在定义索引的列上支持值的全文查找，允许在这些索引列中插入重复值和空值。全文索引可以在 CHAR、VARCHAR 或者 TEXT 类型的列上创建 SPATIAL（空间索引）：空间索引是对空间数据类型的字段建立的索引，MySQL 中的空间数据类型有4种，分别是 GEOMETRY、POINT、LINESTRING和POLYGON。MySQL 使用 SPATIAL 关键字进行扩展，使得能够用于创建正规索引类似的语法创建空间索引。创建空间索引的列必须声明为 NOT NULL 创建原则 索引并非越多越好，一个表中如果有大量的索引，不仅占用磁盘空间，而且会影响 INSERT、DELETE、UPDATE 等语句的性能，因为在表中的数据更改的同时，索引也会进行调整和更新 避免对经常更新的表创建过多的索引，并且索引中的列尽可能少。而对经常用于查询的字段应该创建索引，但要避免添加不必要的字段。 数据量小的表最好不要使用索引，由于数据较少，查询花费的时间可能比遍历索引的时间还要短，索引可能不会产生优化效果。 在条件表达式中经常用到的不同值较多的列上建立索引，在不同值很少的列上不要建立索引。比如在学生表的“性别”字段上只有“男”与“女”两个不同值，因此就无须建立索引。如果建立索引，不但不会提高查询效率，反而会严重降低数据更新速度。 当唯一性是某种数据本身的特征时，指定唯一索引。使用唯一索引需能确保定义的列的数据完整性，以提高查询速度。 在频繁进行排序或分组（即进行 group by 或 order by 操作）的列上建立索引，如果待排序的列有多个，可以在这些列上建立组合索引。 搜索的索引列，不一定是所要选择的列。换句话说，最适合索引的列是出现在 WHERE 子句中的列，或连接子句中指定的列，而不是出现在 SELECT 关键字后的选择列表中的列。 使用短索引。如果对字符串列进行索引，应该指定一个前缀长度，只要有可能就应该这样做。例如，有一个 CHAR(200) 列，如果在前10个或20个字符内，多数值是唯一的，那么就不要对整个列进行索引。对前10个或20个字符进行索引能够节省大量索引空间，也可能会使查询更快。较小的索引涉及的磁盘 IO 较少，较短的值比较起来更快。更为重要的是，对于较短的键值，索引高速缓存中的块能容纳更多的键值，因此，MySQL 也可以在内存中容纳更多的值。这样就增加了找到行而不用读取索引中较多块的可能性。 利用最左前缀。在创建一个 n 列的索引时，实际是创建了 MySQL 可利用的 n 个索引。多列索引可起几个索引的作用，因为可利用索引中最左边的列集来匹配行。这样的列集称为最左前缀。 对于 InnoDB 存储引擎的表，记录默认会按照一定的顺序保存，如果有明确定义的主键，则按照主键顺序保存。如果没有主键，但是有唯一索引，那么就是按照唯一索引的顺序保存。如果既没有主键又没有唯一索引，那么表中会自动生成一个内部列，按照这个列的顺序保存。按照主键或者内部列进行的访问是最快的，所以InnoDB表尽量自己指定主键，当表中同时有几个列都是唯一的，都可以作为主键的时候，要选择最常作为访问条件的列作为主键，提高查询的效率。另外，还需要注意，InnoDB 表的普通索引都会保存主键的键值，所以主键要尽可能选择较短的数据类型，可以有效地减少索引的磁盘占用，提高索引的使用效果 索引的管理和使用 EXPLAIN 基本语法：EXPLAIN SELECT ...\n结果：\nid：SELECT 识别符。这是 SELECT 的查询序列号,表示查询中执行 SELECT 子句或操作表的顺序, id 相同，执行顺序从上到下,id 不同，id 值越大执行优先级越高 select_type：表示 SELECT 语句的类型。它可以是以下几种取值： SIMPLE：表示简单查询，其中不包括连接查询和子查询； PRIMARY：表示主查询，或者是最外层的查询语句，最外层查询为 PRIMARY ，也就是最后加载的就是- PRIMARY； UNION：表示连接查询的第2个或后面的查询语句， 不依赖于外部查询的结果集 DEPENDENT UNION：连接查询中的第2个或后面的 SELECT 语句，依赖于外面的查询； UNION RESULT：连接查询的结果； SUBQUERY：子查询中的第1个 SELECT 语句；不依赖于外部查询的结果集 DEPENDENT SUBQUERY：子查询中的第1个 SELECT，依赖于外面的查询； DERIVED：导出表的 SELECT（FROM 子句的子查询）,MySQL 会递归执行这些子查询，把结果放在临时表里。 DEPENDENT DERIVED：派生表依赖于另一个表 MATERIALIZED：物化子查询 UNCACHEABLE SUBQUERY：子查询，其结果无法缓存，必须针对外部查询的每一行重新进行评估 UNCACHEABLE UNION：UNION中的第二个或随后的 SELECT 查询，属于不可缓存的子查询 table：表示查询的表 partitions：查询将从中匹配记录的分区。该值适用NULL于未分区的表 type：表示表的连接类型 system：该表是仅有一行的系统表。这是 const 连接类型的一个特例 const： 数据表最多只有一个匹配行，它将在查询开始时被读取，并在余下的查询优化中作为常量对待。const 表查询速度很快，因为只读取一次,const 用于使用常数值比较 PRIMARY KEY 或 UNIQUE 索引的所有部分的场合。 eq_ref：对于每个来自前面的表的行组合，从该表中读取一行,可以用于使用“＝”运算符进行比较的索引列 。比较值可以是常量，也可以是使用在此表之前读取的表中列的表达式 ref：对于来自前面的表的任意行组合，将从该表中读取所有匹配的行，ref可以用于使用“＝”或“＜＝＞”操作符的带索引的列。 fulltext：使用 FULLTEXT 索引执行联接 ref_or_null：这种连接类型类似于 ref，但是除了 MySQL 还会额外搜索包含 NULL 值的行。此联接类型优化最常用于解析子查询 index_merge：此联接类型指示使用索引合并优化。在这种情况下，key 输出行中的列包含使用的索引列表，并且 key_len 包含使用的索引的最长键部分的列表 unique_subquery：类型替换 以下形式的eq_ref某些 IN子查询, unique_subquery 只是一个索引查找函数，它完全替代了子查询以提高效率。 index_subquery：连接类型类似于 unique_subquery。它代替 IN 子查询,但只适合子查询中的非唯一索引 range：只检索给定范围的行，使用一个索引来选择行。key列显示使用了哪个索引。key_len 包含所使用索引的最长关键元素。当使用 ＝、＜＞、＞、＞＝、＜、＜＝、IS NULL、＜＝＞、BETWEEN 或者 IN 操作符用常量比较关键字列时，类型为range index：该 index 联接类型是一样的 ALL，只是索引树被扫描。这发生两种方式： 如果索引是查询的覆盖索引，并且可用于满足表中所需的所有数据，则仅扫描索引树。在这种情况下，Extra列显示为 - Using index; 使用对索引的读取执行全表扫描，以按索引顺序查找数据行。 Uses index 没有出现在 Extra列中; ALL：对于前面的表的任意行组合进行完整的表扫描 possible_keys：指出MySQL能使用哪个索引在该表中找到行。若该列是 NULL，则没有相关的索引。在这种情况下，可以通过检查 WHERE 子句看它是否引用某些列或适合索引的列来提高查询性能。如果是这样，可以创建适合的索引来提高查询的性能。 kye：表示查询实际使用的索引，如果没有选择索引，该列的值是 NULL。要想强制 MySQL 使用或忽视 possible_keys 列中的索引，在查询中使用 FORCE INDEX、USE INDEX 或者 IGNORE INDEX key_len：表示 MySQL 选择的索引字段按字节计算的长度，若键是 NULL，则长度为 NULL。注意，通过 key_len 值可以确定 MySQL 将实际使用一个多列索引中的几个字段 ref：表示使用哪个列或常数与索引一起来查询记录。 rows：显示 MySQL 在表中进行查询时必须检查的行数。 Extra：表示 MySQL 在处理查询时的详细信息 更详细说明见官网：https://dev.mysql.com/doc/refman/8.0/en/explain-output.html\n操作索引 创建索引的语法（如下都是默认的innodb存储引擎）：https://dev.mysql.com/doc/refman/8.0/en/create-index.html\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 CREATE [UNIQUE | FULLTEXT | SPATIAL] INDEX index_name [index_type] ON tbl_name (key_part,...) [index_option] [algorithm_option | lock_option] ... key_part: {col_name [(length)] | (expr)} [ASC | DESC] index_option: { KEY_BLOCK_SIZE [=] value | index_type | WITH PARSER parser_name | COMMENT \u0026#39;string\u0026#39; | {VISIBLE | INVISIBLE} | ENGINE_ATTRIBUTE [=] \u0026#39;string\u0026#39; | SECONDARY_ENGINE_ATTRIBUTE [=] \u0026#39;string\u0026#39; } index_type: USING {BTREE | HASH} algorithm_option: ALGORITHM [=] {DEFAULT | INPLACE | COPY} lock_option: LOCK [=] {DEFAULT | NONE | SHARED | EXCLUSIVE} 1 ALTER TABLE table_name ADD [UNIQUE|FULLTEXT|SPATIAL] [INDEX|KEY] [index_name] (col_name[length],...) [ASC|DESC] 1 ALTER TABLE table_name DROP INDEX index_name 1 DROP INDEX index_name ON table_name; 聚集索引和二级索引 聚集索引 InnoDB 存储引擎表是索引组织表，即表中数据按照主键顺序存放。而聚集索引（clustered index）就是按照每张表的主键构造一棵 B+ 树，同时叶子节点中存放的即为整张表的行记录数据，也将聚集索引的叶子节点称为数据页。聚集索引的这个特性决定了索引组织表中数据也是索引的一部分。同 B+ 树数据结构一样，每个数据页都通过一个双向链表来进行链接。\n由于实际的数据页只能按照一棵 B+ 树进行排序，因此每张表只能拥有一个聚集索引。由于定义了数据的逻辑顺序，聚集索引能够特别快地访问针对范围值的查询。查询优化器能够快速发现某一段范围的数据页需要扫描。\n聚集索引的存储并不是物理上连续的，而是逻辑上连续的。这其中有两点：一是前面说过的页通过双向链表链接，页按照主键的顺序排序；另一点是每个页中的记录也是通过双向链表进行维护的，物理存储上可以同样不按照主键存储。\n二级索引 对于辅助索引（Secondary Index），叶子节点并不包含行记录的全部数据。叶子节点除了包含键值以外，每个叶子节点中的索引行中还包含了一个书签（bookmark）。该书签用来告诉 InnoDB 存储引擎哪里可以找到与索引相对应的行数据。由于 InnoDB 存储引擎表是索引组织表，因此 InnoDB 存储引擎的辅助索引的书签就是相应行数据的聚集索引键。\n当通过辅助索引来寻找数据时，InnoDB 存储引擎会遍历辅助索引并通过叶级别的指针获得指向主键索引的主键，然后再通过主键索引来找到一个完整的行记录。\n覆盖索引 InnoDB 存储引擎支持覆盖索引（covering index，或称索引覆盖），即从辅助索引中就可以得到查询的记录，而不需要查询聚集索引中的记录。使用覆盖索引的一个好处是辅助索引不包含整行记录的所有信息，故其大小要远小于聚集索引，因此可以减少大量的 IO 操作。\nMulti-Range Read 优化 Multi-Range Read 优化的目的就是为了减少磁盘的随机访问，并且将随机访问转化为较为顺序的数据访问，这对于 IO-bound 类型的 SQL 查询语句可带来性能极大的提升。Multi-RangeRead 优化可适用于 range、ref、eq_ref 类型的查询。\nMulti-Range Read 的好处：\nMRR 使数据访问变得较为顺序。在查询辅助索引时，首先根据得到的查询结果，按照主键进行排序，并按照主键排序的顺序进行书签查找。 减少缓冲池中页被替换的次数。 批量处理对键值的查询操作 MRR 的工作方式如下：\n将查询得到的辅助索引键值存放于一个缓存中，这时缓存中的数据是根据辅助索引键值排序的。 将缓存中的键值根据 RowID 进行排序。 根据 RowID 的排序顺序来访问实际的数据文件。 MySQL5.6版本开始支持 Multi-Range Read（MRR）优化，通过参数 optimizer_switch 的标记来控制是否使用 MRR，当设置mrr=on 时，表示启用MRR优化。mrr_cost_based 表示是否通过 cost base 的方式来启用 MRR。如果选择 mrr=on,mrr_cost_based=off,则表示总是开启 MRR 优化。\nIndex Condition Pushdown（ICP）优化 MySQL 数据库会在取出索引的同时，判断是否可以进行 WHERE 条件的过滤，也就是将 WHERE 的部分过滤操作放在了存储引擎层。在某些查询下，可以大大减少上层 SQL 层对记录的索取（fetch），从而提高数据库的整体性能，优化支持 range、ref、eq_ref、ref_or_null 类型的查询，选择 Index Condition Pushdown 优化时，可在执行计划的列 Extra 看到 Using index condition 提示。\n当 SQL 需要全表访问时，ICP 的优化策略可用于 range, ref, eq_ref, ref_or_null 类型的访问数据方法。 支持 InnoDB 和 MyISAM 表。 ICP 只能用于二级索引，不能用于主索引。 并非全部 WHERE 条件都可以用ICP筛选，如果 WHERE 条件的字段不在索引列中，还是要读取整表的记录到 server 端做条件过滤。 索引实现的原理 InnoDB 存储的索引是基于 B+ 树实现的，不支持hash的实现方式。首先来了解下 B+ 树的特点；\nB+树的特征 有k个子树的中间节点包含有k个元素（B树中是k-1个元素），每个元素不保存数据，只用来索引，所有数据都保存在叶子节点。 所有的叶子结点中包含了全部元素的信息，及指向含这些元素记录的指针，且叶子结点本身依关键字的大小自小而大顺序链接。 所有的中间节点元素都同时存在于子节点，在子节点元素中是最大（或最小）元素。\nB+树的优势 单一节点存储更多的元素，使得查询的IO次数更少。 所有查询都要查找到叶子节点，查询性能稳定。 所有叶子节点形成有序链表，便于范围查询。 在B+树中，所有记录节点都是按键值的大小顺序存放在同一层的叶子节点上，由各叶子节点指针进行连接。先来看一个B+树，其高度为 2，每页可存放4条记录，扇出（fan out）为5，如下图所示：\n索引的设计思考 索引是一种存储方式，最相关的硬件就是磁盘，索引磁盘的性能会直接影响到数据库的查询效率 磁盘的性能和读写的顺序有关，普通磁盘顺序读写比随机读写快很多，所以尽量避免随机读写。 数据都是以行为单位一行一行的存储的，每一行都包括了所有的列，多行可以连续存储。 每一行数据中，一般都有一个键，其他的列可以称为值，可以理解为键值对。innodb必须有唯一非空的主键，就是默认的键。 在键值对中，键值可以排序，还可以组合键值。 索引的设计 磁盘空间会划分为许多个大小相等的块或者页，一个页中可以存储多行数据，这样就可以符合磁盘的顺序读写，这样一次IO就可以读取很多数据到内存，可以减少磁盘IO。 在一个页内，所有的数据可能会经常变动，并且大小也是相对固定的，所以内部通过链表或者数组管理。 每个键值可以排序，所以在一个块内的所有数据也可以是有序的，这样通过二分法查找可以很快的在一个页内找到指定键对应的数据 一个页设计好之后，可以把页作为B+树的节点，通过页来承载数据，通过B+数来组织不同页之间的关系 B+树的特点是在内节点存储键来提高搜索的性能，所以很自然的，内节点用来存储数据行的键，叶子节点存储所有数据行，可以很好的提升性能 接下来结合2.5节的聚集索引和二级索引来说：\n表中数据按照主键顺序存放。而聚集索引（clustered index）就是按照每张表的主键构造一棵B+树，同时叶子节点中存放的即为整张表的行记录数据，也将聚集索引的叶子节点称为数据页。聚集索引的这个特性决定了索引组织表中数据也是索引的一部分。同B+树数据结构一样，每个数据页都通过一个双向链表来进行链接。如下图所示：\n上图所示的是一个深度为2的B+树，也是我们所称的索引，这里假设页有随机唯一的编号，根页号为20。这里只有一个内节点（根节点），其他的都是叶子节点，也是数据节点，对于内节点来说，存有key和pageno的指针信息，对于叶子节点来说，只存有完整的数据。对于聚集索引，data部分存有除主键外的其他列的组合，如果是二级索引，则这里存放就是这行记录对应主键的组合，用于回表。\n最左边的MIN为了很好的组织树形结构的指针，和其他的内节点一样，主要用来标记它是最小记录Min，还有就是一个pageno指针指向下层最左边的Min记录，其他节点的Min记录用于判断搜索是否到了边界。每个页都有页头页尾用来管理和标记页面的状态，页面中的数据是如何存储，有没有空闲的空间，以什么样的顺序存储等。\n上图中所有的叶子节点从左到右都是从小到大的顺序以双向链表的方式存储的，所以当我们需要遍历全部的数据，只需要通过B+树找到最小的位置，然后通过遍历链表则可以查询到所有的数据，还有就是10,16,25这三条记录在内节点和叶子节点均存在，这既是B+数的特点，叶子节点会存有所有的key和值。而内节点只存储了key，不存储其他的数据，只有用来索引。叶子节点除了第一条记录会有上一层重复的存储，其他数据不会有这样的现象，所以浪费的空间也不大，由于每一个页的大小是固定的（16k），在内节点上只存储key，不存储其他数据，一个页就可以存储更多的key，这样检索也能减少磁盘的IO，由于页存储Key增多，这样就可以使得B+树的深度减少，这样也可以减少磁盘的IO，提高查询性能。\n例如一个三层的B+数，每一个页能存1000个key，所以第二层就有1000*（1+1000）个key，第三层就可以有100010011001=1002001000（十亿级别），一个简单的三层B+数据就可以存十亿级别的数据，很强大。\n上面说到的“回表”其实就是在使用二级索引进行搜索时，因为二级索引只保存了部分列的数据，如果需要获取键值不包括的列的数据时，需要通过二级索引的指针（书签：用于指向聚集索引的指针）来找到聚集索引的全部数据，然后返回需要查询的列的值。如果使用二级索引不能找到需要的值（需要回表），称为非覆盖索引，否则为2.6节介绍的覆盖索引。非覆盖索引需要回表，增加IO，所以性能会差一些。所以可以根据业务需求创建组合索引来避免回表。但是也要权衡索引带来的利是否大于弊。所以在统计行总数的时候可以通过二级索引来统计，这样速度会快一些。大概图形如下：\n这里附带的说一些不能走索引的情况，但是不多说，因为优化这个东西太多，后期准备写一两篇优化的文章，所以这里只是提一下，走索引的强大；虽然可能创建了很多索引，很多情况都不走索引，比如：like \u0026lsquo;%query_name%\u0026rsquo; ，where端使用or条件连接，where端使用函数等，在group by和order by使用的时候要注意组合索引的最左前缀原则。\n引用 copy from https://www.cnblogs.com/zsql/p/13808417.html#_label2\n","date":"2022-11-06T00:00:00Z","permalink":"https://ggxxll.github.io/p/mysql-%E7%B4%A2%E5%BC%95/","title":"Mysql 索引"},{"content":"根据加锁的范围，MySQL 里面的锁大致可以分成全局锁、表级锁和行锁三类\n全局锁 全局锁就是对整个数据库实例加锁。MySQL提供了一个加全局读锁的方法，命令是 Flush tables with read lock 。当需要让整个库处于只读状态的时候，可以使用这个命令，之后其他线程的以下语句会被阻塞：数据更新语句（数据的增删改）、数据定义语句（包括建表、修改表结构等）和更新类事务的提交语句\n全局锁的典型使用场景是，做全库逻辑备份。\n但是整个库都只读，可能出现以下问题：\n如果在主库上备份，那么在备份期间都不能执行更新，业务基本上就得停摆 如果在从库上备份，那么在备份期间从库不能执行主库同步过来的binlog，会导致主从延迟 在可重复读隔离级别下开启一个事务能够拿到一致性视图\n官方自带的逻辑备份工具是 mysqldump。当 mysqldump 使用参数 –single-transaction 的时候，导数据之前就会启动一个事务，来确保拿到一致性视图。而由于 MVCC 的支持，这个过程中数据是可以正常更新的。single-transaction 只适用于所有的表使用事务引擎的库\n1.既然要全库只读，为什么不使用set global readonly=true的方式？\n在有些系统中，readonly 的值会被用来做其他逻辑，比如用来判断一个库是主库还是备库。因此修改 global 变量的方式影响面更大 在异常处理机制上有差异。如果执行Flush tables with read lock命令之后由于客户端发生异常断开，那么MySQL会自动释放这个全局锁，整个库回到可以正常更新的状态。而将整个库设置为 readonly 之后，如果客户端发生异常，则数据库会一直保持 readonly 状态，这样会导致整个库长时间处于不可写状态，风险较高 表级锁 MySQL里面表级别的锁有两种：一种是表锁，一种是元数据锁（meta data lock，MDL）\n表锁 表锁的语法是 lock tables … read/write。可以用 unlock tables 主动释放锁，也可以在客户端断开的时候自动释放。lock tables 语法除了会限制别的线程的读写外，也限定了本线程接下来的操作对象\n如果在某个线程 A 中执行 lock tables t1 read,t2 wirte;这个语句，则其他线程写 t1、读写 t2 的语句都会被阻塞。同时，线程 A 在执行 unlock tables 之前，也只能执行读 t1、读写 t2 的操作，连写t1都不允许\n元数据锁 MDL 不需要显式使用，在访问一个表的时候会被自动加上。MDL 的作用是，保证读写的正确性。如果一个查询正在遍历一个表中的数据，而执行期间另一个线程对这个表结构做了变更，删了一列，那么查询线程拿到的结果跟表结构对不上，肯定不行\n在MySQL5.5版本引入了MDL，当对一个表做增删改查操作的时候，加MDL读锁；当要对表做结构变更操作的时候，加MDL写锁\n读锁之间不互斥，因此可以多个线程同时对一张表增删改查 读写锁之间、写锁之间是互斥的，用来保证变更表结构操作的安全性。 事务中的 MDL 锁，在语句执行开始时申请，但是语句结束后并不会马上释放，而会等到整个事务提交后再释放\n如果安全地给小表加字段？\n首先要解决长事务，事务不提交，就会一直占着 DML 锁。在 MySQL 的 information_schema 库的 innodb_trx 表中，可以查到当前执行的事务。如果要做DDL变更的表刚好有长事务在执行，要考虑先暂停 DDL，或者 kill 掉这个长事务\n如果要变更的表是一个热点表，虽然数据量不大，但是上面的请求很频繁，而又不得不加个字段，该怎么做？\n在 alter table 语句里面设定等待时间，如果在这个指定的等待时间里面能够拿到 MDL 写锁最好，拿不到也不要阻塞后面的业务语句，先放弃，之后再通过重试命令重复这个过程\n行锁 MySQL的行锁是在引擎层由各个引擎自己实现的。但不是所有的引擎都支持行锁，比如 MyISAM 引擎就不支持行锁\n行锁就是针对数据表中行记录的锁。比如事务 A 更新了一行，而这时候事务 B 也要更新同一行，则必须等事务 A 的操作完成后才能进行更。事务 A 持有的两个记录的行锁都是在 commit 的时候才释放的，事务 B 的 update 语句会被阻塞，直到事务 A 执行 commit 之后，事务 B 才能继续执行\n两阶段锁协议 在 InnoDB 事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。这个就是两阶段锁协议\n如果事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放\n死锁和死锁检测 在并发系统中不同线程出现循环资源依赖，涉及的线程都在等待别的线程释放资源时，就会导致这几个线程都进入无限等待的状态，称为死锁\n当出现死锁以后，有两种策略：\n一种策略是，直接进入等待，直到超时。这个超时时间可以通过参数 innodb_lock_wait_timeout 来设置 另一种策略是，发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数innodb_deadlock_detect 设置为 on，表示开启这个逻辑 在 InnoDB 中，innodb_lock_wait_timeout的默认值是 50s，意味着如果采用第一个策略，当出现死锁以后，第一个被锁住的线程要过 50s 才会超时退出，然后其他线程才有可能继续执行。对于在线服务来说，这个等待时间往往是无法接受的\n正常情况下还是要采用主动死锁检查策略，而且 innodb_deadlock_detect 的默认值本身就是on。主动死锁监测在发生死锁的时候，是能够快速发现并进行处理的，但是它有额外负担的。每当一个事务被锁的时候，就要看看它所依赖的线程有没有被别人锁住，如此循环，最后判断是否出现了循环等待，也就是死锁\n如果所有事务都要更新同一行的场景，每个新来的被堵住的线程都要判断会不会由于自己的加入导致死锁，这是一个时间复杂度是 O(n) 的操作\n怎么解决由这种热点行更新导致的性能问题？\n如果确保这个业务一定不会出现死锁，可以临时把死锁检测关掉 控制并发度 将一行改成逻辑上的多行来减少锁冲突。以影院账户为例，可以考虑放在多条记录上，比如10个记录，影院的账户总额等于这10个记录的值的总和。这样每次要给影院账户加金额的时候，随机选其中一条记录来加。这样每次冲突概率变成员原来的1/10，可以减少锁等待个数，也就减少了死锁检测的CPU消耗 引用 https://blog.csdn.net/qq_40378034/article/details/90904573\n","date":"2022-11-06T00:00:00Z","permalink":"https://ggxxll.github.io/p/mysql-%E9%94%81%E4%BA%8C/","title":"Mysql 锁（二）"},{"content":"概述 锁机制用于管理对共享资源的并发访问。\n锁可大致分为两类：\nlock：对象是事务，用来锁定的是数据库中的对象，如表、页、行。并且一般lock的对象仅在事务 commit 或者 rollback 后进行释放。有死锁检测机制。 latch：闩锁（shuang suo），其要求锁定的时间必须非常短。若持续的时间长，则应用的性能会非常差。在 InnoDB 存储引擎中，latch 又分为 mutex 互斥锁 和 rwLock 读写锁。 其目的是为了保证并发线程操作临界资源的正确性。通常没有死锁的检测机制。 锁的类型 共享锁，排他锁 InnoDB存储引擎实现了如下两种标准的行级锁：\n共享锁（S Lock）：允许事务读取一行数据。\n排他锁（X Lock）：允许事务删除或者更新一行数据。\n这里可以理解为读写锁，可并行读，但是不可以：并行写、并行读写。\n排它锁是很强的锁，不与其他类型的锁兼容。这也很好理解，修改和删除某一行的时候，必须获得强锁，禁止这一行上的其他并发，以保障数据的一致性。\n记录锁 Record Lock，仅锁定一行记录（如共享锁、排他锁）\n记录锁总是会去锁定索引记录，如果表在建立的时候，没有设置任何索引，InnoDB 会使用隐式的主键进行锁定。 查询条件的列是唯一索引的情况下，临键锁退化为记录锁。\n间隙锁 Gap Lock，锁定一个范围，但不包括记录本身。 关闭间隙锁的两种方式：\n将事务隔离级别设置为 读已提交（read committed） 将参数 innodb_locks_unsafe_for_binlog 设置为 1 在上述配置下，除了外键和唯一性检查依然需要间隙锁，其余情况均使用行锁进行锁定。\n临键锁 Next-Key Lock，等于记录锁+间隙锁，锁定一个范围，并锁定记录本身.\n主要是阻止多个事务将记录插入到同一个范围内，从而避免幻读。\n当查询的索引是唯一索引时，InnoDB 会将临键锁优化为记录锁，从而提高并发。这时候，将不再使用间隙锁来避免幻读。\n意向锁 事务可能要加共享/排他锁，先提前声明一个意向。\nInnoDB 支持多粒度锁定，这种锁定允许事务在行级上的锁和表级上的锁同时存在。为了支持不同粒度上进行加锁操作，InnoDB 存储引擎支持一种额外的锁方式， 称之为意向锁。意向锁是将锁定的对象分为多个层次，意向锁意味着事务希望在更粗的粒度上进行加锁。\n特点：\n意向锁是表级别锁 意向锁可分为： 意向共享锁（intention shared lock, IS）：表示事务有意向对表中的某些行加共享锁 意向排它锁（intention exclusive lock, IX）：表示事务有意向对表中的某些行加排它锁 意向锁协议： 事务要获得某些行的共享锁，必须先获得表的意向共享锁 IS 事务要获取某些行的排他锁，必须先获得表的意向排他锁 IX 由于意向锁仅表明意向，所以是比较弱的锁，意向锁之间不互斥，可以并行 意向锁之间可以并行，但与共享锁和排它锁互斥，表现为 S+IS 可并行读，其他均互斥。 插入意向锁 对已有数据行的修改与删除，必须使用排他锁，那对于数据的插入，是否还需要加这么强的锁，来实施互斥呢？插入意向锁，孕育而生。\n插入意向锁，是间隙锁（Gap Locks）的一种（所以，也是实施在索引上的），它是专门针对 INSERT 操作的。\n它的用处是：多个事务，在同一个索引上插入记录时，如果插入的位置不冲突，不会阻塞彼此。\n自增锁 自增锁是 MySQL一种特殊的锁，如果表中存在自增字段，MySQL便会自动维护一个自增锁。\n在 InnoDB 存储引擎的内存结构中，对每个含有自增长值的表都有一个自增长计数器。当对含有自增长计数器的表进行插入操作时，这个计数器会被初始化，执行如下操作来得到计数器的值：\nselect max(auto_inc_col) from t for update\n插入操作会依据这个自增长的计数器值加 1 赋予自增长列。这个实现方式称为 Auto-Inc Locking。这种锁其实是采用一种表锁的机制，为了提高插入的性能，自增长锁不是在一个事务完成以后才释放，而是在完成自增长值插入的SQL后立即释放。\n虽然 Auto-Inc Locking 从一定程度上提高了并发插入的效率，但还是存在一些性能上的问题。对于自增长列的并发插入性能较差，事务必须等待前一个插入的完成（虽然不用等待事务的完成）。\n从 MySQL5.12 版本开始，InnoDB 存储引擎提供了一种轻量级互斥量的自增长实现方式。这种方式大大提高了自增长值插入的性能。并且从该版本开始，InnoDB 存储引起提供了一个参数 innodb_innodb_autoinc_lock_mode 来控制自增长模式，该参数的默认值为 1\n自增长的插入分类:\ninsert like: 指所有的插入语句，如 INSERT、REPLACE、INSERT\u0026hellip;SELECT、RPELACE\u0026hellip;SELECT、LOAD DATA等 simple inserts：指能在插入前就确定插入行数的语句，如 INSERT、REPLACE 等。需要注意的是，不包含 INSERT\u0026hellip;ON DUPLICATE KEY UPDATE 这类语句。 bulk inserts：指在插入前不能确定插入行数的语句，如 INSERT\u0026hellip;SELECT、RPELACE\u0026hellip;SELECT、LOAD DATA等 mixed-mode inserts：指插入中有一部分的值是自增长的，有一部分是确定的。如 INSERT INTO T1 (id,name) VALUES (1,\u0026lsquo;A\u0026rsquo;),( NULL,\u0026lsquo;B\u0026rsquo;)；也可以指 INSERT\u0026hellip;ON DUPLICATE KEY UPDATE 这类语句。 innodb_innodb_autoinc_lock_mode 参数值的说明：\n0： 它提供了一个向后兼容的能力 在这一模式下，所有的 insert 语句(\u0026ldquo;insert like\u0026rdquo;) 都要在语句开始的时候得到一个表级的 Auto-Inc Locking 锁，在语句结束的时候才释放这把锁，注意呀，这里说的是语句级而不是事务级的，一个事务可能包涵有一个或多个语句。 它能保证值分配的可预见性，与连续性，可重复性，这个也就保证了 insert 语句在复制到 slave 的时候还能生成和master那边一样的值(它保证了基于语句复制的安全)。 由于在这种模式下 Auto-Inc Locking 锁一直要保持到语句的结束，所以这个就影响到了并发的插入。 1: 这一模式下去 simple insert 做了优化，由于simple insert一次性插入值的个数可以立马得到 确定，所以mysql可以一次生成几个连续的值，用于这个insert语句；总的来说这个对复制也是安全的(它保证了基于语句复制的安全) 这一模式也是mysql的默认模式，这个模式的好处是 Auto-Inc Locking 锁不要一直保持到语句的结束，只要语句得到了相应的值后就可以提前释放锁 2: 由于这个模式下已经没有了 Auto-Inc Locking 锁，所以这个模式下的性能是最好的；但是它也有一个问题，就是对于同一个语句来说它所得到的 auto_incremant 值可能不是连续的。 InnoDB 存储引擎中自增长的实现和 MyISAM 不同。MyISAM 存储引擎是表锁设计，自增长不用考虑并发插入的问题。在 InnoDB 存储引擎中，自增长值的列必须是索引，同时必须是索引的第一个列，如果不是第一个列，则 MySQL 会抛出异常。MyISAM 存储引擎没有这个问题。\n","date":"2022-11-05T00:00:00Z","permalink":"https://ggxxll.github.io/p/mysql-%E9%94%81%E4%B8%80/","title":"Mysql 锁（一）"},{"content":"概述 ACID，是指数据库管理系统（DBMS）在写入或更新资料的过程中，为保证事务（transaction）是正确可靠的，所必须具备的四个特性： 原子性（atomicity [ˌætəˈmɪsəti]， 或称不可分割性）、一致性（consistency [kənˈsɪstənsi]）、 隔离性（isolation [ˌaɪsəˈleɪʃn]，又称独立性）、持久性（durability [ˌdjʊərəˈbɪlɪti]）。\n在数据库系统中，一个事务是指：由一系列数据库操作组成的一个完整的逻辑过程。例如银行转帐，从原账户扣除金额，以及向目标账户添加金额，这两个数据库操作的总和，构成一个完整的逻辑过程，不可拆分。这个过程被称为一个事务，具有ACID特性。\n特性 Atomicity（原子性）：一个事务（transaction）中的所有操作，要么全部完成，要么全部不完成，不会结束在中间某个环节。事务在执行过程中发生错误，会被恢复（Rollback）到事务开始前的状态，就像这个事务从来没有执行过一样。 Consistency（一致性）：在事务开始之前和事务结束以后，数据库的完整性没有被破坏。这表示写入的资料必须完全符合所有的预设规则，这包含资料的精确度、串联性以及后续数据库可以自发性地完成预定的工作。 Isolation（隔离性）：数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。事务隔离分为不同级别，包括读未提交（Read uncommitted）、读提交（read committed）、可重复读（repeatable read）和串行化（Serializable）。 Durability（持久性）：事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。 事务并发时可能出现的问题 脏读（Dirty Read） A 事务读到了 B 未提交事务修改过的数据\n不可重复读（Non-Repeatable Read） A 事务只能读到 B 已经提交的事务修改过的数据，并且其他事务每对该数据进行一次修改并提交后，A 事务都能查询得到最新值。 （不可重复读在读未提交和读已提交隔离级别都可能会出现）\n幻读（Phantom） A 事务先根据某些条件查询出一些记录，之后 B 事务又向表中插入了符合这些条件的记录，A 事务再次按照该条件查询时，能把 B 事务插入的记录也读出来。 （幻读在读未提交、读已提交、可重复读隔离级别都可能会出现）\nMysql事务的隔离级别 MySQL的事务隔离级别一共有四个，分别是读未提交、读已提交、可重复读以及可串行化。\nMySQL的隔离级别的作用就是让事务之间互相隔离，互不影响，这样可以保证事务的一致性。\n隔离级别比较：可串行化 \u0026gt; 可重复读 \u0026gt; 读已提交 \u0026gt; 读未提交\n隔离级别对性能的影响比较：可串行化 \u0026gt; 可重复读 \u0026gt; 读已提交 \u0026gt; 读未提交\n由此看出，隔离级别越高，所需要消耗的MySQL性能越大（如事务并发严重性），为了平衡二者，一般建议设置的隔离级别为可重复读，MySQL Innodb默认的隔离级别也是可重复读。\n读未提交（READ UNCOMMITTED） 在读未提交隔离级别下，事务 A 可以读取到事务 B 修改过但未提交的数据。\n可能发生脏读、不可重复读和幻读问题，一般很少使用此隔离级别。\n读已提交（READ COMMITTED） 在读已提交隔离级别下，事务 A 只能在事务 B 修改过并且已提交后才能读取到事务 B 修改的数据。\n读已提交隔离级别解决了脏读的问题，但可能发生不可重复读和幻读问题，一般很少使用此隔离级别。\n普通读是快照读； 加锁的select, update, delete等语句，除了在外键约束检查(foreign-key constraint checking)以及重复键检查(duplicate-key checking)时会封锁区间，其他时刻都只使用记录锁； 可重复读（REPEATABLE READ） 在可重复读隔离级别下，事务 A 只能在事务 B 修改过数据并提交后，A 也提交事务后，才能读取到事务 B 修改的数据。\n可重复读隔离级别解决了脏读和不可重复读的问题，但可能发生幻读问题。\n提问：为什么上了写锁（写操作），别的事务还可以读操作？\n因为InnoDB有MVCC机制（多版本并发控制），可以使用快照读，而不会被阻塞。\n普通的select使用快照读(snapshot read)，这是一种不加锁的一致性读(Consistent Nonlocking Read)，底层使用MVCC来实现，具体的原理在《InnoDB并发如此高，原因竟然在这？》中有详细的描述； 加锁的select(select ... in share mode | select ... for update), update, delete等语句，它们的锁，依赖于它们是否在唯一索引(unique index)上使用了唯一的查询条件(unique search condition)，或者范围查询条件(range-type search condition)： 在唯一索引上使用唯一的查询条件，会使用记录锁(record lock)，而不会封锁记录之间的间隔，即不会使用间隙锁(gap lock)与临键锁(next-key lock) 范围查询条件，会使用间隙锁与临键锁，锁住索引记录之间的范围，避免范围间插入记录，以避免产生幻影行记录，以及避免不可重复的读 可串行化（SERIALIZABLE） 各种问题（脏读、不可重复读、幻读）都不会发生，通过加锁实现（读锁和写锁）。 事务并发执行时，阻塞情况与读写锁相同，可同时读，不可以读写交叉。\n所有select语句都会被隐式的转化为 select ... in share mode.\n隔离级别比较 级别 脏读 不可重复读 幻读 读未提交 可能 可能 可能 读已提交 不会 可能 可能 可重复读 不会 不会 可能 串行读 不会 不会 不会 隔离级别的实现原理 使用MySQL的默认隔离级别（可重复读）来进行说明。\n每条记录在更新的时候都会同时记录一条回滚操作（回滚操作日志undo log）。同一条记录在系统中可以存在多个版本，这就是数据库的多版本并发控制（MVCC）。 即通过回滚（rollback操作），可以回到前一个状态的值。\n提问：回滚操作日志（undo log）什么时候删除？\nMySQL会判断当没有事务需要用到这些回滚日志的时候，回滚日志会被删除。\n提问：什么时候不需要了？\n当系统里么有比这个回滚日志更早的read-view的时候。\n相关操作 查看隔离级别 SHOW VARIABLES LIKE 'transaction_isolation'; SELECT @@transaction_isolation; 设置隔离级别 SET 命令 SET [GLOBAL|SESSION] TRANSACTION ISOLATION LEVEL level;\n1 2 3 4 5 6 7 其中level有4种值： level: { REPEATABLE READ | READ COMMITTED | READ UNCOMMITTED | SERIALIZABLE } 1.　GLOBAL\n只对执行完该语句之后产生的会话起作用 当前已经存在的会话无效 2.　SESSION\n只对当前会话中下一个即将开启的事务有效 下一个事务执行完后，后续事务将恢复到之前的隔离级别 该语句不能在已经开启的事务中间执行，会报错的 服务启动项命令 可以修改启动参数transaction-isolation的值\n比方说我们在启动服务器时指定了\u0026ndash;transaction-isolation=READ UNCOMMITTED，那么事务的默认隔离级别就从原来的REPEATABLE READ变成了READ UNCOMMITTED。\n","date":"2022-10-23T00:00:00Z","permalink":"https://ggxxll.github.io/p/mysql-acid/","title":"Mysql ACID"},{"content":"问题 order by 和 limit 造成优化器选择索引错误\n测试表 1 2 CREATE DATABASE `app`; USE app; 1 2 3 4 5 6 7 8 9 CREATE TABLE `users` ( `id` int NOT NULL AUTO_INCREMENT, `name` varchar(100) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci DEFAULT NULL, `age` int DEFAULT NULL, `created_at` timestamp NOT NULL, `deleted_at` timestamp NULL DEFAULT NULL, PRIMARY KEY (`id`), KEY `users_age_IDX` (`age`) USING BTREE, ) ENGINE=InnoDB AUTO_INCREMENT=0 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci; 填充测试数据 1 2 3 4 5 6 7 8 9 10 11 12 13 DROP PROCEDURE IF EXISTS addData; DELIMITER $$ $$ CREATE PROCEDURE addData(IN n int) BEGIN DECLARE i int default 1; while (i \u0026lt; n) DO INSERT INTO users(name, age, created_at, deleted_at) VALUES(CONCAT(\u0026#39;name\u0026#39;,i), i, now(), null); set i=i | 1; End while; END$$ DELIMITER ; 1 CALL addData(400) 查询 sql 以下查询中使用的索引正确使用了 users_age_IDX。 1 2 explain SELECT * from users u where age \u0026gt; 1000 limit 1; explain SELECT * from users u where age \u0026gt; 1000 order by id desc limit 2; id select_type table partitions type possible_keys key key_len ref rows filtered Extra 1 SIMPLE u range users_age_IDX,users_deleted_at_IDX users_age_IDX 5 1988 100.0 Using index condition; Using where 这个查询中使用的索引是主键, 数据量 1 explain SELECT * from users u where age \u0026gt; 100 order by id desc limit 1; id select_type table partitions type possible_keys key key_len ref rows filtered Extra 1 SIMPLE u index users_age_IDX,users_deleted_at_IDX PRIMARY 4 65 15.33 Using where; Backward index scan 原因 MySQL 优化器认为在 limit 值较小的情况下，走主键索引能够更快的找到那一条数据，并且如果走联合索引需要扫描索引后进行排序，而主键索引天生有序，所以优化器综合考虑，走了主键索引。\n","date":"2022-09-19T00:00:00Z","permalink":"https://ggxxll.github.io/p/mysql-order-by-%E5%92%8C-limit-%E8%AF%AD%E5%8F%A5%E5%AF%BC%E8%87%B4%E6%85%A2-sql/","title":"Mysql order by 和 limit 语句导致慢 sql"},{"content":"基础知识 map的概念 map的直观意思是映射，是\u0026lt;key, value\u0026gt; 对组成的抽象结构，且 key 不会重复。map 的底层实现一般有两种：\n搜索树（search tree），天然有序，平均/最差的写入/查找复杂度是O(logN) 哈希表（hash table），无序存储，平均的写入/查找复杂度是O(1)，最差是O(N) 底层 源码 在 golang 中，map 的底层实现是哈希表：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 const ( bucketCntBits = 3 bucketCnt = 1 \u0026lt;\u0026lt; bucketCntBits ) type hmap struct { count int // map中的元素个数， golang 中调用 len(map) 的时候直接返回该字段 flags uint8 // 状态标记位，通过与定义的枚举值进行\u0026amp;操作可以判断当前是否处于这种状态 B uint8 // 2^B 表示 buckets 的数量 noverflow uint16 // overflow bucket 的数量的近似数，当其 B\u0026gt;=16 时为近似值 hash0 uint32 // 是哈希的种子。在创建map时 fastrand 函数生成，并在调用哈希函数时作为参数传入 buckets unsafe.Pointer // 指向buckets数组的指针，数组大小为2^B，如果元素个数为0，它为nil。 oldbuckets unsafe.Pointer // 如果发生扩容，oldbuckets 是指向老的buckets数组的指针，老的 buckets 数组大小是新的 buckets 的1/2。非扩容状态下，它为nil nevacuate uintptr // 当桶进行调整时指示的搬迁进度，地址小于当前指针的 bucket 已经迁移完成 extra *mapextra // 这个字段是为了优化GC扫描而设计的 } // A bucket for a Go map. type bmap struct { tophash [bucketCnt]uint8 } 如上图所示哈希表 runtime.hmap 的桶是 runtime.bmap。每一个 runtime.bmap 都能存储 8 个键值对，当哈希表中存储的数据过多，单个桶已经装满时就会使用 extra.nextOverflow 中桶存储溢出的数据。\n上述两种不同的桶在内存中是连续存储的，我们在这里将它们分别称为正常桶和溢出桶，上图中黄色的 runtime.bmap 就是正常桶，绿色的 runtime.bmap 是溢出桶，溢出桶是在 Go 语言还使用 C 语言实现时使用的设计，由于它能够减少扩容的频率所以一直使用至今。\n桶的结构体 runtime.bmap 在 Go 语言源代码中的定义只包含一个简单的 tophash 字段，tophash 存储了键的哈希的高 8 位，通过比较不同键的哈希的高 8 位可以减少访问键值对次数以提高性能：\n在运行期间，runtime.bmap 结构体其实不止包含 tophash 字段，因为哈希表中可能存储不同类型的键值对，而且 Go 语言也不支持泛型，所以键值对占据的内存空间大小只能在编译时进行推导。runtime.bmap 中的其他字段在运行时也都是通过计算内存地址的方式访问的，所以它的定义中就不包含这些字段，不过我们能根据编译期间的 cmd/compile/internal/gc.bmap 函数重建它的结构：\n1 2 3 4 5 6 7 type bmap struct { topbits [8]uint8 // 键的哈希值的高 8 位 keys [8]keytype values [8]valuetype pad uintptr // 内存对齐使用（新版已移除） overflow uintptr // 存放了所指向的溢出桶的地址。当 bucket 的8个key 存满了之后，该字段指向溢出桶 } hmap map 的底层结构是 hmap，hmap 包含若干个结构为 bmap 的 bucket 数组，每个 bucket 底层都采用链表结构。\nbmap bmap 就是我们常说的“桶”的底层数据结构， 一个桶中可以存放最多 8 个 key/value.\nbmap 中 key 和 value 是各自放在一起的 bmap.keys 和 bmap.values，并不是 key/value/key/value/... 这样的形式。源码里说明这样的好处是在某些情况下可以省略掉 padding 字段，节省内存空间。\n使用 key/value/key/value/... 结构，如果存储的键和值的类型不同，在内存中布局中所占字节不同的话，就需要对齐。比如说存储一个 map[int64]int8 类型的字典。\nmapextra 1 2 3 4 5 6 7 8 9 10 11 12 13 14 // mapextra holds fields that are not present on all maps. type mapextra struct { // 如果 key 和 value 都不包含指针，并且可以被 inline(\u0026lt;=128 字节) // 就使用 hmap 的 extra 字段 来存储 overflow buckets，这样可以避免 GC 扫描整个 map // 然而 bmap.overflow 也是个指针，这时候我们只能把这些 overflow 的指针 // 都放在 hmap.extra.overflow 和 hmap.extra.oldoverflow 中了 // overflow 包含的是 hmap.buckets 的 overflow 的 buckets // oldoverflow 包含扩容时的 hmap.oldbuckets 的 overflow 的 bucket overflow *[]*bmap oldoverflow *[]*bmap // 指向空闲的 overflow bucket 的指针 nextOverflow *bmap } 操作 查找 Key 使用创建 map 时生成的 hash 种子 hash0，调用 hash 函数得到 key 的 哈希值。 然后使用哈希值的后 B 位确定 桶序号，再使用前 8 位确定 key 在桶中的位置。\n注意：对于高低位的选择，该操作的实质是取余，但是取余开销很大，在实际代码实现中采用的是位操作。以下是tophash的实现代码：\n1 2 3 4 5 6 7 func tophash(hash uintptr) uint8 { top := uint8(hash \u0026gt;\u0026gt; (sys.PtrSize*8 - 8)) if top \u0026lt; minTopHash { top += minTopHash } return top } 源码实现：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 func mapaccess1(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointer { // 如果开启了竞态检测 -race if raceenabled \u0026amp;\u0026amp; h != nil { callerpc := getcallerpc() pc := funcPC(mapaccess1) racereadpc(unsafe.Pointer(h), callerpc, pc) raceReadObjectPC(t.key, key, callerpc, pc) } // 如果开启了memory sanitizer -msan if msanenabled \u0026amp;\u0026amp; h != nil { msanread(key, t.key.size) } // 如果map为空或者元素个数为0，返回零值 if h == nil || h.count == 0 { if t.hashMightPanic() { t.hasher(key, 0) // see issue 23734 } return unsafe.Pointer(\u0026amp;zeroVal[0]) } // 注意，这里是按位与操作 // 当h.flags对应的值为hashWriting（代表有其他goroutine正在往map中写key）时，那么位计算的结果不为0，因此抛出以下错误。 // 这也表明，go的map是非并发安全的 if h.flags\u0026amp;hashWriting != 0 { throw(\u0026#34;concurrent map read and map write\u0026#34;) } // 不同类型的key，会使用不同的hash算法，可详见src/runtime/alg.go中typehash函数中的逻辑 hash := t.hasher(key, uintptr(h.hash0)) m := bucketMask(h.B) // 按位与操作，找到对应的bucket b := (*bmap)(add(h.buckets, (hash\u0026amp;m)*uintptr(t.bucketsize))) // 如果oldbuckets不为空，那么证明map发生了扩容 // 如果有扩容发生，老的buckets中的数据可能还未搬迁至新的buckets里 // 所以需要先在老的buckets中找 if c := h.oldbuckets; c != nil { if !h.sameSizeGrow() { m \u0026gt;\u0026gt;= 1 } oldb := (*bmap)(add(c, (hash\u0026amp;m)*uintptr(t.bucketsize))) // 如果在oldbuckets中tophash[0]的值，为evacuatedX、evacuatedY，evacuatedEmpty其中之一 // 则evacuated()返回为true，代表搬迁完成。 // 因此，只有当搬迁未完成时，才会从此oldbucket中遍历 if !evacuated(oldb) { b = oldb } } // 取出当前key值的tophash值 top := tophash(hash) // 以下是查找的核心逻辑 // 双重循环遍历：外层循环是从桶到溢出桶遍历；内层是桶中的cell遍历 // 跳出循环的条件有三种：第一种是已经找到key值；第二种是当前桶再无溢出桶； // 第三种是当前桶中有cell位的tophash值是emptyRest，这个值在前面解释过，它代表此时的桶后面的cell还未利用，所以无需再继续遍历。 bucketloop: for ; b != nil; b = b.overflow(t) { for i := uintptr(0); i \u0026lt; bucketCnt; i++ { // 判断tophash值是否相等 if b.tophash[i] != top { if b.tophash[i] == emptyRest { break bucketloop } continue } // 因为在bucket中key是用连续的存储空间存储的，因此可以通过bucket地址+数据偏移量（bmap结构体的大小）+ keysize的大小，得到k的地址 // 同理，value的地址也是相似的计算方法，只是再要加上8个keysize的内存地址 k := add(unsafe.Pointer(b), dataOffset+i*uintptr(t.keysize)) if t.indirectkey() { k = *((*unsafe.Pointer)(k)) } // 判断key是否相等 if t.key.equal(key, k) { e := add(unsafe.Pointer(b), dataOffset+bucketCnt*uintptr(t.keysize)+i*uintptr(t.elemsize)) if t.indirectelem() { e = *((*unsafe.Pointer)(e)) } return e } } } // 所有的bucket都未找到，则返回零值 return unsafe.Pointer(\u0026amp;zeroVal[0]) } 整个查找过程优先在 oldbuckets 里面找(如果存在 oldbuckets 的话)，找完再去新 bmap 里面找。\n有人可能会有疑问，为何这里要加入 tophash 多一次比较呢？\ntophash 的引入是为了加速查找的。由于它只存了 hash 值的高 8 位，比查找完整的 64 位要快很多。通过比较高 8 位，迅速找到高 8 位一致 hash 值的索引，接下来再进行一次完整的比较，如果还一致，那么就判定找到该 key 了。\n如果找到了 key 就返回对应的 value。如果没有找到，还会继续去 overflow 桶继续寻找，直到找到最后一个桶，如果还没有找到就返回对应类型的零值。\n插入 Key 插入 key 的过程和查找 key 的过程大体一致。\n有几点不同，需要注意：\n如果找到要插入的 key ，只需要直接更新对应的 value 值就好了。 如果没有在 bmap 中没有找到待插入的 key ，这么这时分几种情况。 bmap 中还有空位，在遍历 bmap 的时候预先标记空位，一旦查找结束也没有找到 key，就把 key 放到预先遍历时候标记的空位上。 bmap中已经没有空位了。这个时候 bmap 装的很满了。此时需要检查一次最大负载因子是否已经达到了。如果达到了，立即进行扩容操作。扩容以后在新桶里面插入 key，流程和上述的一致。如果没有达到最大负载因子，那么就在新生成一个 bmap，并把前一个 bmap 的 overflow 指针指向新的 bmap。 在扩容过程中，oldbucket 是被冻结的，查找 key 时会在oldbucket 中查找，但不会在 oldbucket 中插入数据。如果在 oldbucket 是找到了相应的 key，做法是将它迁移到新 bmap 后加入 evalucated 标记。 删除 Key 删除操作主要流程和查找 key 流程也差不多，找到对应的 key 以后，如果是指针指向原来的 key，就把指针置为 nil，如果是值就清空它所在的内存。还要清理 tophash 里面的值，最后把 hmap 的 count 减 1。\n如果在扩容过程中，删除操作会在扩容以后在新的 bmap 里面执行删除。\n查找的过程依旧会一直遍历到链表的最后一个 bmap 桶。\n特性 引用类型 map是个指针，底层指向hmap，所以是个引用类型。\ngolang 有三个常用的高级类型 slice、map、channel，它们都是引用类型，当引用类型作为函数参数时，可能会修改原内容数据。\ngolang 中没有引用传递，只有值和指针传递。所以 map 作为函数实参传递时本质上也是值传递，只不过因为 map 底层数据结构是通过指针指向实际的元素存储空间， 在被调函数中修改 map，对调用者同样可见，所以 map 作为函数实参传递时表现出了引用传递的效果。\n因此，传递 map 时，如果想修改map的内容而不是map本身，函数形参无需使用指针。\n1 2 3 4 5 6 7 8 9 10 11 12 func TestSliceFn(t *testing.T) { m := map[string]int{} t.Log(m, len(m)) // map[a:1] mapAppend(m, \u0026#34;b\u0026#34;, 2) t.Log(m, len(m)) // map[a:1 b:2] 2 } func mapAppend(m map[string]int, key string, val int) { m[key] = val } 非线程安全 map默认是并发不安全的，原因如下：\nGo 官方在经过了长时间的讨论后，认为 Go map 更应适配典型使用场景（不需要从多个 goroutine 中进行安全访问），而不是为了小部分情况（并发访问），导致大部分程序付出加锁代价（性能），决定了不支持。\n更改 map 时会检查 hmap 的标志位 flags。如果 flags 的写标志位此时被置 1 了，说明有其他协程在执行“写”操作，进而导致程序 panic。这也说明了 map 对协程是不安全的。\n共享存储空间 map 底层数据结构是通过指针指向实际的元素存储空间 ，这种情况下，对其中一个map的更改，会影响到其他map\n1 2 3 4 5 6 7 8 9 func TestMapShareMemory(t *testing.T) { m1 := map[string]int{} m2 := m1 m1[\u0026#34;a\u0026#34;] = 1 t.Log(m1, len(m1)) // map[a:1] 1 t.Log(m2, len(m2)) // map[a:1] } 哈希冲突 golang中map是一个 kv 对集合。底层使用 hash table，用链表来解决冲突 ，出现冲突时，不是每一个 key 都申请一个结构通过链表串起来， 而是以 bmap 为最小粒度挂载，一个 bmap 可以放 8 个 kv。\n在哈希函数的选择上，会在程序启动时，检测 cpu 是否支持 aes，如果支持，则使用 aes hash，否则使用 memhash。\n遍历无序 map 在没有被修改的情况下，使用 range 多次遍历 map 时输出的 key 和 value 的顺序可能不同。这是 Go 语言的设计者们有意为之， 在每次 range 时的顺序被随机化，旨在提示开发者们，Go 底层实现并不保证 map 遍历顺序稳定，请大家不要依赖 range 遍历结果顺序。\n扩容 扩容条件 再来说触发 map 扩容的时机：在向 map 插入新 key 的时候，会进行条件检测，符合下面这 2 个条件，就会触发扩容：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 // If we hit the max load factor or we have too many overflow buckets, // and we\u0026#39;re not already in the middle of growing, start growing. if !h.growing() \u0026amp;\u0026amp; (overLoadFactor(h.count+1, h.B) || tooManyOverflowBuckets(h.noverflow, h.B)) { hashGrow(t, h) goto again // Growing the table invalidates everything, so try again } func tooManyOverflowBuckets(noverflow uint16, B uint8) bool { // If the threshold is too low, we do extraneous work. // If the threshold is too high, maps that grow and shrink can hold on to lots of unused memory. // \u0026#34;too many\u0026#34; means (approximately) as many overflow buckets as regular buckets. // See incrnoverflow for more details. if B \u0026gt; 15 { B = 15 } // The compiler doesn\u0026#39;t see here that B \u0026lt; 16; mask B to generate shorter shift code. return noverflow \u0026gt;= uint16(1)\u0026lt;\u0026lt;(B\u0026amp;15) } 装载因子超过阈值\n源码里定义的阈值是 6.5 (loadFactorNum/loadFactorDen)，是经过测试后取出的一个比较合理的因子。 我们知道，每个 bucket 有 8 个空位，在没有溢出，且所有的桶都装满了的情况下，装载因子算出来的结果是 8。因此当装载因子超过 6.5 时， 表明很多 bucket 都快要装满了，查找效率和插入效率都变低了。在这个时候进行扩容是有必要的。\n对于条件 1，元素太多，而 bucket 数量太少，很简单：将 B 加 1，bucket 最大数量(2^B)直接变成原来 bucket 数量的 2 倍。于是，就有新老 bucket 了。\n注意，这时候元素都在老 bucket 里，还没迁移到新的 bucket 来。新 bucket 只是最大数量变为原来最大数量的 2 倍(2^B * 2) 。\noverflow 的 bucket 数量过多\n在装载因子比较小的情况下，这时候 map 的查找和插入效率也很低，而第 1 点识别不出来这种情况。表面现象就是计算装载因子的分子比较小， 即 map 里元素总数少，但是 bucket 数量多（真实分配的 bucket 数量多，包括大量的 overflow bucket）。\n不难想像造成这种情况的原因：不停地插入、删除元素。先插入很多元素，导致创建了很多 bucket，但是装载因子达不到第 1 点的临界值，未触发扩容来缓解这种情况。之后，删除元素降低元素总数量，再插入很多元素，导致创建很多的 overflow bucket，但就是不会触发第 1 点的规定，你能拿我怎么办？\noverflow bucket 数量太多，导致 key 会很分散，查找插入效率低得吓人，因此出台第 2 点规定。\n这就像是一座空城，房子很多，但是住户很少，都分散了，找起人来很困难。\n解决办法就是开辟一个新 bucket 空间， 将老 bucket 中的元素移动到新 bucket，使得同一个 bucket 中的 key 排列地更紧密。这样，原来，在 overflow bucket 中的 key 可以移动到 bucket 中来。节省了空间，提高 bucket 利用率，map 的查找和插入效率自然就会提升。\n扩容函数 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 func hashGrow(t *maptype, h *hmap) { // If we\u0026#39;ve hit the load factor, get bigger. // Otherwise, there are too many overflow buckets, // so keep the same number of buckets and \u0026#34;grow\u0026#34; laterally. bigger := uint8(1) if !overLoadFactor(h.count+1, h.B) { bigger = 0 h.flags |= sameSizeGrow } oldbuckets := h.buckets newbuckets, nextOverflow := makeBucketArray(t, h.B+bigger, nil) flags := h.flags \u0026amp;^ (iterator | oldIterator) if h.flags\u0026amp;iterator != 0 { flags |= oldIterator } // commit the grow (atomic wrt gc) h.B += bigger h.flags = flags h.oldbuckets = oldbuckets h.buckets = newbuckets h.nevacuate = 0 h.noverflow = 0 if h.extra != nil \u0026amp;\u0026amp; h.extra.overflow != nil { // Promote current overflow buckets to the old generation. if h.extra.oldoverflow != nil { throw(\u0026#34;oldoverflow is not nil\u0026#34;) } h.extra.oldoverflow = h.extra.overflow h.extra.overflow = nil } if nextOverflow != nil { if h.extra == nil { h.extra = new(mapextra) } h.extra.nextOverflow = nextOverflow } // the actual copying of the hash table data is done incrementally // by growWork() and evacuate(). } 由于 map 扩容需要将原有的 key/value 重新搬迁到新的内存地址，如果有大量的 key/value 需要搬迁，会非常影响性能。因此 Go map 的扩容采取了一种称为“渐进式”的方式，原有的 key 并不会一次性搬迁完毕，每次最多只会搬迁 2 个 bucket。\n上面说的 hashGrow() 函数实际上并没有真正地“搬迁”，它只是分配好了新的 buckets，并将老的 buckets 挂到了 oldbuckets 字段上。真正搬迁 buckets 的动作在 growWork() 函数中，而调用 growWork() 函数的动作是在 mapassign 和 mapdelete 函数中。也就是插入或修改、删除 key 的时候，都会尝试进行搬迁 buckets 的工作。先检查 oldbuckets 是否搬迁完毕，具体来说就是检查 oldbuckets 是否为 nil。\n迁移 hashGrow 操作算是扩容之前的准备工作，实际拷贝的过程在 evacuate 中。\n1 2 3 4 5 6 7 8 9 10 func growWork(t *maptype, h *hmap, bucket uintptr) { // make sure we evacuate the oldbucket corresponding // to the bucket we\u0026#39;re about to use evacuate(t, h, bucket\u0026amp;h.oldbucketmask()) // evacuate one more oldbucket to make progress on growing if h.growing() { evacuate(t, h, h.nevacuate) } } 迁移条件 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 func evacuate(t *maptype, h *hmap, oldbucket uintptr) { b := (*bmap)(add(h.oldbuckets, oldbucket*uintptr(t.bucketsize))) // 在准备扩容之前桶的个数 newbit := h.noldbuckets() alg := t.key.alg if !evacuated(b) { // TODO: reuse overflow buckets instead of using new ones, if there // is no iterator using the old buckets. (If !oldIterator.) var ( x, y *bmap // 在新桶里面 低位桶和高位桶 xi, yi int // key 和 value 值的索引值分别为 xi ， yi xk, yk unsafe.Pointer // 指向 x 和 y 的 key 值的指针 xv, yv unsafe.Pointer // 指向 x 和 y 的 value 值的指针 ) // 新桶中低位的一些桶 x = (*bmap)(add(h.buckets, oldbucket*uintptr(t.bucketsize))) xi = 0 // 扩容以后的新桶中低位的第一个 key 值 xk = add(unsafe.Pointer(x), dataOffset) // 扩容以后的新桶中低位的第一个 key 值对应的 value 值 xv = add(xk, bucketCnt*uintptr(t.keysize)) // 如果不是等量扩容 if !h.sameSizeGrow() { y = (*bmap)(add(h.buckets, (oldbucket+newbit)*uintptr(t.bucketsize))) yi = 0 yk = add(unsafe.Pointer(y), dataOffset) yv = add(yk, bucketCnt*uintptr(t.keysize)) } // 依次遍历溢出桶 for ; b != nil; b = b.overflow(t) { k := add(unsafe.Pointer(b), dataOffset) v := add(k, bucketCnt*uintptr(t.keysize)) // 遍历 key - value 键值对 for i := 0; i \u0026lt; bucketCnt; i, k, v = i+1, add(k, uintptr(t.keysize)), add(v, uintptr(t.valuesize)) { top := b.tophash[i] if top == empty { b.tophash[i] = evacuatedEmpty continue } if top \u0026lt; minTopHash { throw(\u0026#34;bad map state\u0026#34;) } k2 := k // key 值如果是指针，则取出指针里面的值 if t.indirectkey { k2 = *((*unsafe.Pointer)(k2)) } useX := true if !h.sameSizeGrow() { // 如果不是等量扩容，则需要重新计算 hash 值，不管是高位桶 x 中，还是低位桶 y 中 hash := alg.hash(k2, uintptr(h.hash0)) if h.flags\u0026amp;iterator != 0 { if !t.reflexivekey \u0026amp;\u0026amp; !alg.equal(k2, k2) { // 如果两个 key 不相等，那么他们俩极大可能旧的 hash 值也不相等。 // tophash 对要迁移的 key 值也是没有多大意义的，所以我们用低位的 tophash 辅助扩容，标记一些状态。 // 为下一个级 level 重新计算一些新的随机的 hash 值。以至于这些 key 值在多次扩容以后依旧可以均匀分布在所有桶中 // 判断 top 的最低位是否为1 if top\u0026amp;1 != 0 { hash |= newbit } else { hash \u0026amp;^= newbit } top = uint8(hash \u0026gt;\u0026gt; (sys.PtrSize*8 - 8)) if top \u0026lt; minTopHash { top += minTopHash } } } useX = hash\u0026amp;newbit == 0 } if useX { // 标记低位桶存在 tophash 中 b.tophash[i] = evacuatedX // 如果 key 的索引值到了桶最后一个，就新建一个 overflow if xi == bucketCnt { newx := h.newoverflow(t, x) x = newx xi = 0 xk = add(unsafe.Pointer(x), dataOffset) xv = add(xk, bucketCnt*uintptr(t.keysize)) } // 把 hash 的高8位再次存在 tophash 中 x.tophash[xi] = top if t.indirectkey { // 如果是指针指向 key ，那么拷贝指针指向 *(*unsafe.Pointer)(xk) = k2 // copy pointer } else { // 如果是指针指向 key ，那么进行值拷贝 typedmemmove(t.key, xk, k) // copy value } // 同理拷贝 value if t.indirectvalue { *(*unsafe.Pointer)(xv) = *(*unsafe.Pointer)(v) } else { typedmemmove(t.elem, xv, v) } // 继续迁移下一个 xi++ xk = add(xk, uintptr(t.keysize)) xv = add(xv, uintptr(t.valuesize)) } else { // 这里是高位桶 y，迁移过程和上述低位桶 x 一致，下面就不再赘述了 b.tophash[i] = evacuatedY if yi == bucketCnt { newy := h.newoverflow(t, y) y = newy yi = 0 yk = add(unsafe.Pointer(y), dataOffset) yv = add(yk, bucketCnt*uintptr(t.keysize)) } y.tophash[yi] = top if t.indirectkey { *(*unsafe.Pointer)(yk) = k2 } else { typedmemmove(t.key, yk, k) } if t.indirectvalue { *(*unsafe.Pointer)(yv) = *(*unsafe.Pointer)(v) } else { typedmemmove(t.elem, yv, v) } yi++ yk = add(yk, uintptr(t.keysize)) yv = add(yv, uintptr(t.valuesize)) } } } // Unlink the overflow buckets \u0026amp; clear key/value to help GC. if h.flags\u0026amp;oldIterator == 0 { b = (*bmap)(add(h.oldbuckets, oldbucket*uintptr(t.bucketsize))) // Preserve b.tophash because the evacuation // state is maintained there. if t.bucket.kind\u0026amp;kindNoPointers == 0 { memclrHasPointers(add(unsafe.Pointer(b), dataOffset), uintptr(t.bucketsize)-dataOffset) } else { memclrNoHeapPointers(add(unsafe.Pointer(b), dataOffset), uintptr(t.bucketsize)-dataOffset) } } } // Advance evacuation mark if oldbucket == h.nevacuate { h.nevacuate = oldbucket + 1 // Experiments suggest that 1024 is overkill by at least an order of magnitude. // Put it in there as a safeguard anyway, to ensure O(1) behavior. stop := h.nevacuate + 1024 if stop \u0026gt; newbit { stop = newbit } for h.nevacuate != stop \u0026amp;\u0026amp; bucketEvacuated(t, h, h.nevacuate) { h.nevacuate++ } if h.nevacuate == newbit { // newbit == # of oldbuckets // Growing is all done. Free old main bucket array. h.oldbuckets = nil // Can discard old overflow buckets as well. // If they are still referenced by an iterator, // then the iterator holds a pointers to the slice. if h.extra != nil { h.extra.overflow[1] = nil } h.flags \u0026amp;^= sameSizeGrow } } } 如果未迁移完毕，赋值/删除的时候，扩容完毕后（预分配内存），不会马上就进行迁移。而是采取增量扩容的方式，当有访问到具体 bukcet 时，才会逐渐的进行迁移（将 oldbucket 迁移到 bucket）\n迁移函数 hmap.nevacuate 标识的是当前的进度，如果都搬迁完，应该和 2^B 的长度是一样的 在 evacuate 方法实现是把这个位置对应的 bucket，以及其冲突链上的数据都转移到新的 bucket 上。\n先要判断当前 bucket 是不是已经转移。 (oldbucket 标识需要搬迁的 bucket 对应的位置)。转移的判断直接通过 tophash 就可以，判断 tophash 中第一个 hash 值即可\n1 2 3 4 5 6 7 8 9 10 var ( emptyOne = 1 // this cell is empty minTopHash = 5 // minimum tophash for a normal filled cell. ) func evacuated(b *bmap) bool { h := b.tophash[0] return h \u0026gt; emptyOne \u0026amp;\u0026amp; h \u0026lt; minTopHash } 如果没有被转移，那就要迁移数据了。数据迁移时，可能是迁移到大小相同的 buckets 上，也可能迁移到2倍大的 buckets 上。这里 xy 都是标记目标迁移位置的标记：x 标识的是迁移到相同的位置，y 标识的是迁移到2倍大的位置上。我们先看下目标位置的确定：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 var xy [2]evacDst x := \u0026amp;xy[0] x.b = (*bmap)(add(h.buckets, oldbucket*uintptr(t.bucketsize))) x.k = add(unsafe.Pointer(x.b), dataOffset) x.e = add(x.k, bucketCnt*uintptr(t.keysize)) if !h.sameSizeGrow() { // Only calculate y pointers if we\u0026#39;re growing bigger. // Otherwise GC can see bad pointers. y := \u0026amp;xy[1] y.b = (*bmap)(add(h.buckets, (oldbucket+newbit)*uintptr(t.bucketsize))) y.k = add(unsafe.Pointer(y.b), dataOffset) y.e = add(y.k, bucketCnt*uintptr(t.keysize)) } 确定bucket位置后，需要按照kv 一条一条做迁移。\n如果当前搬迁的 bucket 和 总体搬迁的 bucket 的位置是一样的，我们需要更新总体进度的标记 nevacuate\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 func advanceEvacuationMark(h *hmap, t *maptype, newbit uintptr) { h.nevacuate++ // Experiments suggest that 1024 is overkill by at least an order of magnitude. // Put it in there as a safeguard anyway, to ensure O(1) behavior. stop := h.nevacuate + 1024 if stop \u0026gt; newbit { stop = newbit } for h.nevacuate != stop \u0026amp;\u0026amp; bucketEvacuated(t, h, h.nevacuate) { h.nevacuate++ } if h.nevacuate == newbit { // newbit == # of oldbuckets // Growing is all done. Free old main bucket array. h.oldbuckets = nil // Can discard old overflow buckets as well. // If they are still referenced by an iterator, // then the iterator holds a pointers to the slice. if h.extra != nil { h.extra.oldoverflow = nil } h.flags \u0026amp;^= sameSizeGrow } } 总结 map是引用类型 map遍历是无序的 map是非线程安全的 map的哈希冲突解决方式是链表法 map的迁移是逐步进行的，在每次赋值时，会做至少一次迁移工作 map中删除 key，有可能导致出现很多空的 kv，这会导致迁移操作，如果可以避免，尽量避免 扩容分为增量扩容和等量扩容。增量扩容，会增加桶的个数（增加一倍），把原来一个桶中的 keys 被重新分配到两个桶中。等量扩容，不会更改桶的个数，只是会将桶中的数据变得紧凑。不管是增量扩容还是等量扩容，都需要创建新的桶数组，并不是原地操作的。 map中定义了2的B次方个桶，每个桶中能够容纳8个key。根据key的不同哈希值，将其散落到不同的桶中。哈希值的低位（哈希值的后B个bit位）决定桶序号，高位（哈希值的前8个bit位）标识同一个桶中的不同 key。 参考 https://www.cnblogs.com/cnblogs-wangzhipeng/p/13292524.html https://halfrost.com/go_map_chapter_one/ https://zhuanlan.zhihu.com/p/273666774 ","date":"2022-08-25T00:00:00Z","permalink":"https://ggxxll.github.io/p/golang-map/","title":"Golang Map"},{"content":"概述 https://zhuanlan.zhihu.com/p/98135840\n链接 文档 在线体验 数据库引擎 Lazy\n会将最近一次查询的表保存在内存中, 过期时间为expiration_time_in_seconds. 仅适用于*Log表引擎. 如果表的数量较多, 数据量小, 且访问间隔长, 建议使用Lazy引擎数据库. Atomic\n不指定引擎时, 默认使用Atomic. 支持无阻塞删除和重命名表. 默认会生成UUID, 不建议指定UUID 不改变uuid和迁移表数据的话, 执行rename table可以立即生效 DROP/DETACH TABLE不会真正的删除表, 只是标记被删除了. 如果同步模式设置为SYNC, 在等待已有的查询/插入操作结束之后, 表会被立即删除. 使用表引擎ReplicatedMergeTree, 不建议指定path和replica name ReplicatedReplacingMergeTree('/clickhouse/tables/{shard}/{database}/{table}', '{replica}') Mysql\n远程映射Mysql的库表. 在clickhouse执行的查询等, 会被引擎转换, 发送到Mysql服务中. 不能执行 RENAME, CREATE TABLE, ALTER 简单 WHERE 条款如 =, !=, \u0026gt;, \u0026gt;=, \u0026lt;, \u0026lt;= 当前在MySQL服务器上执行。其余的条件和 LIMIT 只有在对MySQL的查询完成后，才会在ClickHouse中执行采样约束。 删除DROP和卸载DETACH 表，不会删除mysql中的表，且允许通过ATTACH再装载回来 不支持的Mysql的数据类型会转换成String, 所有类型都支持Nullable MaterializeMySQL\n使用MySQL中存在的所有表以及这些表中的所有数据创建ClickHouse数据库（即库级别的数据同步）。ClickHouse服务器用作MySQL副本。它读取binlog并执行DDL和DML查询。默认表引擎设置为ReplacingMergeTree\n使用表引擎 ReplacingMergeTree 时, 会添加虚拟列 _sign 和 _version\n_version — Transaction counter. Type [UInt64]\n_sign — Deletion mark. Type [Int8]. Possible values:\n1 — Row is not deleted, -1 — Row is deleted. MySQL DDL查询将转换为相应的ClickHouse DDL查询（ALTER，CREATE，DROP，RENAME）。如果ClickHouse无法解析某些DDL查询，则该查询将被忽略。\n不支持 INSERT, DELETE 和 UPDATE操作. 复制时都视为INSERT操作, 会在数据上使用_sign标记.\nINSERT, 1 DELETE, -1 UPDATE, 1或-1 执行SELECT查询时\n不指定_version, 默认使用FINAL, 即MAX(_version) 不指定_sign, 默认会限制查询条件WHERE _sign=1 注意事项:\n_sign=-1的数据是逻辑删除 不支持update/delete 复制过程很容易崩溃 禁止手动操作数据库和表 通过设置optimize_on_insert, 启用或禁用在插入之前进行数据转换，就像合并是在此块上完成的（根据表引擎）一样。 不支持的Mysql数据类型会异常Unhandled data type并停止复制， 都支持Nullable PostgreSQL 远程链接PostgreSQL. 支持查询, 插入等操作. 支持修改表结构 (注意使用缓存时的情况: DETACH 和ATTCH可刷新缓存). 数据类型中仅INTEGER支持Nullable 表引擎 文档 MergeTree Kakfa，Mysql 创建数据库 1 CREATE DATABASE [IF NOT EXISTS] db_name [ON CLUSTER cluster] [ENGINE = engine(...)] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster] ( name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1] [COMMENT expr1] [TTL expr1], name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2] [COMMENT expr1] [TTL expr2], ... INDEX index_name1 expr1 TYPE type1(...) GRANULARITY value1, INDEX index_name2 expr2 TYPE type2(...) GRANULARITY value2 ) ENGINE = MergeTree() -- 指定数据排序使用字段，默认等同主键 ORDER BY (expr1[, expr2,...]) -- 指定数据分区使用的字段 [PARTITION BY (expr1[, expr2,...])] -- 指定主键 [PRIMARY KEY (expr1[, expr2,...])] -- 指定采样数据使用的字段，启用数据采样时，不会对所有数据执行查询，而只对特定部分数据（样本）执行查询 [SAMPLE BY (expr1[, expr2,...])] [TTL expr [DELETE|TO DISK \u0026#39;xxx\u0026#39;|TO VOLUME \u0026#39;xxx\u0026#39; [, ...] ] [WHERE conditions] [GROUP BY key_expr [SET v1 = aggr_func(v1) [, v2 = aggr_func(v2) ...]] ] ] [SETTINGS name=value, ...] 字典 字典是一个映射 (键 -\u0026gt; 属性）, 是方便各种类型的参考清单。\nClickHouse支持一些特殊函数配合字典在查询中使用。 将字典与函数结合使用比将 JOIN 操作与引用表结合使用更简单、更有效。\n内置字典：处理地理数据库 外部字典：自定义字典 特殊查询 1 2 3 4 5 6 7 8 9 10 11 -- 计算pv值\u0026gt;20的记录条数 select sum(pv\u0026gt;20) from test_all; select count(*) from test_all where pv\u0026gt;20; -- 计算pv\u0026gt;20的记录条数占比：0.5 select avg(pv\u0026gt;20) from test_all; -- 计算pv在1,2，20这几个值中的记录条数的占比 select avg(pv in (1,2,2,20)) from test_all; -- Date系列类型， 插入时可以混合时间戳和字符串插入 INSERT INTO dt Values (1546300800, 1), (\u0026#39;2019-01-01\u0026#39;, 2); 数据类型 UUID FixedString Enum LowCardinality(T), 把其它数据类型转变为字典编码类型。如果该字段的值去重个数小于1W，性能优于普通类型。反之，性能下降。 通常， 一个字符串类型的Enum类型， 建议设置为LowCardinality， 更灵活和高效 Array(T\u0026hellip;) AggregateFunction， 适用于表引擎 AggregatingMergeTree Nested 嵌套字段， 不支持表引擎 MergeTree Tuple Nullable Domains： IPV4， IPV6 Geo： experimental feature Point ： 坐标，Tuple（x float64, y float64） Ring： Array(Point) Polygon： Array(Ring) MultiPolygon：Array(Polygon) Map(key,value) experimental feature SimpleAggregateFunction 性能优于 AggregateFunction， 适用于表引擎 AggregatingMergeTree 注意事项 使用 Nullable 几乎总是对性能产生负面影响，在设计数据库时请记住这一点, 最好设置default值。 删除表的时候， 会先在元数据标记被删除，exists、select、insert等会立即生效（表不存在）， 但是create table创建表等操作会抛异常（表存在）。 稍等几分钟，再新建表即可。 删除表数据，应当操作本地表，操作分布式Distributed表会报错。 ReplacingMergeTree表引擎， 完全相同的数据会直接去重， 数据是在后台不确定的时间去合并去重的。不建议使用OPTIMIZE发起合并， 使用 FINAL 关键字进行查询，可以得到主键相同的、最新被插入的数据。 那些有相同分区表达式值PARTITION BY的数据片段才会合并。这意味着 你不应该用太精细的分区方案（超过一千个分区）。否则，会因为文件系统中的文件数量过多和需要打开的文件描述符过多，导致 SELECT 查询效率不佳。可以根据group by语句来分析如何分区。绝对不能使用带有主键性质的字段做分区（比如唯一id）， 最好选择一个重复率高的字段（比如 日期， 渠道等）。 Alter语句DROP, MODIFY 时， 该字段不能是主键，会报错 使用 1 2 3 4 5 6 7 8 9 10 create table test on cluster cluster1 ( date Date, id UInt32 default 0 comment \u0026#39;123\u0026#39;, name String default \u0026#39;\u0026#39;, pv UInt32 default 0 ) ENGINE = ReplicatedReplacingMergeTree(\u0026#39;/clickhouse/tables/{shard}/{database}/{table}\u0026#39;, \u0026#39;{replica}\u0026#39;) PARTITION BY date ORDER BY (date,id); 1 2 3 4 5 6 7 8 create table test_all on cluster cluster1 ( date Date, id UInt32 default 0 comment \u0026#39;123\u0026#39;, name String default \u0026#39;\u0026#39;, pv UInt32 default 0 ) engine = Distributed(\u0026#39;cluster1\u0026#39;, \u0026#39;wangzhuan\u0026#39;, \u0026#39;test\u0026#39;, sipHash64(date)); 1 2 3 4 -- 结果是一条数据 insert into test_all (date, id, name, pv) VALUES (\u0026#39;2021-05-20\u0026#39;,1,\u0026#39;a\u0026#39;,10), (\u0026#39;2021-05-20\u0026#39;,1,\u0026#39;a\u0026#39;,10); 1 2 3 4 5 6 7 8 9 10 11 12 13 -- 结果是两条数据 insert into test_all (date, id, name, pv) VALUES (\u0026#39;2021-05-20\u0026#39;, 1, \u0026#39;a\u0026#39;, 10) , (\u0026#39;2021-05-20\u0026#39;, 1, \u0026#39;a\u0026#39;, 20); -- 结果是两条数据 30被20覆盖了， insert into test_all (date, id, name, pv) VALUES (\u0026#39;2021-05-20\u0026#39;, 1, \u0026#39;a\u0026#39;, 30) , (\u0026#39;2021-05-20\u0026#39;, 1, \u0026#39;a\u0026#39;, 20); -- 当然， 经过后台的数据合并之后， 应该是只剩1条数据 pv=30的 1 2 3 4 5 6 7 select * from test_all; select * from test_all final; select sum(pv) from test_all; select sum(pv) from test_all final; 1 2 3 alter table test on cluster cluster1 delete where 1; alter table test on cluster cluster1 delete where date=\u0026#39;2021-05-20\u0026#39;; 1 2 3 4 5 6 7 8 9 10 11 12 -- 仅当分区被使用到（PARTITION BY的字段的值超过一个），system.parts表里才会有数据 SELECT partition, name, active FROM system.parts WHERE table = \u0026#39;test\u0026#39;; SELECT table,count(*) FROM system.parts group by table order by count(*) desc; 1 2 drop table test on cluster cluster1; drop table test_all on cluster cluster1; ","date":"2022-08-24T00:00:00Z","permalink":"https://ggxxll.github.io/p/clickhouse%E5%88%9D%E4%BD%93%E9%AA%8C/","title":"Clickhouse初体验"},{"content":"环境 工具：docker，docker-compose 需求描述 在本地搭建业务中可能使用的服务，方便本地进行测试。\n配置文件 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 version: \u0026#34;3\u0026#34; services: zookeeper: container_name: zookeeper image: \u0026#39;bitnami/zookeeper:latest\u0026#39; ports: - \u0026#39;2181:2181\u0026#39; environment: - ALLOW_ANONYMOUS_LOGIN=yes kafka: container_name: kafka image: \u0026#39;bitnami/kafka:latest\u0026#39; ports: - \u0026#39;9092:9092\u0026#39; environment: - KAFKA_BROKER_ID=1 - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092 - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://127.0.0.1:9092 - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181 - ALLOW_PLAINTEXT_LISTENER=yes depends_on: - zookeeper volumes: - \u0026#39;kafka:/bitnami/kafka\u0026#39; redis: container_name: redis image: \u0026#39;redis:latest\u0026#39; ports: - \u0026#39;6379:6379\u0026#39; volumes: - \u0026#39;redis:/usr/local/etc/redis\u0026#39; mysql: container_name: mysql image: \u0026#39;mysql:latest\u0026#39; ports: - \u0026#39;3306:3306\u0026#39; environment: MYSQL_ALLOW_EMPTY_PASSWORD: \u0026#39;yes\u0026#39; MYSQL_DATABASE: app volumes: - \u0026#39;mysql:/var/lib/mysql\u0026#39; mongo: container_name: mongo image: \u0026#39;mongo:latest\u0026#39; ports: - \u0026#39;27017:27017\u0026#39; volumes: - \u0026#39;mongo:/data/db\u0026#39; etcd: container_name: etcd image: \u0026#39;quay.io/coreos/etcd:latest\u0026#39; ports: - \u0026#39;2379:2379\u0026#39; - \u0026#39;2380:2380\u0026#39; volumes: - \u0026#39;etcd:/etcd-data\u0026#39; command: - /usr/local/bin/etcd - --data-dir=/etcd-data - --name=node1 - --initial-advertise-peer-urls=http://192.168.82.116:2380 - --listen-peer-urls=http://0.0.0.0:2380 - --advertise-client-urls=http://192.168.82.116:2379 - --listen-client-urls=http://0.0.0.0:2379 - --initial-cluster - node1=http://192.168.82.116:2380 es: container_name: es image: docker.elastic.co/elasticsearch/elasticsearch:7.14.0 environment: - discovery.type=single-node ulimits: memlock: soft: -1 hard: -1 volumes: - es:/usr/share/elasticsearch/data ports: - \u0026#39;9200:9200\u0026#39; volumes: redis: {} mysql: {} kafka: {} es: {} mongo: {} etcd: {} ","date":"2022-08-24T00:00:00Z","permalink":"https://ggxxll.github.io/p/docker%E6%B5%8B%E8%AF%95%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/","title":"docker测试环境搭建"},{"content":"环境 系统: windows 语言: golang 框架: gin Bug描述 在如下代码中, ctx := c和ctx := context.Background()传递到cli.Set中是没有问题的.\n使用ctx := c.Request.Context()时, 会打印出错误: context canceled.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;github.com/gin-gonic/gin\u0026#34; \u0026#34;github.com/go-redis/redis/v8\u0026#34; ) func main() { cli := redis.NewClient(\u0026amp;redis.Options{ Addr: \u0026#34;127.0.0.1:6379\u0026#34;, }) r := gin.Default() r.GET(\u0026#34;/ping\u0026#34;, func(c *gin.Context) { //ctx := c //ctx := context.Background() ctx := c.Request.Context() go func() { if err := cli.Set(ctx, \u0026#34;hello\u0026#34;, 1, 0).Err(); err != nil { fmt.Println(err) } }() c.JSON(200, gin.H{ \u0026#34;message\u0026#34;: \u0026#34;pong\u0026#34;, }) }) r.Run() // listen and serve on 0.0.0.0:8080 } 分析 gin的Request对象来自*http.Request, handlerFunc被调用完成之后, context就被cancel了\n在ctx := c.Request.Context()打断点, 可以找到如下代码:\n1 2 3 4 5 6 func{ ... serverHandler{c.server}.ServeHTTP(w, w.req) w.cancelCtx() ... } 其他 使用gin+opentracing时, 不能使用 作为上下文传递, 要使用c.Request.Context(), 而不是gin.Context. 需要在goroutine中传递ctx, 应该拿出span新建一个ctx 1 2 3 4 func NewTracingContextWithParentContext(ctx context.Context) context.Context { span := opentracing.SpanFromContext(ctx) return opentracing.ContextWithSpan(context.Background(), span) } ginhttp中封装好了相应的中间件, 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 func Middleware(tr opentracing.Tracer, options ...MWOption) gin.HandlerFunc { opts := mwOptions{ opNameFunc: func(r *http.Request) string { return \u0026#34;HTTP \u0026#34; + r.Method }, spanObserver: func(span opentracing.Span, r *http.Request) {}, urlTagFunc: func(u *url.URL) string { return u.String() }, } for _, opt := range options { opt(\u0026amp;opts) } return func(c *gin.Context) { carrier := opentracing.HTTPHeadersCarrier(c.Request.Header) ctx, _ := tr.Extract(opentracing.HTTPHeaders, carrier) op := opts.opNameFunc(c.Request) sp := tr.StartSpan(op, ext.RPCServerOption(ctx)) ext.HTTPMethod.Set(sp, c.Request.Method) ext.HTTPUrl.Set(sp, opts.urlTagFunc(c.Request.URL)) opts.spanObserver(sp, c.Request) // set component name, use \u0026#34;net/http\u0026#34; if caller does not specify componentName := opts.componentName if componentName == \u0026#34;\u0026#34; { componentName = defaultComponentName } ext.Component.Set(sp, componentName) c.Request = c.Request.WithContext( opentracing.ContextWithSpan(c.Request.Context(), sp)) c.Next() ext.HTTPStatusCode.Set(sp, uint16(c.Writer.Status())) sp.Finish() } } ","date":"2022-08-24T00:00:00Z","permalink":"https://ggxxll.github.io/p/gin-opentracing%E7%9A%84%E4%BD%BF%E7%94%A8/","title":"gin+opentracing的使用"},{"content":"使用Kibana + Filebeat + Nginx 进行日志监控，Nginx的error日志的时区提前了8个小时，解决方案如下：\n原博： https://www.colabug.com/2018/0425/2780769/\n步骤一 将 /usr/share/filebeat/module/nginx/error/ingest/pipeline.json 里面的\n1 2 3 4 5 6 7 \u0026#34;date\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;nginx.error.time\u0026#34;, \u0026#34;target_field\u0026#34;: \u0026#34;@timestamp\u0026#34;, \u0026#34;formats\u0026#34;: [\u0026#34;yyyy/MM/dd H:m:s\u0026#34;], {\u0026lt; if .convert_timezone \u0026gt;}\u0026#34;timezone\u0026#34;: \u0026#34;{{ beat.timezone }}\u0026#34;,{\u0026lt; end \u0026gt;} \u0026#34;ignore_failure\u0026#34;: true } 改为：\n1 2 3 4 5 6 7 \u0026#34;date\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;nginx.error.time\u0026#34;, \u0026#34;target_field\u0026#34;: \u0026#34;@timestamp\u0026#34;, \u0026#34;formats\u0026#34;: [\u0026#34;yyyy/MM/dd H:m:s\u0026#34;], \u0026#34;timezone\u0026#34; : \u0026#34;Asia/Shanghai\u0026#34;, \u0026#34;ignore_failure\u0026#34;: true } 步骤二 1 2 3 4 5 6 7 8 9 删除旧的pipeline: curl -XDELETE \u0026#34;http://localhost:9200/_ingest/pipeline/filebeat-6.7.0-nginx-error-pipeline\u0026#34; # 重新设置 filebeat setup -e # 查看是否生效 curl -XGET \u0026#34;http://localhost:9200/_ingest/pipeline/filebeat-6.7.0-nginx-error-pipeline\u0026#34; 注意： 一般来说索引格式固定，修改版本号（6.7.0）为自己的安装版本\n","date":"2022-08-24T00:00:00Z","permalink":"https://ggxxll.github.io/p/kibana-filebeat-nginx%E7%9A%84error%E6%97%A5%E5%BF%97%E6%97%B6%E5%8C%BA%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/","title":"Kibana+Filebeat+Nginx的error日志时区问题解决方案"},{"content":"原贴: https://www.cnblogs.com/winkey4986/p/6433704.html\n\u0026ndash;数据库中单个表的大小（不包含索引）\n1 select pg_size_pretty(pg_relation_size(\u0026#39;表名\u0026#39;)); \u0026ndash;查出所有表（包含索引）并排序\n1 2 3 4 SELECT table_schema || \u0026#39;.\u0026#39; || table_name AS table_full_name, pg_size_pretty(pg_total_relation_size(\u0026#39;\u0026#34;\u0026#39; || table_schema || \u0026#39;\u0026#34;.\u0026#34;\u0026#39; || table_name || \u0026#39;\u0026#34;\u0026#39;)) AS size FROM information_schema.tables ORDER BY pg_total_relation_size(\u0026#39;\u0026#34;\u0026#39; || table_schema || \u0026#39;\u0026#34;.\u0026#34;\u0026#39; || table_name || \u0026#39;\u0026#34;\u0026#39;) DESC limit 20 \u0026ndash;查出表大小按大小排序并分离data与index\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 SELECT table_name, pg_size_pretty(table_size) AS table_size, pg_size_pretty(indexes_size) AS indexes_size, pg_size_pretty(total_size) AS total_size FROM ( SELECT table_name, pg_table_size(table_name) AS table_size, pg_indexes_size(table_name) AS indexes_size, pg_total_relation_size(table_name) AS total_size FROM ( SELECT (\u0026#39;\u0026#34;\u0026#39; || table_schema || \u0026#39;\u0026#34;.\u0026#34;\u0026#39; || table_name || \u0026#39;\u0026#34;\u0026#39;) AS table_name FROM information_schema.tables ) AS all_tables ORDER BY total_size DESC ) AS pretty_sizes ","date":"2022-08-24T00:00:00Z","permalink":"https://ggxxll.github.io/p/postgresql%E6%9F%A5%E8%AF%A2%E8%A1%A8%E7%9A%84%E5%A4%A7%E5%B0%8F/","title":"Postgresql查询表的大小"},{"content":"环境 系统: windows 语言: golang IDE: goland 安装 Protoc 下载protoc-xxx-win64.zip, 并解压到安装位置(如: d:/protoc)即可 添加环境变量d:/protoc/bin到PATH中 使用protoc --version验证是否安装成功 配置 Golang 环境 下载第三方protobuf库, 并解压到d:/protoc/include, 如: googleapis gogo/protobuf go install google.golang.org/protobuf/cmd/protoc-gen-go 安装goland插件 Protocol Buffer Editor, 按照README在配置中添加d:/protoc/include和自己实现的库的路径(目前好像是每个项目使用时都需要配置一下), 完成之后就有自动提示了（可能需要重启IDE） 使用 命令: protoc -I=. --go_out=. example.proto -I用于指定imports path, 第三方库统一放到d:/protoc/include的话是不需要指定的 官方文档 包括protobuf语法和其他语言的使用教程点击\n","date":"2022-08-24T00:00:00Z","permalink":"https://ggxxll.github.io/p/protoc%E4%BD%BF%E7%94%A8/","title":"Protoc使用"},{"content":"某日突然发现Sublime Text3（3.2.2）插件OmniMarkupPreviewer打开页面出现404错误\n1 2 3 4 5 6 7 Error: 404 Not Found Sorry, the requested URL \u0026#39;http://127.0.0.1:51004/view/26\u0026#39; caused an error: \u0026#39;buffer_id(26) is not valid (closed or unsupported file format)\u0026#39; **NOTE:** If you run multiple instances of Sublime Text, you may want to adjust the `server_port` option in order to get this plugin work again. 查看Console窗口出现如下错误：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 OmniMarkupPreviewer: [ERROR] Exception occured while rendering using MarkdownRenderer Traceback (most recent call last): File \u0026#34;%AppData%\\Roaming\\Sublime Text 3\\Packages\\OmniMarkupPreviewer\\OmniMarkupLib\\RendererManager.py\u0026#34;, line 266, in render_text rendered_text = renderer.render(text, filename=filename) File \u0026#34;%AppData%\\Roaming\\Sublime Text 3\\Packages\\OmniMarkupPreviewer\\OmniMarkupLib\\Renderers/MarkdownRenderer.py\u0026#34;, line 48, in render extensions=self.extensions) File \u0026#34;%AppData%\\Roaming\\SUBLIM~1\\Packages\\PYTHON~3\\st3\\markdown\\core.py\u0026#34;, line 390, in markdown md = Markdown(**kwargs) File \u0026#34;%AppData%\\Roaming\\SUBLIM~1\\Packages\\PYTHON~3\\st3\\markdown\\core.py\u0026#34;, line 100, in __init__ configs=kwargs.get(\u0026#39;extension_configs\u0026#39;, {})) File \u0026#34;%AppData%\\Roaming\\SUBLIM~1\\Packages\\PYTHON~3\\st3\\markdown\\core.py\u0026#34;, line 126, in registerExtensions ext = self.build_extension(ext, configs.get(ext, {})) File \u0026#34;%AppData%\\Roaming\\SUBLIM~1\\Packages\\PYTHON~3\\st3\\markdown\\core.py\u0026#34;, line 166, in build_extension module = importlib.import_module(ext_name) File \u0026#34;./python3.3/importlib/__init__.py\u0026#34;, line 90, in import_module File \u0026#34;\u0026lt;frozen importlib._bootstrap\u0026gt;\u0026#34;, line 1584, in _gcd_import File \u0026#34;\u0026lt;frozen importlib._bootstrap\u0026gt;\u0026#34;, line 1565, in _find_and_load File \u0026#34;\u0026lt;frozen importlib._bootstrap\u0026gt;\u0026#34;, line 1529, in _find_and_load_unlocked ImportError: No module named \u0026#39;xxx\u0026#39; 原因 OmniMarkupPreviewer插件使用python-markdown@3.2.2\nSublime Text3使用python-markdown@3.1.1\n当sys.path中的路径下有相同包名时，import会优先导入路径靠前的包\n所以Sublime Text3按照启动顺序加载sys.path后，会优先使用python-markdown@3.1.1，出现导包错误\n解决方案 本人是windows系统，使用%AppData%\\Sublime Text 3\\Packages\\OmniMarkupPreviewer\\OmniMarkupLib\\Renderers\\libs\\markdown文件夹覆盖%AppData%\\Sublime Text 3\\Packages\\python-markdown\\st3\\markdown\n然后重启Sublime Text3即可\n","date":"2022-08-24T00:00:00Z","permalink":"https://ggxxll.github.io/p/sublimetext3-omnimarkuppreviewer%E5%87%BA%E7%8E%B0404%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/","title":"SublimeText3+OmniMarkupPreviewer出现404问题解决方案"},{"content":"supervisor安装使用, 并配置服务崩溃邮件报警\n环境 系统: Centos7, python3\n安装依赖: 1 yum install -y supervisor sendmail mailx \u0026amp;\u0026amp; pip3 install superlance 配置邮件: vim /etc/mail.rc, 然后添加如下内容: 1 2 3 4 5 6 7 8 9 # 发件人邮箱 set from=xxx@xxx.com # smtp服务 set smtp=smtps://smtp.xxx.com:465 # 用户名 set smtp-auth-user=xxx@xxx.com # 密码 set smtp-auth-password=xxx set ssl-verify=ignore 测试邮件: echo 'this is test'| /usr/bin/mail -s 'xxxxx' xxx@xx.com 配置项目: 编辑项目的配置文件, vim /etc/supervisord.d/xxx.ini\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # 项目相关配置 [program:projectName] # 设置命令在指定的目录内执行 directory=projectPath # 这里为您要管理的项目的启动命令 command=projectRunCmd # 以哪个用户来运行该进程 user=root # supervisor 启动时自动该应用 autostart=true # 进程退出后自动重启进程 autorestart=true # 进程持续运行多久才认为是启动成功 startsecs=2 # 重试次数 startretries=3 # stderr 日志输出位置 stderr_logfile=path/stderr.log # stdout 日志输出位置 stdout_logfile=path/stdout.log 1 2 3 4 5 6 7 8 # 报警邮件相关配置 [eventlistener:crashmail] command=crashmail -p senddemo -s \u0026#34;echo \u0026#39;projectName crashed!!\u0026#39;| /usr/bin/mail -s \u0026#39;projectName\u0026#39; xxx@xx.com,xxx@xx.com\u0026#34; events=PROCESS_STATE_EXITED ## stderr 日志输出位置 stderr_logfile=path/crashmail/stderr.log ## stdout 日志输出位置 stdout_logfile=path/crashmail/stdout.log 相关命令: 1 2 3 4 5 6 7 8 9 supervisord -c /etc/supervisord.conf # 启动supervisor, 然后才可可以使用supervisorctl supervisorctl stop program_name # 停止某一个进程，program_name 为 [program:name] 里的 name supervisorctl start program_name # 启动某个进程 supervisorctl restart program_name # 重启某个进程 supervisorctl stop groupworker: # 结束所有属于名为 groupworker 这个分组的进程 (start，restart 同理) supervisorctl stop groupworker:name1 # 结束 groupworker:name1 这个进程 (start，restart 同理) supervisorctl stop all # 停止全部进程，注：start、restartUnlinking stale socket /tmp/supervisor.sock supervisorctl reload # 载入最新的配置文件，停止原有进程并按新的配置启动、管理所有进程 supervisorctl update # 根据最新的配置文件，启动新配置或有改动的进程，配置没有改动的进程不会受影响而重启 ","date":"2022-08-24T00:00:00Z","permalink":"https://ggxxll.github.io/p/supervisor%E4%BD%BF%E7%94%A8/","title":"Supervisor使用"},{"content":"简介 core 是由大佬Reasno创建的一个golang开源库.。这个库的定位是一个服务容器，负责管理和协调符合 Twelve-Factor的网络应用程序。\n目前这个库的结构：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 core ├── clihttp ├── config ├── container ├── contract ├── cronopts ├── di ├── doc ├── dtx ├── events ├── internal ├── key ├── leader ├── logging ├── observability ├── otes ├── otetcd ├── otgorm ├── otkafka ├── otmongo ├── otredis ├── ots3 ├── queue ├── srvhttp ├── text └── unierr ","date":"2022-08-24T00:00:00Z","permalink":"https://ggxxll.github.io/p/%E5%BC%80%E6%BA%90%E9%A1%B9%E7%9B%AEcore/","title":"开源项目core"},{"content":"题一 问题1： 会输出什么？为什么？ 问题2： 应该如何修改？ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 package main import \u0026#34;fmt\u0026#34; func main() { cache := Cache{} fmt.Println(cache.Get(\u0026#34;foo\u0026#34;)) cache.Set(\u0026#34;foo\u0026#34;, \u0026#34;bar\u0026#34;) fmt.Println(cache.Get(\u0026#34;foo\u0026#34;)) } type Cache struct { data map[string]interface{} } func (c Cache) Get(s string) (interface{},bool) { v, ok := c.data[s] return v, ok } func (c Cache) Set(s string, v interface{}) { c.data[s] = v } 题二 问题1：会输出什么？为什么？ 问题2：应该如何修改？ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 package main import \u0026#34;fmt\u0026#34; func main() { cache := Cache{} cache.Add(1) fmt.Println(cache.List()) } type Cache struct { data []interface{} } func (c Cache) Add(s interface{}) { c.data = append(c.data, s) } func (c Cache) List() []interface{} { return c.data } 题三 问题1：为什么题一需要初始化data，题二不需要? 问题2：实现这个Cache还需要什么补充的吗? 题一解 问题1： Get 返回 nil, false Set 会报空指针错误 问题2： Cache 需要初始化 data 题二解 问题1： 输出空数组 问题2： 修改 func (c Cache) Add 为 func (c *Cache) Add 题三解 问题1：因为 map 是引用传递，数组是值传递 问题2：需要添加锁 sync.Mutex ，并将 Cache 的方法改为指针调用 ","date":"2022-08-24T00:00:00Z","permalink":"https://ggxxll.github.io/p/%E4%BD%BF%E7%94%A8-map-%E8%AE%BE%E8%AE%A1%E7%BC%93%E5%AD%98/","title":"使用 map 设计缓存"},{"content":"环境 golang 问题 在k8s中部署多个节点服务时，CDN的Range回源经常失败。\n原因：每个节点生成渠道包的时间（Modify-Time）不同，导致CDN回源时认为文件变动了，自动停止回源。\n解决：使用http.ServeContent, 手动传递\n代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 // serveFile // 将newApkPath地址下的文件返回给请求方。 // 注意：曾经这里是使用ctx.File(调用了Go的http.ServeFile)方法返回文件， // 但是这样做在集群部署的情况下存在问题。集群每个节点文件生成的时间存在差异， // 而go标准库中的实现是根据文件的最后修改时间判断文件是否发生了变动。 // 大文件下载时CDN会采用分片回源（Range Header）， 每个分片获取到的最后修改时间不一致，会判定为下载失败！ // 这里改成http.ServeContent实现后，强制将修改时间置为0值，Go的标准库会忽略最后修改时间检测，在集群-多节点环境下也能正常分片下载了。 func serveFile(ctx *gin.Context, newApkPath string) error { _, apkName := filepath.Split(newApkPath) ctx.Header(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/octet-stream\u0026#34;) ctx.Header(\u0026#34;Content-Disposition\u0026#34;, \u0026#34;attachment; filename=\u0026#34;+apkName) ctx.Header(\u0026#34;Content-Transfer-Encoding\u0026#34;, \u0026#34;binary\u0026#34;) f, err := os.OpenFile(newApkPath, os.O_RDONLY, os.ModePerm) if err != nil { return err } defer f.Close() /* 最后修改时间，强制0值，跳过检查 */ http.ServeContent(ctx.Writer, ctx.Request, apkName, time.Time{}, f) return nil } ","date":"2022-08-24T00:00:00Z","permalink":"https://ggxxll.github.io/p/%E6%96%87%E4%BB%B6%E4%B8%8B%E8%BD%BDbug/","title":"文件下载Bug"}]