[{"content":"根据加锁的范围，MySQL 里面的锁大致可以分成全局锁、表级锁和行锁三类\n全局锁 全局锁就是对整个数据库实例加锁。MySQL提供了一个加全局读锁的方法，命令是 Flush tables with read lock 。当需要让整个库处于只读状态的时候，可以使用这个命令，之后其他线程的以下语句会被阻塞：数据更新语句（数据的增删改）、数据定义语句（包括建表、修改表结构等）和更新类事务的提交语句\n全局锁的典型使用场景是，做全库逻辑备份。\n但是整个库都只读，可能出现以下问题：\n如果在主库上备份，那么在备份期间都不能执行更新，业务基本上就得停摆 如果在从库上备份，那么在备份期间从库不能执行主库同步过来的binlog，会导致主从延迟 在可重复读隔离级别下开启一个事务能够拿到一致性视图\n官方自带的逻辑备份工具是 mysqldump。当 mysqldump 使用参数 –single-transaction 的时候，导数据之前就会启动一个事务，来确保拿到一致性视图。而由于 MVCC 的支持，这个过程中数据是可以正常更新的。single-transaction 只适用于所有的表使用事务引擎的库\n1.既然要全库只读，为什么不使用set global readonly=true的方式？\n在有些系统中，readonly 的值会被用来做其他逻辑，比如用来判断一个库是主库还是备库。因此修改 global 变量的方式影响面更大 在异常处理机制上有差异。如果执行Flush tables with read lock命令之后由于客户端发生异常断开，那么MySQL会自动释放这个全局锁，整个库回到可以正常更新的状态。而将整个库设置为 readonly 之后，如果客户端发生异常，则数据库会一直保持 readonly 状态，这样会导致整个库长时间处于不可写状态，风险较高 表级锁 MySQL里面表级别的锁有两种：一种是表锁，一种是元数据锁（meta data lock，MDL）\n表锁 表锁的语法是 lock tables … read/write。可以用 unlock tables 主动释放锁，也可以在客户端断开的时候自动释放。lock tables 语法除了会限制别的线程的读写外，也限定了本线程接下来的操作对象\n如果在某个线程 A 中执行 lock tables t1 read,t2 wirte;这个语句，则其他线程写 t1、读写 t2 的语句都会被阻塞。同时，线程 A 在执行 unlock tables 之前，也只能执行读 t1、读写 t2 的操作，连写t1都不允许\n元数据锁 MDL 不需要显式使用，在访问一个表的时候会被自动加上。MDL 的作用是，保证读写的正确性。如果一个查询正在遍历一个表中的数据，而执行期间另一个线程对这个表结构做了变更，删了一列，那么查询线程拿到的结果跟表结构对不上，肯定不行\n在MySQL5.5版本引入了MDL，当对一个表做增删改查操作的时候，加MDL读锁；当要对表做结构变更操作的时候，加MDL写锁\n读锁之间不互斥，因此可以多个线程同时对一张表增删改查 读写锁之间、写锁之间是互斥的，用来保证变更表结构操作的安全性。 事务中的 MDL 锁，在语句执行开始时申请，但是语句结束后并不会马上释放，而会等到整个事务提交后再释放\n如果安全地给小表加字段？\n首先要解决长事务，事务不提交，就会一直占着 DML 锁。在 MySQL 的 information_schema 库的 innodb_trx 表中，可以查到当前执行的事务。如果要做DDL变更的表刚好有长事务在执行，要考虑先暂停 DDL，或者 kill 掉这个长事务\n如果要变更的表是一个热点表，虽然数据量不大，但是上面的请求很频繁，而又不得不加个字段，该怎么做？\n在 alter table 语句里面设定等待时间，如果在这个指定的等待时间里面能够拿到 MDL 写锁最好，拿不到也不要阻塞后面的业务语句，先放弃，之后再通过重试命令重复这个过程\n行锁 MySQL的行锁是在引擎层由各个引擎自己实现的。但不是所有的引擎都支持行锁，比如 MyISAM 引擎就不支持行锁\n行锁就是针对数据表中行记录的锁。比如事务 A 更新了一行，而这时候事务 B 也要更新同一行，则必须等事务 A 的操作完成后才能进行更。事务 A 持有的两个记录的行锁都是在 commit 的时候才释放的，事务 B 的 update 语句会被阻塞，直到事务 A 执行 commit 之后，事务 B 才能继续执行\n两阶段锁协议 在 InnoDB 事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。这个就是两阶段锁协议\n如果事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放\n死锁和死锁检测 在并发系统中不同线程出现循环资源依赖，涉及的线程都在等待别的线程释放资源时，就会导致这几个线程都进入无限等待的状态，称为死锁\n当出现死锁以后，有两种策略：\n一种策略是，直接进入等待，直到超时。这个超时时间可以通过参数 innodb_lock_wait_timeout 来设置 另一种策略是，发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数innodb_deadlock_detect 设置为 on，表示开启这个逻辑 在 InnoDB 中，innodb_lock_wait_timeout的默认值是 50s，意味着如果采用第一个策略，当出现死锁以后，第一个被锁住的线程要过 50s 才会超时退出，然后其他线程才有可能继续执行。对于在线服务来说，这个等待时间往往是无法接受的\n正常情况下还是要采用主动死锁检查策略，而且 innodb_deadlock_detect 的默认值本身就是on。主动死锁监测在发生死锁的时候，是能够快速发现并进行处理的，但是它有额外负担的。每当一个事务被锁的时候，就要看看它所依赖的线程有没有被别人锁住，如此循环，最后判断是否出现了循环等待，也就是死锁\n如果所有事务都要更新同一行的场景，每个新来的被堵住的线程都要判断会不会由于自己的加入导致死锁，这是一个时间复杂度是 O(n) 的操作\n怎么解决由这种热点行更新导致的性能问题？\n如果确保这个业务一定不会出现死锁，可以临时把死锁检测关掉 控制并发度 将一行改成逻辑上的多行来减少锁冲突。以影院账户为例，可以考虑放在多条记录上，比如10个记录，影院的账户总额等于这10个记录的值的总和。这样每次要给影院账户加金额的时候，随机选其中一条记录来加。这样每次冲突概率变成员原来的1/10，可以减少锁等待个数，也就减少了死锁检测的CPU消耗 引用 https://blog.csdn.net/qq_40378034/article/details/90904573\n","date":"2022-11-06T00:00:00Z","permalink":"https://ggxxll.github.io/p/%E9%94%81%E4%BA%8C/","title":"锁（二）"},{"content":"概述 锁机制用于管理对共享资源的并发访问。\n锁可大致分为两类：\nlock：对象是事务，用来锁定的是数据库中的对象，如表、页、行。并且一般lock的对象仅在事务 commit 或者 rollback 后进行释放。有死锁检测机制。 latch：闩锁（shuang suo），其要求锁定的时间必须非常短。若持续的时间长，则应用的性能会非常差。在 InnoDB 存储引擎中，latch 又分为 mutex 互斥锁 和 rwLock 读写锁。 其目的是为了保证并发线程操作临界资源的正确性。通常没有死锁的检测机制。 锁的类型 共享锁，排他锁 InnoDB存储引擎实现了如下两种标准的行级锁：\n共享锁（S Lock）：允许事务读取一行数据。\n排他锁（X Lock）：允许事务删除或者更新一行数据。\n这里可以理解为读写锁，可并行读，但是不可以：并行写、并行读写。\n排它锁是很强的锁，不与其他类型的锁兼容。这也很好理解，修改和删除某一行的时候，必须获得强锁，禁止这一行上的其他并发，以保障数据的一致性。\n记录锁 Record Lock，仅锁定一行记录（如共享锁、排他锁）\n记录锁总是会去锁定索引记录，如果表在建立的时候，没有设置任何索引，InnoDB 会使用隐式的主键进行锁定。 查询条件的列是唯一索引的情况下，临键锁退化为记录锁。\n间隙锁 Gap Lock，锁定一个范围，但不包括记录本身。 关闭间隙锁的两种方式：\n将事务隔离级别设置为 读已提交（read committed） 将参数 innodb_locks_unsafe_for_binlog 设置为 1 在上述配置下，除了外键和唯一性检查依然需要间隙锁，其余情况均使用行锁进行锁定。\n临键锁 Next-Key Lock，等于记录锁+间隙锁，锁定一个范围，并锁定记录本身.\n主要是阻止多个事务将记录插入到同一个范围内，从而避免幻读。\n当查询的索引是唯一索引时，InnoDB 会将临键锁优化为记录锁，从而提高并发。这时候，将不再使用间隙锁来避免幻读。\n意向锁 事务可能要加共享/排他锁，先提前声明一个意向。\nInnoDB 支持多粒度锁定，这种锁定允许事务在行级上的锁和表级上的锁同时存在。为了支持不同粒度上进行加锁操作，InnoDB 存储引擎支持一种额外的锁方式， 称之为意向锁。意向锁是将锁定的对象分为多个层次，意向锁意味着事务希望在更粗的粒度上进行加锁。\n特点：\n意向锁是表级别锁 意向锁可分为： 意向共享锁（intention shared lock, IS）：表示事务有意向对表中的某些行加共享锁 意向排它锁（intention exclusive lock, IX）：表示事务有意向对表中的某些行加排它锁 意向锁协议： 事务要获得某些行的共享锁，必须先获得表的意向共享锁 IS 事务要获取某些行的排他锁，必须先获得表的意向排他锁 IX 由于意向锁仅表明意向，所以是比较弱的锁，意向锁之间不互斥，可以并行 意向锁之间可以并行，但与共享锁和排它锁互斥，表现为 S+IS 可并行读，其他均互斥。 插入意向锁 对已有数据行的修改与删除，必须使用排他锁，那对于数据的插入，是否还需要加这么强的锁，来实施互斥呢？插入意向锁，孕育而生。\n插入意向锁，是间隙锁（Gap Locks）的一种（所以，也是实施在索引上的），它是专门针对 INSERT 操作的。\n它的用处是：多个事务，在同一个索引上插入记录时，如果插入的位置不冲突，不会阻塞彼此。\n自增锁 自增锁是 MySQL一种特殊的锁，如果表中存在自增字段，MySQL便会自动维护一个自增锁。\n在 InnoDB 存储引擎的内存结构中，对每个含有自增长值的表都有一个自增长计数器。当对含有自增长计数器的表进行插入操作时，这个计数器会被初始化，执行如下操作来得到计数器的值：\nselect max(auto_inc_col) from t for update\n插入操作会依据这个自增长的计数器值加 1 赋予自增长列。这个实现方式称为 Auto-Inc Locking。这种锁其实是采用一种表锁的机制，为了提高插入的性能，自增长锁不是在一个事务完成以后才释放，而是在完成自增长值插入的SQL后立即释放。\n虽然 Auto-Inc Locking 从一定程度上提高了并发插入的效率，但还是存在一些性能上的问题。对于自增长列的并发插入性能较差，事务必须等待前一个插入的完成（虽然不用等待事务的完成）。\n从 MySQL5.12 版本开始，InnoDB 存储引擎提供了一种轻量级互斥量的自增长实现方式。这种方式大大提高了自增长值插入的性能。并且从该版本开始，InnoDB 存储引起提供了一个参数 innodb_innodb_autoinc_lock_mode 来控制自增长模式，该参数的默认值为 1\n自增长的插入分类:\ninsert like: 指所有的插入语句，如 INSERT、REPLACE、INSERT\u0026hellip;SELECT、RPELACE\u0026hellip;SELECT、LOAD DATA等 simple inserts：指能在插入前就确定插入行数的语句，如 INSERT、REPLACE 等。需要注意的是，不包含 INSERT\u0026hellip;ON DUPLICATE KEY UPDATE 这类语句。 bulk inserts：指在插入前不能确定插入行数的语句，如 INSERT\u0026hellip;SELECT、RPELACE\u0026hellip;SELECT、LOAD DATA等 mixed-mode inserts：指插入中有一部分的值是自增长的，有一部分是确定的。如 INSERT INTO T1 (id,name) VALUES (1,\u0026lsquo;A\u0026rsquo;),( NULL,\u0026lsquo;B\u0026rsquo;)；也可以指 INSERT\u0026hellip;ON DUPLICATE KEY UPDATE 这类语句。 innodb_innodb_autoinc_lock_mode 参数值的说明：\n0： 它提供了一个向后兼容的能力 在这一模式下，所有的 insert 语句(\u0026ldquo;insert like\u0026rdquo;) 都要在语句开始的时候得到一个表级的 Auto-Inc Locking 锁，在语句结束的时候才释放这把锁，注意呀，这里说的是语句级而不是事务级的，一个事务可能包涵有一个或多个语句。 它能保证值分配的可预见性，与连续性，可重复性，这个也就保证了 insert 语句在复制到 slave 的时候还能生成和master那边一样的值(它保证了基于语句复制的安全)。 由于在这种模式下 Auto-Inc Locking 锁一直要保持到语句的结束，所以这个就影响到了并发的插入。 1: 这一模式下去 simple insert 做了优化，由于simple insert一次性插入值的个数可以立马得到 确定，所以mysql可以一次生成几个连续的值，用于这个insert语句；总的来说这个对复制也是安全的(它保证了基于语句复制的安全) 这一模式也是mysql的默认模式，这个模式的好处是 Auto-Inc Locking 锁不要一直保持到语句的结束，只要语句得到了相应的值后就可以提前释放锁 2: 由于这个模式下已经没有了 Auto-Inc Locking 锁，所以这个模式下的性能是最好的；但是它也有一个问题，就是对于同一个语句来说它所得到的 auto_incremant 值可能不是连续的。 InnoDB 存储引擎中自增长的实现和 MyISAM 不同。MyISAM 存储引擎是表锁设计，自增长不用考虑并发插入的问题。在 InnoDB 存储引擎中，自增长值的列必须是索引，同时必须是索引的第一个列，如果不是第一个列，则 MySQL 会抛出异常。MyISAM 存储引擎没有这个问题。\n","date":"2022-11-05T00:00:00Z","permalink":"https://ggxxll.github.io/p/%E9%94%81%E4%B8%80/","title":"锁（一）"},{"content":"概述 ACID，是指数据库管理系统（DBMS）在写入或更新资料的过程中，为保证事务（transaction）是正确可靠的，所必须具备的四个特性： 原子性（atomicity [ˌætəˈmɪsəti]， 或称不可分割性）、一致性（consistency [kənˈsɪstənsi]）、 隔离性（isolation [ˌaɪsəˈleɪʃn]，又称独立性）、持久性（durability [ˌdjʊərəˈbɪlɪti]）。\n在数据库系统中，一个事务是指：由一系列数据库操作组成的一个完整的逻辑过程。例如银行转帐，从原账户扣除金额，以及向目标账户添加金额，这两个数据库操作的总和，构成一个完整的逻辑过程，不可拆分。这个过程被称为一个事务，具有ACID特性。\n特性 Atomicity（原子性）：一个事务（transaction）中的所有操作，要么全部完成，要么全部不完成，不会结束在中间某个环节。事务在执行过程中发生错误，会被恢复（Rollback）到事务开始前的状态，就像这个事务从来没有执行过一样。 Consistency（一致性）：在事务开始之前和事务结束以后，数据库的完整性没有被破坏。这表示写入的资料必须完全符合所有的预设规则，这包含资料的精确度、串联性以及后续数据库可以自发性地完成预定的工作。 Isolation（隔离性）：数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。事务隔离分为不同级别，包括读未提交（Read uncommitted）、读提交（read committed）、可重复读（repeatable read）和串行化（Serializable）。 Durability（持久性）：事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。 事务并发时可能出现的问题 脏读（Dirty Read） A 事务读到了 B 未提交事务修改过的数据\n不可重复读（Non-Repeatable Read） A 事务只能读到 B 已经提交的事务修改过的数据，并且其他事务每对该数据进行一次修改并提交后，A 事务都能查询得到最新值。 （不可重复读在读未提交和读已提交隔离级别都可能会出现）\n幻读（Phantom） A 事务先根据某些条件查询出一些记录，之后 B 事务又向表中插入了符合这些条件的记录，A 事务再次按照该条件查询时，能把 B 事务插入的记录也读出来。 （幻读在读未提交、读已提交、可重复读隔离级别都可能会出现）\nMysql事务的隔离级别 MySQL的事务隔离级别一共有四个，分别是读未提交、读已提交、可重复读以及可串行化。\nMySQL的隔离级别的作用就是让事务之间互相隔离，互不影响，这样可以保证事务的一致性。\n隔离级别比较：可串行化 \u0026gt; 可重复读 \u0026gt; 读已提交 \u0026gt; 读未提交\n隔离级别对性能的影响比较：可串行化 \u0026gt; 可重复读 \u0026gt; 读已提交 \u0026gt; 读未提交\n由此看出，隔离级别越高，所需要消耗的MySQL性能越大（如事务并发严重性），为了平衡二者，一般建议设置的隔离级别为可重复读，MySQL Innodb默认的隔离级别也是可重复读。\n读未提交（READ UNCOMMITTED） 在读未提交隔离级别下，事务 A 可以读取到事务 B 修改过但未提交的数据。\n可能发生脏读、不可重复读和幻读问题，一般很少使用此隔离级别。\n读已提交（READ COMMITTED） 在读已提交隔离级别下，事务 A 只能在事务 B 修改过并且已提交后才能读取到事务 B 修改的数据。\n读已提交隔离级别解决了脏读的问题，但可能发生不可重复读和幻读问题，一般很少使用此隔离级别。\n普通读是快照读； 加锁的select, update, delete等语句，除了在外键约束检查(foreign-key constraint checking)以及重复键检查(duplicate-key checking)时会封锁区间，其他时刻都只使用记录锁； 可重复读（REPEATABLE READ） 在可重复读隔离级别下，事务 A 只能在事务 B 修改过数据并提交后，A 也提交事务后，才能读取到事务 B 修改的数据。\n可重复读隔离级别解决了脏读和不可重复读的问题，但可能发生幻读问题。\n提问：为什么上了写锁（写操作），别的事务还可以读操作？\n因为InnoDB有MVCC机制（多版本并发控制），可以使用快照读，而不会被阻塞。\n普通的select使用快照读(snapshot read)，这是一种不加锁的一致性读(Consistent Nonlocking Read)，底层使用MVCC来实现，具体的原理在《InnoDB并发如此高，原因竟然在这？》中有详细的描述； 加锁的select(select ... in share mode | select ... for update), update, delete等语句，它们的锁，依赖于它们是否在唯一索引(unique index)上使用了唯一的查询条件(unique search condition)，或者范围查询条件(range-type search condition)： 在唯一索引上使用唯一的查询条件，会使用记录锁(record lock)，而不会封锁记录之间的间隔，即不会使用间隙锁(gap lock)与临键锁(next-key lock) 范围查询条件，会使用间隙锁与临键锁，锁住索引记录之间的范围，避免范围间插入记录，以避免产生幻影行记录，以及避免不可重复的读 可串行化（SERIALIZABLE） 各种问题（脏读、不可重复读、幻读）都不会发生，通过加锁实现（读锁和写锁）。 事务并发执行时，阻塞情况与读写锁相同，可同时读，不可以读写交叉。\n所有select语句都会被隐式的转化为 select ... in share mode.\n隔离级别比较 级别 脏读 不可重复读 幻读 读未提交 可能 可能 可能 读已提交 不会 可能 可能 可重复读 不会 不会 可能 串行读 不会 不会 不会 隔离级别的实现原理 使用MySQL的默认隔离级别（可重复读）来进行说明。\n每条记录在更新的时候都会同时记录一条回滚操作（回滚操作日志undo log）。同一条记录在系统中可以存在多个版本，这就是数据库的多版本并发控制（MVCC）。 即通过回滚（rollback操作），可以回到前一个状态的值。\n提问：回滚操作日志（undo log）什么时候删除？\nMySQL会判断当没有事务需要用到这些回滚日志的时候，回滚日志会被删除。\n提问：什么时候不需要了？\n当系统里么有比这个回滚日志更早的read-view的时候。\n相关操作 查看隔离级别 SHOW VARIABLES LIKE 'transaction_isolation'; SELECT @@transaction_isolation; 设置隔离级别 SET 命令 SET [GLOBAL|SESSION] TRANSACTION ISOLATION LEVEL level;\n1 2 3 4 5 6 7 其中level有4种值： level: { REPEATABLE READ | READ COMMITTED | READ UNCOMMITTED | SERIALIZABLE } 1.　GLOBAL\n只对执行完该语句之后产生的会话起作用 当前已经存在的会话无效 2.　SESSION\n只对当前会话中下一个即将开启的事务有效 下一个事务执行完后，后续事务将恢复到之前的隔离级别 该语句不能在已经开启的事务中间执行，会报错的 服务启动项命令 可以修改启动参数transaction-isolation的值\n比方说我们在启动服务器时指定了\u0026ndash;transaction-isolation=READ UNCOMMITTED，那么事务的默认隔离级别就从原来的REPEATABLE READ变成了READ UNCOMMITTED。\n","date":"2022-10-23T00:00:00Z","permalink":"https://ggxxll.github.io/p/acid/","title":"ACID"},{"content":"问题 order by 和 limit 造成优化器选择索引错误\n测试表 1 2 CREATE DATABASE `app`; USE app; 1 2 3 4 5 6 7 8 9 CREATE TABLE `users` ( `id` int NOT NULL AUTO_INCREMENT, `name` varchar(100) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci DEFAULT NULL, `age` int DEFAULT NULL, `created_at` timestamp NOT NULL, `deleted_at` timestamp NULL DEFAULT NULL, PRIMARY KEY (`id`), KEY `users_age_IDX` (`age`) USING BTREE, ) ENGINE=InnoDB AUTO_INCREMENT=0 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci; 填充测试数据 1 2 3 4 5 6 7 8 9 10 11 12 13 DROP PROCEDURE IF EXISTS addData; DELIMITER $$ $$ CREATE PROCEDURE addData(IN n int) BEGIN DECLARE i int default 1; while (i \u0026lt; n) DO INSERT INTO users(name, age, created_at, deleted_at) VALUES(CONCAT(\u0026#39;name\u0026#39;,i), i, now(), null); set i=i | 1; End while; END$$ DELIMITER ; 1 CALL addData(400) 查询 sql 以下查询中使用的索引正确使用了 users_age_IDX。 1 2 explain SELECT * from users u where age \u0026gt; 1000 limit 1; explain SELECT * from users u where age \u0026gt; 1000 order by id desc limit 2; id select_type table partitions type possible_keys key key_len ref rows filtered Extra 1 SIMPLE u range users_age_IDX,users_deleted_at_IDX users_age_IDX 5 1988 100.0 Using index condition; Using where 这个查询中使用的索引是主键, 数据量 1 explain SELECT * from users u where age \u0026gt; 100 order by id desc limit 1; id select_type table partitions type possible_keys key key_len ref rows filtered Extra 1 SIMPLE u index users_age_IDX,users_deleted_at_IDX PRIMARY 4 65 15.33 Using where; Backward index scan 原因 MySQL 优化器认为在 limit 值较小的情况下，走主键索引能够更快的找到那一条数据，并且如果走联合索引需要扫描索引后进行排序，而主键索引天生有序，所以优化器综合考虑，走了主键索引。\n","date":"2022-09-19T00:00:00Z","permalink":"https://ggxxll.github.io/p/mysql-%E7%9A%84-order-by-%E5%92%8C-limit-%E8%AF%AD%E5%8F%A5%E5%AF%BC%E8%87%B4%E6%85%A2-sql/","title":"mysql 的 order by 和 limit 语句导致慢 sql"},{"content":"基础知识 map的概念 map的直观意思是映射，是\u0026lt;key, value\u0026gt; 对组成的抽象结构，且 key 不会重复。map 的底层实现一般有两种：\n搜索树（search tree），天然有序，平均/最差的写入/查找复杂度是O(logN) 哈希表（hash table），无序存储，平均的写入/查找复杂度是O(1)，最差是O(N) 底层 源码 在 golang 中，map 的底层实现是哈希表：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 const ( bucketCntBits = 3 bucketCnt = 1 \u0026lt;\u0026lt; bucketCntBits ) type hmap struct { count int // map中存入元素的个数， golang 中调用 len(map) 的时候直接返回该字段 flags uint8 // 状态标记位，通过与定义的枚举值进行\u0026amp;操作可以判断当前是否处于这种状态 B uint8 // 2^B 表示 bucket 的数量， B 表示取hash后多少位来做 bucket 的分组 noverflow uint16 // overflow bucket 的数量的近似数 hash0 uint32 // hash seed （hash 种子） 一般是一个素数 buckets unsafe.Pointer // 共有2^B个 bucket，但是如果没有元素存入，这个字段可能为nil oldbuckets unsafe.Pointer // 在扩容期间，将旧的 bucket 数组放在这里， 新 bucket 会是这个的两倍大; 非扩容状态，值为 nil nevacuate uintptr // 表示已经完成扩容迁移的bucket的指针， 地址小于当前指针的 bucket 已经迁移完成 extra *mapextra // 为了优化GC扫描而设计的。当 key 和 value 均不包含指针，并且都可以inline时使用。extra是指向 mapextra 类型的指针 } // A bucket for a Go map. type bmap struct { tophash [bucketCnt]uint8 } // 编译期间会给它加料，动态地创建一个新的结构： type bmap struct { // 长度为8的数组，用来快速定位 key 是否在桶里 topbits [8]uint8 keys [8]keytype values [8]valuetype pad uintptr // 内存对齐使用，可能不需要 overflow uintptr // 当 bucket 的8个key 存满了之后 } hmap map 的底层结构是 hmap，hmap 包含若干个结构为 bmap 的 bucket 数组，每个 bucket 底层都采用链表结构。\nbmap bmap 就是我们常说的“桶”的底层数据结构， 一个桶中可以存放最多 8 个 key/value, map 使用 hash 函数 得到 hash 值决定分配到哪个桶， 然后又会根据 hash 值的高 8 位来寻找放在桶的哪个位置。具体的 map 的组成结构如下图所示：\nbmap 中 key 和 value 是各自放在一起的，并不是 key/value/key/value/\u0026hellip; 这样的形式。源码里说明这样的好处是在某些情况下可以省略掉 padding 字段，节省内存空间。\nmapextra 1 2 3 4 5 6 7 8 9 10 11 12 13 14 // mapextra holds fields that are not present on all maps. type mapextra struct { // 如果 key 和 value 都不包含指针，并且可以被 inline(\u0026lt;=128 字节) // 就使用 hmap 的 extra 字段 来存储 overflow buckets，这样可以避免 GC 扫描整个 map // 然而 bmap.overflow 也是个指针。这时候我们只能把这些 overflow 的指针 // 都放在 hmap.extra.overflow 和 hmap.extra.oldoverflow 中了 // overflow 包含的是 hmap.buckets 的 overflow 的 buckets // oldoverflow 包含扩容时的 hmap.oldbuckets 的 overflow 的 bucket overflow *[]*bmap oldoverflow *[]*bmap // 指向空闲的 overflow bucket 的指针 nextOverflow *bmap } 特性 引用类型 map是个指针，底层指向hmap，所以是个引用类型。 golang 有三个常用的高级类型slice、map、channel，它们都是引用类型，当引用类型作为函数参数时，可能会修改原内容数据。 golang 中没有引用传递，只有值和指针传递。所以 map 作为函数实参传递时本质上也是值传递，只不过因为 map 底层数据结构是通过指针指向实际的元素存储空间， 在被调函数中修改 map，对调用者同样可见，所以 map 作为函数实参传递时表现出了引用传递的效果。 因此，传递 map 时，如果想修改map的内容而不是map本身，函数形参无需使用指针。\n1 2 3 4 5 6 7 8 9 10 11 12 func TestSliceFn(t *testing.T) { m := map[string]int{} t.Log(m, len(m)) // map[a:1] mapAppend(m, \u0026#34;b\u0026#34;, 2) t.Log(m, len(m)) // map[a:1 b:2] 2 } func mapAppend(m map[string]int, key string, val int) { m[key] = val } 非线程安全 map默认是并发不安全的，原因如下：\nGo 官方在经过了长时间的讨论后，认为 Go map 更应适配典型使用场景（不需要从多个 goroutine 中进行安全访问），而不是为了小部分情况（并发访问），导致大部分程序付出加锁代价（性能），决定了不支持。\n更改 map 时会检查 hmap 的标志位 flags。如果 flags 的写标志位此时被置 1 了，说明有其他协程在执行“写”操作，进而导致程序 panic。这也说明了 map 对协程是不安全的。\n共享存储空间 map 底层数据结构是通过指针指向实际的元素存储空间 ，这种情况下，对其中一个map的更改，会影响到其他map\n1 2 3 4 5 6 7 8 9 func TestMapShareMemory(t *testing.T) { m1 := map[string]int{} m2 := m1 m1[\u0026#34;a\u0026#34;] = 1 t.Log(m1, len(m1)) // map[a:1] 1 t.Log(m2, len(m2)) // map[a:1] } 哈希冲突 golang中map是一个kv对集合。底层使用hash table，用链表来解决冲突 ，出现冲突时，不是每一个key都申请一个结构通过链表串起来， 而是以bmap为最小粒度挂载，一个bmap可以放8个kv。在哈希函数的选择上，会在程序启动时，检测 cpu 是否支持 aes，如果支持，则使用 aes hash，否则使用 memhash。\n遍历无序 map 在没有被修改的情况下，使用 range 多次遍历 map 时输出的 key 和 value 的顺序可能不同。这是 Go 语言的设计者们有意为之， 在每次 range 时的顺序被随机化，旨在提示开发者们，Go 底层实现并不保证 map 遍历顺序稳定，请大家不要依赖 range 遍历结果顺序。\n扩容 扩容条件 再来说触发 map 扩容的时机：在向 map 插入新 key 的时候，会进行条件检测，符合下面这 2 个条件，就会触发扩容：\n1 2 3 4 5 6 // If we hit the max load factor or we have too many overflow buckets, // and we\u0026#39;re not already in the middle of growing, start growing. if !h.growing() \u0026amp;\u0026amp; (overLoadFactor(h.count+1, h.B) || tooManyOverflowBuckets(h.noverflow, h.B)) { hashGrow(t, h) goto again // Growing the table invalidates everything, so try again } 装载因子超过阈值\n源码里定义的阈值是 6.5 (loadFactorNum/loadFactorDen)，是经过测试后取出的一个比较合理的因子。 我们知道，每个 bucket 有 8 个空位，在没有溢出，且所有的桶都装满了的情况下，装载因子算出来的结果是 8。因此当装载因子超过 6.5 时， 表明很多 bucket 都快要装满了，查找效率和插入效率都变低了。在这个时候进行扩容是有必要的。\n对于条件 1，元素太多，而 bucket 数量太少，很简单：将 B 加 1，bucket 最大数量(2^B)直接变成原来 bucket 数量的 2 倍。于是，就有新老 bucket 了。\n注意，这时候元素都在老 bucket 里，还没迁移到新的 bucket 来。新 bucket 只是最大数量变为原来最大数量的 2 倍(2^B * 2) 。\noverflow 的 bucket 数量过多\n在装载因子比较小的情况下，这时候 map 的查找和插入效率也很低，而第 1 点识别不出来这种情况。表面现象就是计算装载因子的分子比较小， 即 map 里元素总数少，但是 bucket 数量多（真实分配的 bucket 数量多，包括大量的 overflow bucket）。\n不难想像造成这种情况的原因：不停地插入、删除元素。先插入很多元素，导致创建了很多 bucket，但是装载因子达不到第 1 点的临界值，未触发扩容来缓解这种情况。 之后，删除元素降低元素总数量，再插入很多元素，导致创建很多的 overflow bucket，但就是不会触发第 1 点的规定，你能拿我怎么办？ overflow bucket 数量太多，导致 key 会很分散，查找插入效率低得吓人，因此出台第 2 点规定。\n这就像是一座空城，房子很多，但是住户很少，都分散了，找起人来很困难。\n对于条件 2，其实元素没那么多，但是 overflow bucket 数特别多，说明很多 bucket 都没装满。解决办法就是开辟一个新 bucket 空间， 将老 bucket 中的元素移动到新 bucket，使得同一个 bucket 中的 key 排列地更紧密。这样，原来，在 overflow bucket 中的 key 可以移动到 bucket 中来。 结果是节省空间，提高 bucket 利用率，map 的查找和插入效率自然就会提升。\n扩容函数 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 func hashGrow(t *maptype, h *hmap) { // If we\u0026#39;ve hit the load factor, get bigger. // Otherwise, there are too many overflow buckets, // so keep the same number of buckets and \u0026#34;grow\u0026#34; laterally. bigger := uint8(1) if !overLoadFactor(h.count+1, h.B) { bigger = 0 h.flags |= sameSizeGrow } oldbuckets := h.buckets newbuckets, nextOverflow := makeBucketArray(t, h.B+bigger, nil) flags := h.flags \u0026amp;^ (iterator | oldIterator) if h.flags\u0026amp;iterator != 0 { flags |= oldIterator } // commit the grow (atomic wrt gc) h.B += bigger h.flags = flags h.oldbuckets = oldbuckets h.buckets = newbuckets h.nevacuate = 0 h.noverflow = 0 if h.extra != nil \u0026amp;\u0026amp; h.extra.overflow != nil { // Promote current overflow buckets to the old generation. if h.extra.oldoverflow != nil { throw(\u0026#34;oldoverflow is not nil\u0026#34;) } h.extra.oldoverflow = h.extra.overflow h.extra.overflow = nil } if nextOverflow != nil { if h.extra == nil { h.extra = new(mapextra) } h.extra.nextOverflow = nextOverflow } // the actual copying of the hash table data is done incrementally // by growWork() and evacuate(). } 由于 map 扩容需要将原有的 key/value 重新搬迁到新的内存地址，如果有大量的 key/value 需要搬迁，会非常影响性能。因此 Go map 的扩容采取了一种称为“渐进式”的方式，原有的 key 并不会一次性搬迁完毕，每次最多只会搬迁 2 个 bucket。\n上面说的 hashGrow() 函数实际上并没有真正地“搬迁”，它只是分配好了新的 buckets，并将老的 buckets 挂到了 oldbuckets 字段上。真正搬迁 buckets 的动作在 growWork() 函数中，而调用 growWork() 函数的动作是在 mapassign 和 mapdelete 函数中。也就是插入或修改、删除 key 的时候，都会尝试进行搬迁 buckets 的工作。先检查 oldbuckets 是否搬迁完毕，具体来说就是检查 oldbuckets 是否为 nil。\n迁移 1 2 3 4 5 6 7 8 9 10 func growWork(t *maptype, h *hmap, bucket uintptr) { // make sure we evacuate the oldbucket corresponding // to the bucket we\u0026#39;re about to use evacuate(t, h, bucket\u0026amp;h.oldbucketmask()) // evacuate one more oldbucket to make progress on growing if h.growing() { evacuate(t, h, h.nevacuate) } } 迁移条件 如果未迁移完毕，赋值/删除的时候，扩容完毕后（预分配内存），不会马上就进行迁移。而是采取增量扩容的方式，当有访问到具体 bukcet 时，才会逐渐的进行迁移（将 oldbucket 迁移到 bucket）\n迁移函数 hmap.nevacuate 标识的是当前的进度，如果都搬迁完，应该和2^B的长度是一样的 在evacuate 方法实现是把这个位置对应的 bucket，以及其冲突链上的数据都转移到新的 bucket 上。\n先要判断当前 bucket 是不是已经转移。 (oldbucket 标识需要搬迁的 bucket 对应的位置)。转移的判断直接通过tophash 就可以，判断 tophash 中第一个 hash 值即可\n1 2 3 4 5 6 7 8 9 10 var ( emptyOne = 1 // this cell is empty minTopHash = 5 // minimum tophash for a normal filled cell. ) func evacuated(b *bmap) bool { h := b.tophash[0] return h \u0026gt; emptyOne \u0026amp;\u0026amp; h \u0026lt; minTopHash } 如果没有被转移，那就要迁移数据了。数据迁移时，可能是迁移到大小相同的 buckets 上，也可能迁移到2倍大的 buckets 上。这里 xy 都是标记目标迁移位置的标记：x 标识的是迁移到相同的位置，y 标识的是迁移到2倍大的位置上。我们先看下目标位置的确定：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 var xy [2]evacDst x := \u0026amp;xy[0] x.b = (*bmap)(add(h.buckets, oldbucket*uintptr(t.bucketsize))) x.k = add(unsafe.Pointer(x.b), dataOffset) x.e = add(x.k, bucketCnt*uintptr(t.keysize)) if !h.sameSizeGrow() { // Only calculate y pointers if we\u0026#39;re growing bigger. // Otherwise GC can see bad pointers. y := \u0026amp;xy[1] y.b = (*bmap)(add(h.buckets, (oldbucket+newbit)*uintptr(t.bucketsize))) y.k = add(unsafe.Pointer(y.b), dataOffset) y.e = add(y.k, bucketCnt*uintptr(t.keysize)) } 确定bucket位置后，需要按照kv 一条一条做迁移。\n如果当前搬迁的bucket 和 总体搬迁的bucket的位置是一样的，我们需要更新总体进度的标记 nevacuate\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 func advanceEvacuationMark(h *hmap, t *maptype, newbit uintptr) { h.nevacuate++ // Experiments suggest that 1024 is overkill by at least an order of magnitude. // Put it in there as a safeguard anyway, to ensure O(1) behavior. stop := h.nevacuate + 1024 if stop \u0026gt; newbit { stop = newbit } for h.nevacuate != stop \u0026amp;\u0026amp; bucketEvacuated(t, h, h.nevacuate) { h.nevacuate++ } if h.nevacuate == newbit { // newbit == # of oldbuckets // Growing is all done. Free old main bucket array. h.oldbuckets = nil // Can discard old overflow buckets as well. // If they are still referenced by an iterator, // then the iterator holds a pointers to the slice. if h.extra != nil { h.extra.oldoverflow = nil } h.flags \u0026amp;^= sameSizeGrow } } 总结 map是引用类型 map遍历是无序的 map是非线程安全的 map的哈希冲突解决方式是链表法 map的扩容不是一定会新增空间，也有可能是只是做了内存整理 map的迁移是逐步进行的，在每次赋值时，会做至少一次迁移工作 map中删除key，有可能导致出现很多空的kv，这会导致迁移操作，如果可以避免，尽量避免 ","date":"2022-08-25T00:00:00Z","permalink":"https://ggxxll.github.io/p/map-%E6%8E%A2%E7%A9%B6/","title":"Map 探究"},{"content":"概述 https://zhuanlan.zhihu.com/p/98135840\n链接 文档 在线体验 数据库引擎 Lazy\n会将最近一次查询的表保存在内存中, 过期时间为expiration_time_in_seconds. 仅适用于*Log表引擎. 如果表的数量较多, 数据量小, 且访问间隔长, 建议使用Lazy引擎数据库. Atomic\n不指定引擎时, 默认使用Atomic. 支持无阻塞删除和重命名表. 默认会生成UUID, 不建议指定UUID 不改变uuid和迁移表数据的话, 执行rename table可以立即生效 DROP/DETACH TABLE不会真正的删除表, 只是标记被删除了. 如果同步模式设置为SYNC, 在等待已有的查询/插入操作结束之后, 表会被立即删除. 使用表引擎ReplicatedMergeTree, 不建议指定path和replica name ReplicatedReplacingMergeTree('/clickhouse/tables/{shard}/{database}/{table}', '{replica}') Mysql\n远程映射Mysql的库表. 在clickhouse执行的查询等, 会被引擎转换, 发送到Mysql服务中. 不能执行 RENAME, CREATE TABLE, ALTER 简单 WHERE 条款如 =, !=, \u0026gt;, \u0026gt;=, \u0026lt;, \u0026lt;= 当前在MySQL服务器上执行。其余的条件和 LIMIT 只有在对MySQL的查询完成后，才会在ClickHouse中执行采样约束。 删除DROP和卸载DETACH 表，不会删除mysql中的表，且允许通过ATTACH再装载回来 不支持的Mysql的数据类型会转换成String, 所有类型都支持Nullable MaterializeMySQL\n使用MySQL中存在的所有表以及这些表中的所有数据创建ClickHouse数据库（即库级别的数据同步）。ClickHouse服务器用作MySQL副本。它读取binlog并执行DDL和DML查询。默认表引擎设置为ReplacingMergeTree\n使用表引擎 ReplacingMergeTree 时, 会添加虚拟列 _sign 和 _version\n_version — Transaction counter. Type [UInt64]\n_sign — Deletion mark. Type [Int8]. Possible values:\n1 — Row is not deleted, -1 — Row is deleted. MySQL DDL查询将转换为相应的ClickHouse DDL查询（ALTER，CREATE，DROP，RENAME）。如果ClickHouse无法解析某些DDL查询，则该查询将被忽略。\n不支持 INSERT, DELETE 和 UPDATE操作. 复制时都视为INSERT操作, 会在数据上使用_sign标记.\nINSERT, 1 DELETE, -1 UPDATE, 1或-1 执行SELECT查询时\n不指定_version, 默认使用FINAL, 即MAX(_version) 不指定_sign, 默认会限制查询条件WHERE _sign=1 注意事项:\n_sign=-1的数据是逻辑删除 不支持update/delete 复制过程很容易崩溃 禁止手动操作数据库和表 通过设置optimize_on_insert, 启用或禁用在插入之前进行数据转换，就像合并是在此块上完成的（根据表引擎）一样。 不支持的Mysql数据类型会异常Unhandled data type并停止复制， 都支持Nullable PostgreSQL 远程链接PostgreSQL. 支持查询, 插入等操作. 支持修改表结构 (注意使用缓存时的情况: DETACH 和ATTCH可刷新缓存). 数据类型中仅INTEGER支持Nullable 表引擎 文档 MergeTree Kakfa，Mysql 创建数据库 1 CREATE DATABASE [IF NOT EXISTS] db_name [ON CLUSTER cluster] [ENGINE = engine(...)] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster] ( name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1] [COMMENT expr1] [TTL expr1], name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2] [COMMENT expr1] [TTL expr2], ... INDEX index_name1 expr1 TYPE type1(...) GRANULARITY value1, INDEX index_name2 expr2 TYPE type2(...) GRANULARITY value2 ) ENGINE = MergeTree() -- 指定数据排序使用字段，默认等同主键 ORDER BY (expr1[, expr2,...]) -- 指定数据分区使用的字段 [PARTITION BY (expr1[, expr2,...])] -- 指定主键 [PRIMARY KEY (expr1[, expr2,...])] -- 指定采样数据使用的字段，启用数据采样时，不会对所有数据执行查询，而只对特定部分数据（样本）执行查询 [SAMPLE BY (expr1[, expr2,...])] [TTL expr [DELETE|TO DISK \u0026#39;xxx\u0026#39;|TO VOLUME \u0026#39;xxx\u0026#39; [, ...] ] [WHERE conditions] [GROUP BY key_expr [SET v1 = aggr_func(v1) [, v2 = aggr_func(v2) ...]] ] ] [SETTINGS name=value, ...] 字典 字典是一个映射 (键 -\u0026gt; 属性）, 是方便各种类型的参考清单。\nClickHouse支持一些特殊函数配合字典在查询中使用。 将字典与函数结合使用比将 JOIN 操作与引用表结合使用更简单、更有效。\n内置字典：处理地理数据库 外部字典：自定义字典 特殊查询 1 2 3 4 5 6 7 8 9 10 11 -- 计算pv值\u0026gt;20的记录条数 select sum(pv\u0026gt;20) from test_all; select count(*) from test_all where pv\u0026gt;20; -- 计算pv\u0026gt;20的记录条数占比：0.5 select avg(pv\u0026gt;20) from test_all; -- 计算pv在1,2，20这几个值中的记录条数的占比 select avg(pv in (1,2,2,20)) from test_all; -- Date系列类型， 插入时可以混合时间戳和字符串插入 INSERT INTO dt Values (1546300800, 1), (\u0026#39;2019-01-01\u0026#39;, 2); 数据类型 UUID FixedString Enum LowCardinality(T), 把其它数据类型转变为字典编码类型。如果该字段的值去重个数小于1W，性能优于普通类型。反之，性能下降。 通常， 一个字符串类型的Enum类型， 建议设置为LowCardinality， 更灵活和高效 Array(T\u0026hellip;) AggregateFunction， 适用于表引擎 AggregatingMergeTree Nested 嵌套字段， 不支持表引擎 MergeTree Tuple Nullable Domains： IPV4， IPV6 Geo： experimental feature Point ： 坐标，Tuple（x float64, y float64） Ring： Array(Point) Polygon： Array(Ring) MultiPolygon：Array(Polygon) Map(key,value) experimental feature SimpleAggregateFunction 性能优于 AggregateFunction， 适用于表引擎 AggregatingMergeTree 注意事项 使用 Nullable 几乎总是对性能产生负面影响，在设计数据库时请记住这一点, 最好设置default值。 删除表的时候， 会先在元数据标记被删除，exists、select、insert等会立即生效（表不存在）， 但是create table创建表等操作会抛异常（表存在）。 稍等几分钟，再新建表即可。 删除表数据，应当操作本地表，操作分布式Distributed表会报错。 ReplacingMergeTree表引擎， 完全相同的数据会直接去重， 数据是在后台不确定的时间去合并去重的。不建议使用OPTIMIZE发起合并， 使用 FINAL 关键字进行查询，可以得到主键相同的、最新被插入的数据。 那些有相同分区表达式值PARTITION BY的数据片段才会合并。这意味着 你不应该用太精细的分区方案（超过一千个分区）。否则，会因为文件系统中的文件数量过多和需要打开的文件描述符过多，导致 SELECT 查询效率不佳。可以根据group by语句来分析如何分区。绝对不能使用带有主键性质的字段做分区（比如唯一id）， 最好选择一个重复率高的字段（比如 日期， 渠道等）。 Alter语句DROP, MODIFY 时， 该字段不能是主键，会报错 使用 1 2 3 4 5 6 7 8 9 10 create table test on cluster cluster1 ( date Date, id UInt32 default 0 comment \u0026#39;123\u0026#39;, name String default \u0026#39;\u0026#39;, pv UInt32 default 0 ) ENGINE = ReplicatedReplacingMergeTree(\u0026#39;/clickhouse/tables/{shard}/{database}/{table}\u0026#39;, \u0026#39;{replica}\u0026#39;) PARTITION BY date ORDER BY (date,id); 1 2 3 4 5 6 7 8 create table test_all on cluster cluster1 ( date Date, id UInt32 default 0 comment \u0026#39;123\u0026#39;, name String default \u0026#39;\u0026#39;, pv UInt32 default 0 ) engine = Distributed(\u0026#39;cluster1\u0026#39;, \u0026#39;wangzhuan\u0026#39;, \u0026#39;test\u0026#39;, sipHash64(date)); 1 2 3 4 -- 结果是一条数据 insert into test_all (date, id, name, pv) VALUES (\u0026#39;2021-05-20\u0026#39;,1,\u0026#39;a\u0026#39;,10), (\u0026#39;2021-05-20\u0026#39;,1,\u0026#39;a\u0026#39;,10); 1 2 3 4 5 6 7 8 9 10 11 12 13 -- 结果是两条数据 insert into test_all (date, id, name, pv) VALUES (\u0026#39;2021-05-20\u0026#39;, 1, \u0026#39;a\u0026#39;, 10) , (\u0026#39;2021-05-20\u0026#39;, 1, \u0026#39;a\u0026#39;, 20); -- 结果是两条数据 30被20覆盖了， insert into test_all (date, id, name, pv) VALUES (\u0026#39;2021-05-20\u0026#39;, 1, \u0026#39;a\u0026#39;, 30) , (\u0026#39;2021-05-20\u0026#39;, 1, \u0026#39;a\u0026#39;, 20); -- 当然， 经过后台的数据合并之后， 应该是只剩1条数据 pv=30的 1 2 3 4 5 6 7 select * from test_all; select * from test_all final; select sum(pv) from test_all; select sum(pv) from test_all final; 1 2 3 alter table test on cluster cluster1 delete where 1; alter table test on cluster cluster1 delete where date=\u0026#39;2021-05-20\u0026#39;; 1 2 3 4 5 6 7 8 9 10 11 12 -- 仅当分区被使用到（PARTITION BY的字段的值超过一个），system.parts表里才会有数据 SELECT partition, name, active FROM system.parts WHERE table = \u0026#39;test\u0026#39;; SELECT table,count(*) FROM system.parts group by table order by count(*) desc; 1 2 drop table test on cluster cluster1; drop table test_all on cluster cluster1; ","date":"2022-08-24T00:00:00Z","permalink":"https://ggxxll.github.io/p/clickhouse%E5%88%9D%E4%BD%93%E9%AA%8C/","title":"Clickhouse初体验"},{"content":"环境 工具：docker，docker-compose 需求描述 在本地搭建业务中可能使用的服务，方便本地进行测试。\n配置文件 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 version: \u0026#34;3\u0026#34; services: zookeeper: container_name: zookeeper image: \u0026#39;bitnami/zookeeper:latest\u0026#39; ports: - \u0026#39;2181:2181\u0026#39; environment: - ALLOW_ANONYMOUS_LOGIN=yes kafka: container_name: kafka image: \u0026#39;bitnami/kafka:latest\u0026#39; ports: - \u0026#39;9092:9092\u0026#39; environment: - KAFKA_BROKER_ID=1 - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092 - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://127.0.0.1:9092 - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181 - ALLOW_PLAINTEXT_LISTENER=yes depends_on: - zookeeper volumes: - \u0026#39;kafka:/bitnami/kafka\u0026#39; redis: container_name: redis image: \u0026#39;redis:latest\u0026#39; ports: - \u0026#39;6379:6379\u0026#39; volumes: - \u0026#39;redis:/usr/local/etc/redis\u0026#39; mysql: container_name: mysql image: \u0026#39;mysql:latest\u0026#39; ports: - \u0026#39;3306:3306\u0026#39; environment: MYSQL_ALLOW_EMPTY_PASSWORD: \u0026#39;yes\u0026#39; MYSQL_DATABASE: app volumes: - \u0026#39;mysql:/var/lib/mysql\u0026#39; mongo: container_name: mongo image: \u0026#39;mongo:latest\u0026#39; ports: - \u0026#39;27017:27017\u0026#39; volumes: - \u0026#39;mongo:/data/db\u0026#39; etcd: container_name: etcd image: \u0026#39;quay.io/coreos/etcd:latest\u0026#39; ports: - \u0026#39;2379:2379\u0026#39; - \u0026#39;2380:2380\u0026#39; volumes: - \u0026#39;etcd:/etcd-data\u0026#39; command: - /usr/local/bin/etcd - --data-dir=/etcd-data - --name=node1 - --initial-advertise-peer-urls=http://192.168.82.116:2380 - --listen-peer-urls=http://0.0.0.0:2380 - --advertise-client-urls=http://192.168.82.116:2379 - --listen-client-urls=http://0.0.0.0:2379 - --initial-cluster - node1=http://192.168.82.116:2380 es: container_name: es image: docker.elastic.co/elasticsearch/elasticsearch:7.14.0 environment: - discovery.type=single-node ulimits: memlock: soft: -1 hard: -1 volumes: - es:/usr/share/elasticsearch/data ports: - \u0026#39;9200:9200\u0026#39; volumes: redis: {} mysql: {} kafka: {} es: {} mongo: {} etcd: {} ","date":"2022-08-24T00:00:00Z","permalink":"https://ggxxll.github.io/p/docker%E6%B5%8B%E8%AF%95%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/","title":"docker测试环境搭建"},{"content":"环境 系统: windows 语言: golang 框架: gin Bug描述 在如下代码中, ctx := c和ctx := context.Background()传递到cli.Set中是没有问题的.\n使用ctx := c.Request.Context()时, 会打印出错误: context canceled.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;github.com/gin-gonic/gin\u0026#34; \u0026#34;github.com/go-redis/redis/v8\u0026#34; ) func main() { cli := redis.NewClient(\u0026amp;redis.Options{ Addr: \u0026#34;127.0.0.1:6379\u0026#34;, }) r := gin.Default() r.GET(\u0026#34;/ping\u0026#34;, func(c *gin.Context) { //ctx := c //ctx := context.Background() ctx := c.Request.Context() go func() { if err := cli.Set(ctx, \u0026#34;hello\u0026#34;, 1, 0).Err(); err != nil { fmt.Println(err) } }() c.JSON(200, gin.H{ \u0026#34;message\u0026#34;: \u0026#34;pong\u0026#34;, }) }) r.Run() // listen and serve on 0.0.0.0:8080 } 分析 gin的Request对象来自*http.Request, handlerFunc被调用完成之后, context就被cancel了\n在ctx := c.Request.Context()打断点, 可以找到如下代码:\n1 2 3 4 5 6 func{ ... serverHandler{c.server}.ServeHTTP(w, w.req) w.cancelCtx() ... } 其他 使用gin+opentracing时, 不能使用 作为上下文传递, 要使用c.Request.Context(), 而不是gin.Context. 需要在goroutine中传递ctx, 应该拿出span新建一个ctx 1 2 3 4 func NewTracingContextWithParentContext(ctx context.Context) context.Context { span := opentracing.SpanFromContext(ctx) return opentracing.ContextWithSpan(context.Background(), span) } ginhttp中封装好了相应的中间件, 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 func Middleware(tr opentracing.Tracer, options ...MWOption) gin.HandlerFunc { opts := mwOptions{ opNameFunc: func(r *http.Request) string { return \u0026#34;HTTP \u0026#34; + r.Method }, spanObserver: func(span opentracing.Span, r *http.Request) {}, urlTagFunc: func(u *url.URL) string { return u.String() }, } for _, opt := range options { opt(\u0026amp;opts) } return func(c *gin.Context) { carrier := opentracing.HTTPHeadersCarrier(c.Request.Header) ctx, _ := tr.Extract(opentracing.HTTPHeaders, carrier) op := opts.opNameFunc(c.Request) sp := tr.StartSpan(op, ext.RPCServerOption(ctx)) ext.HTTPMethod.Set(sp, c.Request.Method) ext.HTTPUrl.Set(sp, opts.urlTagFunc(c.Request.URL)) opts.spanObserver(sp, c.Request) // set component name, use \u0026#34;net/http\u0026#34; if caller does not specify componentName := opts.componentName if componentName == \u0026#34;\u0026#34; { componentName = defaultComponentName } ext.Component.Set(sp, componentName) c.Request = c.Request.WithContext( opentracing.ContextWithSpan(c.Request.Context(), sp)) c.Next() ext.HTTPStatusCode.Set(sp, uint16(c.Writer.Status())) sp.Finish() } } ","date":"2022-08-24T00:00:00Z","permalink":"https://ggxxll.github.io/p/gin-opentracing%E7%9A%84%E4%BD%BF%E7%94%A8/","title":"gin+opentracing的使用"},{"content":"使用Kibana + Filebeat + Nginx 进行日志监控，Nginx的error日志的时区提前了8个小时，解决方案如下：\n原博： https://www.colabug.com/2018/0425/2780769/\n步骤一 将 /usr/share/filebeat/module/nginx/error/ingest/pipeline.json 里面的\n1 2 3 4 5 6 7 \u0026#34;date\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;nginx.error.time\u0026#34;, \u0026#34;target_field\u0026#34;: \u0026#34;@timestamp\u0026#34;, \u0026#34;formats\u0026#34;: [\u0026#34;yyyy/MM/dd H:m:s\u0026#34;], {\u0026lt; if .convert_timezone \u0026gt;}\u0026#34;timezone\u0026#34;: \u0026#34;{{ beat.timezone }}\u0026#34;,{\u0026lt; end \u0026gt;} \u0026#34;ignore_failure\u0026#34;: true } 改为：\n1 2 3 4 5 6 7 \u0026#34;date\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;nginx.error.time\u0026#34;, \u0026#34;target_field\u0026#34;: \u0026#34;@timestamp\u0026#34;, \u0026#34;formats\u0026#34;: [\u0026#34;yyyy/MM/dd H:m:s\u0026#34;], \u0026#34;timezone\u0026#34; : \u0026#34;Asia/Shanghai\u0026#34;, \u0026#34;ignore_failure\u0026#34;: true } 步骤二 1 2 3 4 5 6 7 8 9 删除旧的pipeline: curl -XDELETE \u0026#34;http://localhost:9200/_ingest/pipeline/filebeat-6.7.0-nginx-error-pipeline\u0026#34; # 重新设置 filebeat setup -e # 查看是否生效 curl -XGET \u0026#34;http://localhost:9200/_ingest/pipeline/filebeat-6.7.0-nginx-error-pipeline\u0026#34; 注意： 一般来说索引格式固定，修改版本号（6.7.0）为自己的安装版本\n","date":"2022-08-24T00:00:00Z","permalink":"https://ggxxll.github.io/p/kibana-filebeat-nginx%E7%9A%84error%E6%97%A5%E5%BF%97%E6%97%B6%E5%8C%BA%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/","title":"Kibana+Filebeat+Nginx的error日志时区问题解决方案"},{"content":"原贴: https://www.cnblogs.com/winkey4986/p/6433704.html\n\u0026ndash;数据库中单个表的大小（不包含索引）\n1 select pg_size_pretty(pg_relation_size(\u0026#39;表名\u0026#39;)); \u0026ndash;查出所有表（包含索引）并排序\n1 2 3 4 SELECT table_schema || \u0026#39;.\u0026#39; || table_name AS table_full_name, pg_size_pretty(pg_total_relation_size(\u0026#39;\u0026#34;\u0026#39; || table_schema || \u0026#39;\u0026#34;.\u0026#34;\u0026#39; || table_name || \u0026#39;\u0026#34;\u0026#39;)) AS size FROM information_schema.tables ORDER BY pg_total_relation_size(\u0026#39;\u0026#34;\u0026#39; || table_schema || \u0026#39;\u0026#34;.\u0026#34;\u0026#39; || table_name || \u0026#39;\u0026#34;\u0026#39;) DESC limit 20 \u0026ndash;查出表大小按大小排序并分离data与index\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 SELECT table_name, pg_size_pretty(table_size) AS table_size, pg_size_pretty(indexes_size) AS indexes_size, pg_size_pretty(total_size) AS total_size FROM ( SELECT table_name, pg_table_size(table_name) AS table_size, pg_indexes_size(table_name) AS indexes_size, pg_total_relation_size(table_name) AS total_size FROM ( SELECT (\u0026#39;\u0026#34;\u0026#39; || table_schema || \u0026#39;\u0026#34;.\u0026#34;\u0026#39; || table_name || \u0026#39;\u0026#34;\u0026#39;) AS table_name FROM information_schema.tables ) AS all_tables ORDER BY total_size DESC ) AS pretty_sizes ","date":"2022-08-24T00:00:00Z","permalink":"https://ggxxll.github.io/p/postgresql%E6%9F%A5%E8%AF%A2%E8%A1%A8%E7%9A%84%E5%A4%A7%E5%B0%8F/","title":"Postgresql查询表的大小"},{"content":"环境 系统: windows 语言: golang IDE: goland 安装 Protoc 下载protoc-xxx-win64.zip, 并解压到安装位置(如: d:/protoc)即可 添加环境变量d:/protoc/bin到PATH中 使用protoc --version验证是否安装成功 配置 Golang 环境 下载第三方protobuf库, 并解压到d:/protoc/include, 如: googleapis gogo/protobuf go install google.golang.org/protobuf/cmd/protoc-gen-go 安装goland插件 Protocol Buffer Editor, 按照README在配置中添加d:/protoc/include和自己实现的库的路径(目前好像是每个项目使用时都需要配置一下), 完成之后就有自动提示了（可能需要重启IDE） 使用 命令: protoc -I=. --go_out=. example.proto -I用于指定imports path, 第三方库统一放到d:/protoc/include的话是不需要指定的 官方文档 包括protobuf语法和其他语言的使用教程点击\n","date":"2022-08-24T00:00:00Z","permalink":"https://ggxxll.github.io/p/protoc%E4%BD%BF%E7%94%A8/","title":"Protoc使用"},{"content":"某日突然发现Sublime Text3（3.2.2）插件OmniMarkupPreviewer打开页面出现404错误\n1 2 3 4 5 6 7 Error: 404 Not Found Sorry, the requested URL \u0026#39;http://127.0.0.1:51004/view/26\u0026#39; caused an error: \u0026#39;buffer_id(26) is not valid (closed or unsupported file format)\u0026#39; **NOTE:** If you run multiple instances of Sublime Text, you may want to adjust the `server_port` option in order to get this plugin work again. 查看Console窗口出现如下错误：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 OmniMarkupPreviewer: [ERROR] Exception occured while rendering using MarkdownRenderer Traceback (most recent call last): File \u0026#34;%AppData%\\Roaming\\Sublime Text 3\\Packages\\OmniMarkupPreviewer\\OmniMarkupLib\\RendererManager.py\u0026#34;, line 266, in render_text rendered_text = renderer.render(text, filename=filename) File \u0026#34;%AppData%\\Roaming\\Sublime Text 3\\Packages\\OmniMarkupPreviewer\\OmniMarkupLib\\Renderers/MarkdownRenderer.py\u0026#34;, line 48, in render extensions=self.extensions) File \u0026#34;%AppData%\\Roaming\\SUBLIM~1\\Packages\\PYTHON~3\\st3\\markdown\\core.py\u0026#34;, line 390, in markdown md = Markdown(**kwargs) File \u0026#34;%AppData%\\Roaming\\SUBLIM~1\\Packages\\PYTHON~3\\st3\\markdown\\core.py\u0026#34;, line 100, in __init__ configs=kwargs.get(\u0026#39;extension_configs\u0026#39;, {})) File \u0026#34;%AppData%\\Roaming\\SUBLIM~1\\Packages\\PYTHON~3\\st3\\markdown\\core.py\u0026#34;, line 126, in registerExtensions ext = self.build_extension(ext, configs.get(ext, {})) File \u0026#34;%AppData%\\Roaming\\SUBLIM~1\\Packages\\PYTHON~3\\st3\\markdown\\core.py\u0026#34;, line 166, in build_extension module = importlib.import_module(ext_name) File \u0026#34;./python3.3/importlib/__init__.py\u0026#34;, line 90, in import_module File \u0026#34;\u0026lt;frozen importlib._bootstrap\u0026gt;\u0026#34;, line 1584, in _gcd_import File \u0026#34;\u0026lt;frozen importlib._bootstrap\u0026gt;\u0026#34;, line 1565, in _find_and_load File \u0026#34;\u0026lt;frozen importlib._bootstrap\u0026gt;\u0026#34;, line 1529, in _find_and_load_unlocked ImportError: No module named \u0026#39;xxx\u0026#39; 原因 OmniMarkupPreviewer插件使用python-markdown@3.2.2\nSublime Text3使用python-markdown@3.1.1\n当sys.path中的路径下有相同包名时，import会优先导入路径靠前的包\n所以Sublime Text3按照启动顺序加载sys.path后，会优先使用python-markdown@3.1.1，出现导包错误\n解决方案 本人是windows系统，使用%AppData%\\Sublime Text 3\\Packages\\OmniMarkupPreviewer\\OmniMarkupLib\\Renderers\\libs\\markdown文件夹覆盖%AppData%\\Sublime Text 3\\Packages\\python-markdown\\st3\\markdown\n然后重启Sublime Text3即可\n","date":"2022-08-24T00:00:00Z","permalink":"https://ggxxll.github.io/p/sublimetext3-omnimarkuppreviewer%E5%87%BA%E7%8E%B0404%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/","title":"SublimeText3+OmniMarkupPreviewer出现404问题解决方案"},{"content":"supervisor安装使用, 并配置服务崩溃邮件报警\n环境 系统: Centos7, python3\n安装依赖: 1 yum install -y supervisor sendmail mailx \u0026amp;\u0026amp; pip3 install superlance 配置邮件: vim /etc/mail.rc, 然后添加如下内容: 1 2 3 4 5 6 7 8 9 # 发件人邮箱 set from=xxx@xxx.com # smtp服务 set smtp=smtps://smtp.xxx.com:465 # 用户名 set smtp-auth-user=xxx@xxx.com # 密码 set smtp-auth-password=xxx set ssl-verify=ignore 测试邮件: echo 'this is test'| /usr/bin/mail -s 'xxxxx' xxx@xx.com 配置项目: 编辑项目的配置文件, vim /etc/supervisord.d/xxx.ini\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # 项目相关配置 [program:projectName] # 设置命令在指定的目录内执行 directory=projectPath # 这里为您要管理的项目的启动命令 command=projectRunCmd # 以哪个用户来运行该进程 user=root # supervisor 启动时自动该应用 autostart=true # 进程退出后自动重启进程 autorestart=true # 进程持续运行多久才认为是启动成功 startsecs=2 # 重试次数 startretries=3 # stderr 日志输出位置 stderr_logfile=path/stderr.log # stdout 日志输出位置 stdout_logfile=path/stdout.log 1 2 3 4 5 6 7 8 # 报警邮件相关配置 [eventlistener:crashmail] command=crashmail -p senddemo -s \u0026#34;echo \u0026#39;projectName crashed!!\u0026#39;| /usr/bin/mail -s \u0026#39;projectName\u0026#39; xxx@xx.com,xxx@xx.com\u0026#34; events=PROCESS_STATE_EXITED ## stderr 日志输出位置 stderr_logfile=path/crashmail/stderr.log ## stdout 日志输出位置 stdout_logfile=path/crashmail/stdout.log 相关命令: 1 2 3 4 5 6 7 8 9 supervisord -c /etc/supervisord.conf # 启动supervisor, 然后才可可以使用supervisorctl supervisorctl stop program_name # 停止某一个进程，program_name 为 [program:name] 里的 name supervisorctl start program_name # 启动某个进程 supervisorctl restart program_name # 重启某个进程 supervisorctl stop groupworker: # 结束所有属于名为 groupworker 这个分组的进程 (start，restart 同理) supervisorctl stop groupworker:name1 # 结束 groupworker:name1 这个进程 (start，restart 同理) supervisorctl stop all # 停止全部进程，注：start、restartUnlinking stale socket /tmp/supervisor.sock supervisorctl reload # 载入最新的配置文件，停止原有进程并按新的配置启动、管理所有进程 supervisorctl update # 根据最新的配置文件，启动新配置或有改动的进程，配置没有改动的进程不会受影响而重启 ","date":"2022-08-24T00:00:00Z","permalink":"https://ggxxll.github.io/p/supervisor%E4%BD%BF%E7%94%A8/","title":"Supervisor使用"},{"content":"简介 core 是由大佬Reasno创建的一个golang开源库.。这个库的定位是一个服务容器，负责管理和协调符合 Twelve-Factor的网络应用程序。\n目前这个库的结构：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 core ├── clihttp ├── config ├── container ├── contract ├── cronopts ├── di ├── doc ├── dtx ├── events ├── internal ├── key ├── leader ├── logging ├── observability ├── otes ├── otetcd ├── otgorm ├── otkafka ├── otmongo ├── otredis ├── ots3 ├── queue ├── srvhttp ├── text └── unierr ","date":"2022-08-24T00:00:00Z","permalink":"https://ggxxll.github.io/p/%E5%BC%80%E6%BA%90%E9%A1%B9%E7%9B%AEcore/","title":"开源项目core"},{"content":"题一 问题1： 会输出什么？为什么？ 问题2： 应该如何修改？ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 package main import \u0026#34;fmt\u0026#34; func main() { cache := Cache{} fmt.Println(cache.Get(\u0026#34;foo\u0026#34;)) cache.Set(\u0026#34;foo\u0026#34;, \u0026#34;bar\u0026#34;) fmt.Println(cache.Get(\u0026#34;foo\u0026#34;)) } type Cache struct { data map[string]interface{} } func (c Cache) Get(s string) (interface{},bool) { v, ok := c.data[s] return v, ok } func (c Cache) Set(s string, v interface{}) { c.data[s] = v } 题二 问题1：会输出什么？为什么？ 问题2：应该如何修改？ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 package main import \u0026#34;fmt\u0026#34; func main() { cache := Cache{} cache.Add(1) fmt.Println(cache.List()) } type Cache struct { data []interface{} } func (c Cache) Add(s interface{}) { c.data = append(c.data, s) } func (c Cache) List() []interface{} { return c.data } 题三 问题1：为什么题一需要初始化data，题二不需要? 问题2：实现这个Cache还需要什么补充的吗? 题一解 问题1： Get 返回 nil, false Set 会报空指针错误 问题2： Cache 需要初始化 data 题二解 问题1： 输出空数组 问题2： 修改 func (c Cache) Add 为 func (c *Cache) Add 题三解 问题1：因为 map 是引用传递，数组是值传递 问题2：需要添加锁 sync.Mutex ，并将 Cache 的方法改为指针调用 ","date":"2022-08-24T00:00:00Z","permalink":"https://ggxxll.github.io/p/%E4%BD%BF%E7%94%A8-map-%E8%AE%BE%E8%AE%A1%E7%BC%93%E5%AD%98/","title":"使用 map 设计缓存"},{"content":"环境 golang 问题 在k8s中部署多个节点服务时，CDN的Range回源经常失败。\n原因：每个节点生成渠道包的时间（Modify-Time）不同，导致CDN回源时认为文件变动了，自动停止回源。\n解决：使用http.ServeContent, 手动传递\n代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 // serveFile // 将newApkPath地址下的文件返回给请求方。 // 注意：曾经这里是使用ctx.File(调用了Go的http.ServeFile)方法返回文件， // 但是这样做在集群部署的情况下存在问题。集群每个节点文件生成的时间存在差异， // 而go标准库中的实现是根据文件的最后修改时间判断文件是否发生了变动。 // 大文件下载时CDN会采用分片回源（Range Header）， 每个分片获取到的最后修改时间不一致，会判定为下载失败！ // 这里改成http.ServeContent实现后，强制将修改时间置为0值，Go的标准库会忽略最后修改时间检测，在集群-多节点环境下也能正常分片下载了。 func serveFile(ctx *gin.Context, newApkPath string) error { _, apkName := filepath.Split(newApkPath) ctx.Header(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/octet-stream\u0026#34;) ctx.Header(\u0026#34;Content-Disposition\u0026#34;, \u0026#34;attachment; filename=\u0026#34;+apkName) ctx.Header(\u0026#34;Content-Transfer-Encoding\u0026#34;, \u0026#34;binary\u0026#34;) f, err := os.OpenFile(newApkPath, os.O_RDONLY, os.ModePerm) if err != nil { return err } defer f.Close() /* 最后修改时间，强制0值，跳过检查 */ http.ServeContent(ctx.Writer, ctx.Request, apkName, time.Time{}, f) return nil } ","date":"2022-08-24T00:00:00Z","permalink":"https://ggxxll.github.io/p/%E6%96%87%E4%BB%B6%E4%B8%8B%E8%BD%BDbug/","title":"文件下载Bug"}]